[{"content":"ODATA actions in Data Entities provide a way to inject behaviors into the data model, or expose custom business logic from Dynamics 365 Finance \u0026amp; Operations. You can add actions by adding a method to the data entity and then decorating the method with specific attributes [SysODataActionAttribute]\nI use this Odata actions mostly in automation job like after refreshing data from PROD to UAT, we need to enable users, assign company to users, enable batches \u0026hellip; Or simply consume it in Power Automate.\n1. Create an action to OData entity You can create a new entity following this standard docs or you can duplicate any standard entity. I created AutomationDataEntity. Right-click the enitity, select View code and add the code\npublic class AutomationDataEntity extends common { //1st example [SysODataActionAttribute(\u0026#34;assignUserToCompany\u0026#34;, false)] public static void assignUserToCompany(NetworkAlias _networkAlias, DataAreaName _company) { UserInfo userInfo; ttsbegin; select firstonly forupdate userInfo where userInfo.networkAlias == _networkAlias; userInfo.company = _company; userInfo.update(); ttscommit; } //2nd example [SysODataActionAttribute(\u0026#34;ReturnRental\u0026#34;, true)] public str ReturnRental() { return \u0026#34;Rental was successfully returned. Thanks for your business\u0026#34;; } //following 3rd example of an OData action takes in a parameter and returns a list [SysODataActionAttribute(\u0026#34;GetColors\u0026#34;, true), SysODataCollectionAttribute(\u0026#34;return\u0026#34;, Types::Record, \u0026#34;CarColor\u0026#34;)] public List GetColorsByAvailability(boolean onlyAvailableVehicles) { List returnList = new List(Types::Record); // do something  return returnList; } } In this example, the SysODataActionAttribute class decorates the assginUserToCompany method that is exposed as an action. The first argument of the attribute is the publicly exposed name of the action, and the second argument indicates whether this action need an entity instance or not. If you set the second argument to false, the method has to be static.\n You might need reset IIS service to update Odata endpoint.\n 2. Test Entity Odata actions with Postman and Power Automate 2.1. With Postman Please follow this document for basic configurations in Dynamics 365 Finance \u0026amp; Operation, Azure to work with Postman.\n2.1.1. Let\u0026rsquo;s use the first example. Specify Odata endpoint request with POST method into Postman application\n[finopsURL]/data/AutomationDatas/Microsoft.Dynamics.DataEntities.assignUserToCompany [finopsURL] = https://[yourenvironment].cloudax.dynamics.com\nHere is the Json file contains the parameters for assignUserToCompany method\n{ \u0026#34;_networkAlias\u0026#34;:\u0026#34;Max.Nguyen@Microsoft.com\u0026#34;, \u0026#34;_company\u0026#34;:\u0026#34;USMF\u0026#34; } Click Send and you will get your logic executed.\n2.1.2. Let\u0026rsquo;s try with the second example Everything should be remain the same, you just need to change the method to ReturnRental\n[finopsURL]/data/AutomationDatas/Microsoft.Dynamics.DataEntities.ReturnRental Click Send and you will get an error\n{ \u0026#34;Message\u0026#34;: \u0026#34;No HTTP resource was found that matches the request URI \u0026#39;https://[devaos].cloudax.dynamics.com/data/AutomationDatas/Microsoft.Dynamics.DataEntities.ReturnRental\u0026#39;. No route data was found for this request.\u0026#34; } The reason is that you set the second argument to true, that means you need an instance for AutomationDatas entity before you can use ReturnRental method. My entity created based on CustGroup table, so to get an instance I need DataAreaId and CustGroupID. The correct endpoint should be\n[finopsURL]/data/AutomationDatas(dataAreaId=\u0026#39;USMF\u0026#39;,CustomerGroupId=\u0026#39;BRIDGE\u0026#39;)/Microsoft.Dynamics.DataEntities.ReturnRental The result\n{ \u0026#34;@odata.context\u0026#34;: \u0026#34;https://[devaos].cloudax.dynamics.com/data/$metadata#Edm.String\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Rental was successfully returned. Thanks for your business\u0026#34; } 2.2. With Power Automate 2.2.1. With the first example Create a simple Power Automate with Dynamics 365 Finance \u0026amp; Operations connector, to consume Odata actions we use \u0026ldquo;Execute action\u0026rdquo; action following\n2.2.2. With the second example when specify action in Execute action, Dynamics 365 Finance \u0026amp; Operations connector understand that this needs an instance\n3. More In Odata actions, you can return a list\n[SysODataActionAttribute(\u0026#34;GetColors\u0026#34;, true), SysODataCollectionAttribute(\u0026#34;return\u0026#34;, Types::Record, \u0026#34;CarColor\u0026#34;)] public List GetColorsByAvailability(boolean onlyAvailableVehicles) { List returnList = new List(Types::Record); // do something  return returnList; } The following example of an OData action takes in list a parameter.\n[SysODataActionAttribute(\u0026#34;GetColorsByAvailability\u0026#34;, false), SysODataCollectionAttribute(\u0026#34;InventSiteIdList\u0026#34;, Types::String), SysODataCollectionAttribute(\u0026#34;return\u0026#34;, Types::String)] public static str GetColorsByAvailability(List InventSiteIdList) { str\tstrCommaSeperated; List list = new List(Types::String); ListEnumerator ListEnumerator; ListEnumerator = InventSiteIdList.getEnumerator(); while (ListEnumerator.moveNext()) { strCommaSeperated += strFmt(\u0026#39;%1, \u0026#39;, ListEnumerator.current()) ; } return strCommaSeperated; } In those examples,the SysODataCollectionAttribute class enables OData to expose strongly typed collections from X++. This class takes in three parameters:\n The name of the parameter that is a list (Use return for the return value of the method.). The X++ type of the members of this list. The public name of the OData resource that is contained in the collection.  You can find actions that are defined on data entities by searching for the SysODataActionAttribute attribute in metadatasearch.\nIf you want to check how many Odata actions available for an entity, you can go here and search for an entity.\nhttps://[devaos].cloudax.dynamics.com/data/$metadata \u0026lt;Action Name=\u0026#34;removeDeleteCT\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Collection(Microsoft.Dynamics.DataEntities.AutomationData)\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_entityName\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; \u0026lt;Action Name=\u0026#34;assginUserToCompany\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Collection(Microsoft.Dynamics.DataEntities.AutomationData)\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_networkAlias\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_company\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; \u0026lt;Action Name=\u0026#34;ReturnRental\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Microsoft.Dynamics.DataEntities.AutomationData\u0026#34;/\u0026gt; \u0026lt;ReturnType Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; \u0026lt;Action Name=\u0026#34;addToAllUserGroups\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Collection(Microsoft.Dynamics.DataEntities.AutomationData)\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_userId\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; At the time I\u0026rsquo;m writing this post, Odata actions does not support COC extension (A 18, P 42). So if you write some thing like this, it won\u0026rsquo;t work.\n[ExtensionOf(dataentityviewstr(CustCustomerGroupEntity))] final class CustCustomerGroupEntity_KA_Extension { [SysODataActionAttribute(\u0026#34;ReturnRental\u0026#34;, false)] public static str ReturnRental() { return \u0026#34;Rental was successfully returned. Thanks for your business\u0026#34;; } } Thank you for reading.\n","permalink":"https://nuxulu.com/posts/2021-05-25-all-about-odata-actions-in-dynamics-365-finance-and-operations/","summary":"ODATA actions in Data Entities provide a way to inject behaviors into the data model, or expose custom business logic from Dynamics 365 Finance \u0026amp; Operations. You can add actions by adding a method to the data entity and then decorating the method with specific attributes [SysODataActionAttribute]\nI use this Odata actions mostly in automation job like after refreshing data from PROD to UAT, we need to enable users, assign company to users, enable batches \u0026hellip; Or simply consume it in Power Automate.","title":"All about Odata actions in Dynamics 365 Finance And Operations"},{"content":"This post has been written thanks to Joris de Gruyter‘s session in the past DynamicsCon: Azure Devops Automation for Finance and Operations Like You’ve Never Seen! And there’s also been some investigation and (a lot of) trial-and-error from my side until everything has been working.\nConfiguring the build VM in Azure DevTest Labs\nIf you want to know more about builds, releases, and the Dev ALM of Dynamics 365 you can read my full guide on MSDyn365 \u0026amp; Azure DevOps ALM.\nBut first… What I’m showing in this post is not a perfect blueprint. There’s a high probability that if you try exactly the same as I do here, you won’t get the same result. But it’s a good guide to get started and do some investigation on your own and learn.\nAzure DevTest Labs Azure DevTest Labs is an Azure tool/service that allows us to deploy virtual machines and integrate them with Azure DevOps pipelines, and many other things, but what I’m going to explain is just the VM and pipeline part.\nWhat will I show in this post? How to prepare a Dynamics 365 Finance and Operations VHD image to be used as the base to create a build virtual machine from an Azure DevOps pipeline, build our codebase, synchronize the DB, run tests, even deploy the reports, generate the deployable package and delete the VM.\nGetting and preparing the VHD This is by far the most tedious part of all the process because you need to download 11 ZIP files from LCS’ Shared Asset Library, and we all know how fast things download from LCS.\nHow is LCS download speed?\nAnd to speed it up we can create a blob storage account on Azure and once more turn to Mötz Jensen‘s d365fo.tools and use the Invoke-D365AzCopyTransfer cmdlet. Just go to LCS, click on the “Generate SAS link” button for each file, use it as the source parameter in the command and your blob SAS URL as the destination one. Once you have all the files in your blob you can download them to your local PC at a good speed.\nOnce you’ve unzipped the VHD you need to change it from Dynamic to Fixed using this PowerShell command:\nConvert-VHD –Path VHDLOCATION.vhd –DestinationPath NEWVHD.vhd –VHDType Fixed The reason is you can’t create an Azure VM from a dynamically-sized VHD. And it took me several attempts to notice this 🙂\nCreate a DevTest Labs account To do this part you need an Azure account. If you don’t have one you can sign up for a free Azure account with a credit of 180 Euros (200 US Dollars) to be spent during 30 days, plus many other free services during 12 months.\nSearch for DevTest Labs in the top bar and create a new DevTest Lab. Once it’s created open the details and you should see something like this:\nAzure DevTest Labs\nClick on the “Configuration and policies” menu item at the bottom of the list and scroll down in the menu until you see the “Virtual machine bases” section:\nDevTest Labs custom image\nAnd now comes the second funniest part of the process: we need to upload the 130GB VHD image to a blob storage account! So, click the “Add” button on top and in the new dialog that will open click the “Upload a VHD using PowerShell”. This will generate a PowerShell script to upload the VHD to the DevTest Labs blob. For example:\n\u0026lt;# Generated script to upload a local VHD to Azure. WARNING: The destination will be publicly available for 24 hours, after which it will expire. Ensure you complete your upload by then. Run the following command in a Azure PowerShell console after entering the LocalFilePath to your VHD. #\u0026gt; Add-AzureRmVhd \\-Destination \u0026#34;https://YOURBLOB.blob.core.windows.net/uploads/tempImage.vhd?sv=2019-07-07\u0026amp;st=2020-12-27T09%3A08%3A26Z\u0026amp;se=2020-12-28T09%3A23%3A26Z\u0026amp;sr=b\u0026amp;sp=rcw\u0026amp;sig=YTeXpxpVEJdSM7KZle71w8NVw9oznNizSnYj8Q3hngI%3D\u0026#34; \\-LocalFilePath \u0026#34;\u0026lt;Enter VHD location here\u0026gt;\u0026#34; DevTest Labs custom image upload\nAn alternative to this is using the Azure Storage Explorer as you can see in the image on the left.\nYou should upload the VHD to the uploads blob.\nAny of these methods is good to upload the VHD and I don’t really know which one is faster.\nOnce the VHD is uploaded open the “Custom images” option again and you should see the VHD in the drop-down:\nDevTest Labs custom image\nGive the image a name and click OK.\nWhat we have now is the base for a Dynamics 365 Finance and Operations dev VM which we need to prepare to use it as a build VM.\nCreating the VM We’ve got the essential, a VHD ready to be used as a base to create a virtual machine in Azure. Our next step is finding a way to make the deployment of this VM predictable and automated. We will attain this thanks to Azure ARM templates.\nGo back to your DevTest Labs overview page and click the “Add” button, on the “Choose base” page select the base you’ve just created, and on the next screen click on the “Add or Remove Artifacts” link:\nAdd artifacts to the VM\nSearch for WinRM, select “Configure WinRM”, and on the next screen enter “Shared IP address” as the hostname box and click “Add”.\nNote: if when the VM runs the artifacts can’t be installed check whether the Azure VM Agent is installed on the base VHD. Thanks to Joris for pointing this out!\nConfigure Azure DevOps Agent Service Option A: use an artifact Update: thanks to Florian Hopfner for reminding me this because I forgot… If you choose Option A to install the agent service you need to do some things first!\nThe first thing we need to do is running some PowerShell scripts that create registry entries and environment variables in the VM, go to C:\\DynamicsSDK and run these:\nImport-Module $(Join-Path \\-Path \u0026#34;C:\\\\DynamicsSDK\u0026#34; \\-ChildPath \u0026#34;DynamicsSDKCommon.psm1\u0026#34;) \\-Function \u0026#34;Write-Message\u0026#34;, \u0026#34;Set-AX7SdkRegistryValues\u0026#34;, \u0026#34;Set-AX7SdkEnvironmentVariables\u0026#34; Set\\-AX7SdkEnvironmentVariables \\-DynamicsSDK \u0026#34;C:\\\\DynamicsSDK\u0026#34; Set\\-AX7SdkRegistryValues \\-DynamicsSDK \u0026#34;c:\\\\DynamicsSDK\u0026#34; \\-TeamFoundationServerUrl \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-AosWebsiteName $AosWebsiteName \u0026#34;AosService\u0026#34; The first one will load the functions and make them available in the command-line and the other two create the registry entries and environment variables.\nNow we need to add an artifact for the Azure DevOps agent service. This will configure the agent service on the VM each time the VM is deployed. Search for “Azure Pipelines Agent” and click it. You will see this:\nDevTest Labs Azure DevOps Agent\nWe need to fill some information:\nOn “Azure DevOps Organization Name” you need to provide the name of your organization. For example if your AZDO URL is https://dev.azure.com/blackbeltcorp you need to use blackbeltcorp.\nOn “AZDO Personal Access Token” you need to provide a token generated from AZDO.\nOn “Agent Name” give your agent a name, like DevTestAgent. And on “Agent Pool” a name for your pool, a new like DevTestPool or an existing one as Default.\nOn “Account Name” use the same user that we’ll use in our pipeline later. Remember this. And on “Account Password” its password. Using secrets with a KeyVault is better, but I won’t explain this here.\nAnd, finally, set “Replace Agent” to true.\nOption B: Configure Azure DevOps Agent in the VM To do this you have to create a VM from the base image you created before and then go to C:\\DynamicsSDK and run the SetupBuildAgent script with the needed parameters:\nSetupBuildAgent.ps1 \\-VSO\\_ProjectCollection \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-ServiceAccountName \u0026#34;myUser\u0026#34; \\-ServiceAccountPassword \u0026#34;mYPassword\u0026#34; \\-AgentName \u0026#34;DevTestAgent\u0026#34; \\-AgentPoolName \u0026#34;DevTestPool\u0026#34; \\-VSOAccessToken \u0026#34;YOUR\\_VSTS\\_TOKEN\u0026#34; WARNING: If you choose option B you must create a new base image from the VM where you’ve run the script. Then repeat the WinRM steps to generate the new ARM template which we’ll see next.\nARM template Then go to the “Advanced Settings” tab and click the “View ARM template” button:\nGet the ARM template\nThis will display the ARM template to create the VM from our pipeline. It’s something like this:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;newVMName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste001\u0026#34; }, \u0026#34;labName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aristeinfo\u0026#34; }, \u0026#34;size\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard\\_B4ms\u0026#34; }, \u0026#34;userName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;myUser\u0026#34; }, \u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\\[\\[\\[VmPassword\\]\\]\u0026#34; }, \u0026#34;Configure\\_WinRM\\_hostName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Public IP address\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;ariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_poolName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestPool\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;C\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: true } }, \u0026#34;variables\u0026#34;: { \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[concat(variables(\u0026#39;labVirtualNetworkName\u0026#39;), \u0026#39;Subnet\u0026#39;)\\]\u0026#34;, \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/virtualnetworks\u0026#39;, parameters(\u0026#39;labName\u0026#39;), variables(\u0026#39;labVirtualNetworkName\u0026#39;))\\]\u0026#34;, \u0026#34;labVirtualNetworkName\u0026#34;: \u0026#34;\\[concat(\u0026#39;Dtl\u0026#39;, parameters(\u0026#39;labName\u0026#39;))\\]\u0026#34;, \u0026#34;vmId\u0026#34;: \u0026#34;\\[resourceId (\u0026#39;Microsoft.DevTestLab/labs/virtualmachines\u0026#39;, parameters(\u0026#39;labName\u0026#39;), parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;\\[concat(parameters(\u0026#39;labName\u0026#39;), \u0026#39;/\u0026#39;, parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34; }, \u0026#34;resources\u0026#34;: \\[ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-15-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.DevTestLab/labs/virtualmachines\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmName\u0026#39;)\\]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;\\[resourceGroup().location\\]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[variables(\u0026#39;labVirtualNetworkId\u0026#39;)\\]\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;Dynamics365FnO10013AgentLessV2\u0026#34;, \u0026#34;customImageId\u0026#34;: \u0026#34;/subscriptions/6715778f-c852-453d-b6bb-907ac34f280f/resourcegroups/devtestlabs365/providers/microsoft.devtestlab/labs/devtestd365/customimages/dynamics365fno10013agentlessv2\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;\\[parameters(\u0026#39;size\u0026#39;)\\]\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;\\[parameters(\u0026#39;userName\u0026#39;)\\]\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\\[parameters(\u0026#39;password\u0026#39;)\\]\u0026#34;, \u0026#34;isAuthenticationWithSshKey\u0026#34;: false, \u0026#34;artifacts\u0026#34;: \\[ { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-winrm\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;hostName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Configure\\_WinRM\\_hostName\u0026#39;)\\]\u0026#34; } \\] }, { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-vsts-build-agent\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;vstsAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vstsPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentNameSuffix\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;poolName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_poolName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;RunAsAutoLogon\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;driveLetter\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;workDirectory\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;replaceAgent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#39;)\\]\u0026#34; } \\] } \\], \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[variables(\u0026#39;labSubnetName\u0026#39;)\\]\u0026#34;, \u0026#34;disallowPublicIpAddress\u0026#34;: true, \u0026#34;storageType\u0026#34;: \u0026#34;Premium\u0026#34;, \u0026#34;allowClaim\u0026#34;: false, \u0026#34;networkInterface\u0026#34;: { \u0026#34;sharedPublicIpAddressConfiguration\u0026#34;: { \u0026#34;inboundNatRules\u0026#34;: \\[ { \u0026#34;transportProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389 } \\] } } } } \\], \u0026#34;outputs\u0026#34;: { \u0026#34;labVMId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmId\u0026#39;)\\]\u0026#34; } } } NOTE: if you’re using option B you won’t have the artifact node for the VSTS agent.\nThis JSON file will be used as the base to create our VMs from the Azure DevOps pipeline. This is known as Infrastructure as Code (IaC) and it’s a way of defining our infrastructure in a file as it were code. It’s another part of the DevOps practice that should solve the “it works on my machine” issue.\nIf we take a look to the JSON’s parameters node there’s the following information:\n newVMName and labName will be the name of the VM and the DevTest Labs lab we’re using. The VM name is not really important because we’ll set the name later in the pipeline. size is the VM size, a D3 V2 in the example above, but we can change it and will do it later. userName \u0026amp; passWord will be the credentials to access the VM and must be the same we’ve used to configure the Azure DevOps agent. Configure_WinRM_hostName is the artifact we added to the VM template that will allow the pipelines to run in this machine.  To do it faster and for demo purposes I’m using a plain text password in the ARM template, changing the password node to something like this:\n\u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;yourPassword\u0026#34; }, I will do the same with all the secureString nodes, but you shouldn’t and should instead use an Azure KeyVault which comes with the DevTest Labs account.\nOf course you would never upload this template to Azure DevOps with a password in plain text. There’s plenty of resources online that teach how to use parameters, Azure KeyVault, etc. to accomplish this, for example this one: 6 Ways Passing Secrets to ARM Templates.\nOK, now grab that file and save it to your Azure DevOps repo. I’ve created a folder in my repo’s root called ARM where I’m saving all the ARM templates:\nARM templates on Azure DevOps\nPreparing the VM The VHD image you download can be used as a developer VM with no additional work, just run Visual Studio, connect it to your AZDO project and done. But if you want to use it as a build box you need to do several things first.\nRemember that the default user and password for these VHDs are Administrator and Pass@word1.\nDisable services First of all we will stop and disable services like the Batch, Management Reporter, SSAS, SSIS, etc. Anything you see that’s not needed to run a build.\nCreate a new SQL user Open SSMS (as an Administrator) and create a new SQL user as a copy of the axdbadmin one. Then open the web.config file and update the DB user and password to use the one you’ve just created.\nPrepare SSRS (optional) If you want to deploy reports as part of your build pipeline you need to go to SSMS again (and as an Admin again), and open a new query in the reporting DB to execute the following query:\nexec DeleteEncryptedContent PowerShell Scripts The default build definition that runs on a build VM uses several PowerShell scripts to run some tasks. I’m adding an additional script called PrepareForAgent.\nThe scripts can be found in the C:\\DynamicsSDK folder of the VM.\nPrepareForBuild This script comes with the VM and we need to modify it to avoid one thing: the PackagesLocalDirectory backup which is usually done in the first build. We need to get rid of this or we’ll waste around an hour per run until the files are copied.\nWe don’t need this because our VM will be new each time we run the pipeline!\nSo open the script, go to line 696 and look for this piece of code:\n\\# Create packages backup (if it does not exist). $NewBackupCreated \\= Backup-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation \\# Restore packages backup (unless a new backup was just created). if (!$NewBackupCreated) { Restore-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation \\-RestoreAllFiles:$RestorePackagesAllFiles } if (!$DatabaseBackupToRestore) { $DatabaseBackupPath \\= Get-BackupPath \\-Purpose \u0026#34;Databases\u0026#34; Backup-AX7Database \\-BackupPath $DatabaseBackupPath } else { \\# Restore a database backup (if specified). Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We need to modify it until we end up with this:\nif ($DatabaseBackupToRestore) { Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We just need the DB restore part and skip the backup, otherwise we’ll be losing 45 minutes in each run for something we don’t need because the VM will be deleted when the build is completed.\nOptional (but recommended): install d365fo.tools Just run this:\nInstall-Module \\-Name d365fo.tools We can use the tools to do a module sync, partial sync or deploy just our reports instead of all.\nCreate a new image Once we’ve done all these prepare steps we can log out of this VM and stop it. Do not delete it yet! Go to “Create custom image”, give the new image a name, select “I have not generalized this virtual machine” and click the “OK” button.\nThis will generate a new image that you can use as a base image with all the changes you’ve done to the original VHD.\nAzure DevOps Pipelines We’re ready to setup our new build pipeline in Azure DevOps. This pipeline will consist of three steps: create a new VM, run all the build steps, and delete the VM:\nFirst of all check that your pipeline runs on Azure pipelines (aka Azure-hosted):\nDevTest Labs Azure Pipelines\nThe create and delete steps will run on the Azure Pipelines pool. The build step will run on our DevTestLabs pool, or the name you gave it when configuring the artifact on DevTest Labs or the script on the VM.\nCreate Azure DevTest Labs VM Create a new pipeline and choose the “Use the classic editor” option. Make sure you’ve selected TFVC as your source and click “Continue” and “Empty job”. Add a new task to the pipeline, look for “Azure DevTest Labs Create VM”. We just need to fill in the missing parameters with our subscription, lab, etc.\nCreate VM Azure DevTest Labs\nRemember this step must run on the Azure-hosted pipeline.\nBuild This is an easy one. Just export a working pipeline and import it. And this step needs to run on your self-hosted pool:\nRuns on self-hosted pool\nOptional: use SelectiveSync (not recommended, see next option) You can replace the Database Sync task for a PowerShell script that will only sync the tables in your models:\nSelectiveSync.ps1\nThanks Joris for the tip!\nOptional: use d365fo.tools to sync your packages/models This is a better option than the SelectiveSync above. You can synchronize your packages or models only to gain some time. This cmdlet uses sync.exe like Visual Studio does and should be better than SelectiveSync.\nAdd a new PowerShell task, select Inline Script and this is the command:\nInvoke-D365DbSyncModule \\-Module \u0026#34;Module1\u0026#34;, \u0026#34;Module2\u0026#34; \\-ShowOriginalProgress \\-Verbose Optional: use d365fo.tools to deploy SSRS reports If you really want to add the report deployment step to your pipeline you can save some more extra time using d365fo.tools and just deploy the reports in your models like we’ve done with the DB sync.\nRun this in a new PowerShell task to do it:\nPublish-D365SsrsReport \\-Module YOUR\\_MODULE \\-ReportName \\* Delete Azure DevTest Labs VM It’s almost the same as the create step, complete the subscription, lab and VM fields and done:\nDelete VM\nAnd this step, like the create one, will run on the Azure-hosted agent.\nDependencies and conditions When all three steps are configured we need to add dependencies and conditions to some of them. For example, to make sure that the delete VM step runs when the build step fails, but it doesn’t when the create VM step fails.\nBuild The build step depends on the create VM step, and will only run if the previous step succeeds:\nBuild step dependencies and conditions\nDelete VM The delete step depends on all previous steps and must run when the create VM step succeeds. If the create step fails there’s no VM and we don’t need to delete it:\nDependencies and conditions on delete VM step\nThis is the custom condition we’ll use:\nand(always(), eq(dependencies.Job\\_1.status, \u0026#39;Succeeded\u0026#39;)) If you need to know your first step’s job name just export the pipeline to YAML and you’ll find it there:\nExport pipeline to YAML\nJob name on YAML\nIf this step fails when the pipeline is run, wait to delete the VM manually, first change the VM name in the delete step, save your pipeline and then use the dropdown to show the VMs in the selected subscription, and save the pipeline.\nRun the build And, I think, we’re done and ready to run our Azure DevTest Labs pipeline for Dynamics 365 Finance and Operations… click “Run pipeline” and wait…\nTadaaaa!!\nTimes The pipeline from the image above is one with real code from a customer but I can’t compare the times with the Azure-hosted builds because there’s no sync, or tests there. Regarding the build time the Azure-hosted takes one minute less, but it needs to install the nugets first.\nBut for example this is a comparison I did:\nAzure DevTest Labs B2ms vs B4ms\nIt takes around 1 hour to create the VM, build, do a full DB synch, deploy reports, run tests, generate a Deployable Package and, finally, delete the VM:\nIf you skip deploying the SSRS reports your build will run in 15 minutes less, that’s around 45 minutes.\nIf you use the partial sync process instead of a full DB sync it’ll be 5-7 minutes less.\nThis would leave us with a 35-40 minutes build.\nComparison 1 No DB Sync\nThe image above shows a simple package being compiled, without any table, so the selective sync goes really fast. The build times improve with VM size.\nComparison 2 Same code Full DB Sync\nThis one is compiling the same codebase but is doing a full DB sync. The sync time improves in the B4ms VM compared to the B2ms but it’s almost the same in the B8ms. Build times are better for larger VM sizes.\nComparison 3 Real code + full sync\nAnd in the image above we see a more realistic build. The codebase is larger and we’re doing a full DB sync.\nSimilar as the comparison before there a good enhancement between a B2ms and a B4ms, but not between a B4ms and B8ms.\nShow me the money! I think this is the interesting comparison. How did a Tier-1 MS-hosted build VM cost? Around 400€? How does it compare to using the Azure DevTest Labs alternative?\nThere’s only one fix cost when using Azure DevTest Labs: the blob storage where the VHD is uploaded. The VHD’s size is around 130GB and this should have a cost of, more or less, 5 euros/month. Keep in mind that you need to clean up your custom images when yours is prepared, the new ones are created as snapshots and also take space in the storage account.\nThen we have the variable costs that come with the deployment of a VM each build but it’s just absurd. Imagine we’re using a B4ms VM, with a 256GB Premium SSD disk, we would pay 0.18€/hour for the VM plus the proportional part of 35.26€/month of the SSD disk, which would be like 5 cents/hour?\nBut this can run on a B2ms VM too which is half the compute price of the VM, down to 9 cents per hour.\nIf we run this build once a day each month, 30 times, the cost of a B4ms would be like… 7€? Add the blob storage and we’ll be paying 12€ per month to run our builds with DB sync and tests.\nIs it cheaper than deploying a cloud-hosted environment, and starting and stopping it using the new d365fo.tools Cmdlets each time we run the build? Yes it is! Because if we deploy a CHE we’ll be paying the price of the SSD disk for the whole month!\nSome final remarks  I have accomplished this mostly through trial-and-error. There’s lots of enhancements and best practices to be applied to all the process, specially using an Azure Key Vault to store all the secrets to be used in the Azure DevOps Agent artifact and the pipeline. This in another clear example that X++ developers need to step outside of X++ and Dynamics 365 FnO. We’re not X++ only developers anymore, we’re very lucky to be working on a product that is using Azure. I’m sure there’s scenarios where using DevTest Labs to create a build VM is useful. Maybe not for an implementation partner, but maybe it is for an ISV partner. It’s just an additional option. The only bad thing to me is that we need to apply the version upgrades manually to the VHDs because they’re published only twice a year. As I said at the beginning of the post, it may have worked to me with all these steps, but if you try you maybe need to change some things. But it’s a good way to start.  ","permalink":"https://nuxulu.com/posts/2021-05-03-azure-devtest-labs-powered-builds-for-dynamics-365-finops/","summary":"This post has been written thanks to Joris de Gruyter‘s session in the past DynamicsCon: Azure Devops Automation for Finance and Operations Like You’ve Never Seen! And there’s also been some investigation and (a lot of) trial-and-error from my side until everything has been working.\nConfiguring the build VM in Azure DevTest Labs\nIf you want to know more about builds, releases, and the Dev ALM of Dynamics 365 you can read my full guide on MSDyn365 \u0026amp; Azure DevOps ALM.","title":"Azure DevTest Labs powered builds for Dynamics 365 FinOps"},{"content":"Dual-write has been around for almost two years now. It’s one of the ways of integrating Dynamics 365 Finance and Operations and Dataverse along with Virtual Entities.\nThe standard solution comes with many out-of-the-box entities available to synchronize. This has been one of the great improvements since Dual-write was made available in preview, when Juanan and I demoed it in the 2019 Dynamics Saturday in Madrid.\nThis is how Dual write really works\nBut what if we need to develop a new custom Data Entity in MSDyn365FO and use it in Dual-write? It’s easy but there’s some things we need to remember when doing it.\nDual-write Dual-write is a bidirectional integration that will synchronously write in Dataverse when data is created, updated or deleted in MSDyn365FO in near-real-time. On the Finance and Operations side it uses data entities to export data to Dataverse.\nRight now there’s a set of several OOB entities that come ready to be synchronized, and thanks to Initial Sync we can populate data in Dataverse choosing FnO as the source when starting the sync, or also choose Dataverse as the source.\nIf you want to learn more about Dual-write you can:\n Read the docs which have plenty of information. Read the docs. Always. Guidance for Dual-Write setup System requirements and prerequisites Watch some of Faisal Fareed‘s sessions about Dual-write: DynamicsCon 2020: The Power of Dual-write or Scottish Summit 2021: D365 FO integration with Dataverse – Dual write, Virtual Entities, OR Data Integrator. He’s got some more which you can find on Youtube.  Create the Data-entity In Visual Studio we need to create the entity from our table. I’ve created a new table called AASBookTable with just four fields: BookId, Author, Name and ISBN. Its primary key is the BookId field which is also its alternate key and will be used as natural key in the entity.\nNext, we create the data entity and make sure we’re marking the “Enable data management capabilities” checkbox:\nEnable data management capabilities must be checked\nIf the entity doesn’t have data management enabled it won’t be displayed in the list in the Dual-write tables setup.\nCreate a table in Dataverse Now we need to create a table in our Dataverse environment. This table must have at least some of the fields we want to synchronize to Dataverse AND a company field. The company concept doesn’t exist in Dataverse/CRM but thanks to the OOB mappings and Initial Sync we can solve this with just a few clicks and will have a company table in Dataverse with all our FnO legal entities.\nCompany field related to table company\nFollowing my example I’ve created a table with the same four fields and a company field with the data type Lookup and its related table Company, where the FnO legal entities are synchronized.\nAs I said, if we don’t create this field we won-t be able to setup Dual-write for this table.\nCreate table map Our table and data entity are ready, and now we need to create a mapping between them in the Dual-write workspace in FnO. Click the “Add table map” button:\nDynamics 365 Dual-write add table map\nA new dialog will open and we need to select the FnO entity and the Dataverse table:\nEntity map\nSelect the entity and table we’ve created and click save. Then we can define the field mapping:\nDynamics 365 Dual-write field maps\nBecause I’ve created both it’s clear what to map. And after doing this we can click save and it’s done, right? No! WRONG! If we do just this we’ll get an error, this error:\n Project validation failed. SourceEntitySchema: Books has a primaryCompanyField set to DataAreaId and DestinationEntitySchema: cr008_bookses doesn’t have primary company field set. Dual-write only supports mapping between cross-company entities or company-specific entities from both sides..keys are missing for either AX or CRM.keys are missing for either CRM or AX\n Or we can also get an error regarding a missing integration key for the company field. In the end both are caused because we’ve missed defining the integration key for this new Dataverse table:\nIntegration key Go back to the main Dual-write form and click on the “Integration key” button:\nDual-write integration key\nThe integration key will be the same as the FnO data entity key, plus the company if your data entity has a company context. Remember that when we create indexes in FnO the DataAreaId field isn’t included in the field, but it is in the SQL Server index along the partition field.\nThe integration key for our custom Dual-write mapping will look like this:\nDual/write integration key\nRemember we’ve added the company field to our Dataverse table? You can see in the image above that the field includes the relation to the Company table in Dataverse. We won’t be able to save the field mapping if we create the key using our Dataverse table’s company field instead of its Company table relation, like this:\nDual/write integration key\nSee the difference? In the first image the field reads c008_company.cdm_companycode while in the second one it’s c008_company. If we set the integration key using the field in our table instead of the related table and save the fields’ mapping we’ll get an error saying the company is missing in the key because it’s expecting the relation!\nAction! The table and field mappings are ready, just click run and go create a new book in the FnO form:\nFinance and Operations form\nThen we go to our Dataverse table and check its data…\nDataverse table data\nIt’s there! And of course it’s working in both directions. If I create a record in Dataverse it’ll be created in FnO too. I’ll use the Excel add-in to add a new book:\nDataverse Excel add-in\nAnd after refreshing the form in FnO we can see it there too:\nThis is a really simple example of how we can create a custom table, use it in a data entity and then use this entity in our dual-write setup. It’s something that can be easily done but we need to remember the “company thing”, otherwise this will never work!\nDual-write is even easier to configure nowadays thanks to LCS allowing us to create and link a new Dataverse environment when we deploy a new Finance and Operations environment.\n","permalink":"https://nuxulu.com/posts/2021-05-02-develop-custom-data-entities-for-dual-write/","summary":"Dual-write has been around for almost two years now. It’s one of the ways of integrating Dynamics 365 Finance and Operations and Dataverse along with Virtual Entities.\nThe standard solution comes with many out-of-the-box entities available to synchronize. This has been one of the great improvements since Dual-write was made available in preview, when Juanan and I demoed it in the 2019 Dynamics Saturday in Madrid.\nThis is how Dual write really works","title":"Develop custom Data Entities for Dual-write"},{"content":"Dynamics 365 for Finance \u0026amp; Operations and Azure DevOps Azure DevOps Azure DevOps will be the service we will use for source control. Microsoft Dynamics 365 for Finance and Operations supports TFVC out of the box as its version-control system.\nBut Azure DevOps does not only offer a source control tool. Of course, developers will be the most benefited of using it, but from project management to the functional team and customers, everybody can be involved in using Azure DevOps. BPM synchronization and task creation, team planning, source control, automated builds and releases, are some of the tools it offers. All these changes will need some learning from the team, but in the short-term all of this will help to better manage implementations.\nAs I said it looks like the technical team is the most affected by the addition of source control to Visual Studio, but it’s the most benefited too…\nFirst steps To use all the features described in this guide we need to create an Azure DevOps project and connect it to LCS. This will be the first step and it’s mandatory so let’s see how we have to do everything.\nCreate an Azure DevOps organization You might or might not have to do this. If you or your customer already have an account, you can use it and just create a new project in it. Otherwise head to https://dev.azure.com and create a new organization:\nAzure DevOps sign up\nAfter creating it you need to create a new project with the following options:\nCreate Azure DevOps project\nPress the “Create project” button and you’re done. Now let’s connect this Azure DevOps project to our LCS project.\nWhen a customer signs up for Finance and Operations the LCS project is of type “Implementation project” is created automatically. Your customers need to invite you to their project. If you’re an ISV you can use the “Migrate, create solutions, and learn” projects.\nIn any of both cases you need to go to “Project settings” and select the “Visual Studio Team Services” Tab. Scroll down and you should see two fields. Fill the field with your DevOps URL without the project part. If you got a https://dev.azure.com/YOUR_ORG URL type you need to change it to https://YOUR_ORG.visualstudio.com:\nAzure DevOps setup on LCS\nAnd to get the “Personal access token” we go back to our Azure DevOps project, click on the user settings icon, and then select “Personal access tokens”:\nWe add a new token, set its expiration and give it full access. Finally press the “Create” button and a new dialog will appear with your token, copy it, and paste it in LCS.\nBack to LCS, once you’ve pasted the token press the “Continue” button. On the next step just select your project, press “Continue” and finally “Save” on the last step.\nIf you have any problem you can take a look at the docs where everything is really well documented.\nThe build server Once we’ve linked LCS and Azure DevOps we’ll have to deploy the build server. This will be the heart of our CI/CD processes.\nEven though the build virtual machine has the same topology as a developer box, it really isn’t a developer VM and should never be used as one, do not use it as a developer VM! It has Visual Studio installed in it, the AosService folder with all the standard packages and SQL Server with an AxDB, just like all other developer machines, but that’s not its purpose.\nWe won’t be using any of those features. The “heart” of the build machine is the build agent, an application which Azure DevOps uses to execute the build definition’s tasks from Azure DevOps.\nWe can also use Azure hosted build agents. Azure hosted agents allow us to run a build without a VM, the pipeline runs on Azure. We’ll see this later.\nThe build VM This VM is usually the dev box on Microsoft’s subscription but you can also use a regular cloud-hosted environment as a build VM.\nWhen this VM is deployed there’s two things happening: the basic source code structure and the default build definition are created.\nVisual Studio We have the basics to start working. Log into your dev VM and start Visual Studio, we must map the Main folder to the development machine’s packages folder. Open the team explorer and select “Connect to a Project…”:\nIt will ask for your credentials and then show all projects available with the account you’ve used. Select the project we have created in the steps earlier and click on “Connect”:\nNow open the “Source Control Explorer”, select the Main folder and click on the “Not mapped” text:\nMap the Main folder to the K:\\AosService\\PackagesLocalDirectory folder on your service drive (this could be drive C if you’re using a local VM instead of a cloud-hosted environment):\nWhat we’ve done in this step is telling Visual Studio that what’s in our Azure DevOps project, inside the Main folder, will go into the K:\\AosService\\PackagesLocalDirectory folder of our development VM.\nThe Main folder we have in our source control tree is a regular folder, but we can convert it into a branch if we need it.\nIn the image above, you can see the icon for Main changes when it’s converted to a branch. Branches allow us to perform some actions that aren’t available to folders. Some differences can be seen in the context menu:\nFolder context menu\nBranch context menu\nFor instance, branches can display the hierarchy of all the project branches (in this case it’s only Main and Dev so it’s quite simple).\nProperties dialogs are different too. The folder one:\nAnd the branch one, where we can see the different relationships between the other branches created from Main:\nThis might be not that interesting or useful, but one of the things converting a folder into a branch is seeing where has a changeset been merge into.\nSome advice I strongly recommend moving the Projects folder out of the Main branch (or whatever you call it) into the root of the project, at the same level as BuildProcessTemplates and Trunk. In fact, and this is my personal preference, I would keep anything that’s not code outside of a branch. By doing this you only need to take care of the code when merging and branching.\nThose who have been working with AX for several years were used to not use version-control systems. MSDyn365FO has taken us to uncharted territory, so it is not uncommon for different teams to work in different ways, depending on their experience and what they’ve found in the path. Each team will need to invest some time to discover what’s better for them regarding code, branching and methodologies. Many times, this will be based on experimentation and trial and error, and with the pace of implementation projects trial and error turns out bad.\nBranching strategies I want to make it clear in advance: I’m not an expert in managing code nor Azure DevOps, at all. All that I’ve written here is product of my experience (good and bad) of over 4 years working with Finance and Operations. In this article on branching strategies from the docs there’s more information regarding branching and links to articles of the DevOps team. And there’s even more info in the DevOps Rangers’ Library of tooling and guidance solutions!\nMain-Release One possible strategy is using a Main and a Release branch. We have already learnt that the Main branch is created when the Build VM is deployed. The usual is that in an implementation project all development will be done on that branch until the Go Live, and just before that a new Release branch will be created.\nWe will keep development work on the Main branch, and when that passes validation, we’ll move it to Release. This branching strategy is really simple and will keep us mostly worry-free.\nDev – Main – Release This strategy is similar to the Main – Release one but includes a Dev branch for each developer. This dev branch must be maintained by the developer using it. He can do as many check-ins as he wants during a development, and when it’s done merge all these changes to the Main branch in a single changeset. Of course, this adds some bureaucracy because we also need to forward integrate changes from Main into our Dev branch, but it will allow us to have a cleaner list of changesets when merging them from Main to the Release branch.\nWhatever branching strategy you choose try to avoid having pending changesets to be merged for a long time. The amount of merge conflicts that will appear is directly proportional to the time the changeset has been waiting to be merged.\nI wrote all of this based on my experience. It’s obviously not the same working for an ISV than for an implementation partner. An ISV has different needs, it must maintain different code versions to support all their customers and they don’t necessarily need to work in a Main – Release manner. They could have one (or more) branch for each version. However, since the end of overlayering this is not necessary. More ideas about this can be found in the article linked at the beginning.\nAzure Pipelines Builds We’ve already seen that the default build definition has all the default steps active. We can disable (or remove) all the steps we’re not going to use. For example, the testing steps can be removed if we have no unit testing. We can also create new build definitions from scratch, however it’s easier to clone the default one and modify it to other branches or needs.\nSince version 8.1 all the X++ hotfixes are gone, the updates are applied in a single deployable package as binaries. This implies that the source-controlled Metadata folder will only contain our custom packages and models, no standard packages anymore.\nContinuous Integration Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. (source)\nShould your project/team use CI? Yes, yes, yes. This is one of the key feature of using an automated build process.\nThis is how a build definition for CI that will only compile our codebase looks like:\nOnly the prepare and build steps. Then we need to go to the “Triggers” tab and enable the CI option:\nRight after each developer check-in, a build will be queued, and the code compiled. In case there’s a compilation error we’ll be notified about it. Of course, we all build the solutions before checking them in and don’t need this CI build. Right?\n![tysonjaja](./MSDyn365 \u0026amp; Azure DevOps ALM - ariste.info_files/tysonjaja.gif \u0026ldquo;MSDyn365 \u0026amp; Azure DevOps ALM 23\u0026rdquo;)\nAnd because we all know that “Slow and steady wins the race”, but at some point during a project that’s not possible, so this kind of build definition can help us out. Especially when merging code between branches. This will allow us to be 100% sure when creating a DP to release to production that it’ll work. I can tell you that having to do a release to prod in a hurry and seeing the Main build failing is not nice.\nGated check-ins A gated check-in is a bit different than a CI build. The gated check-in will trigger an automated build BEFORE checking-in the code. If it fails, the changeset is not cheked-in until the errors are fixed and checked-in again.\nThis option might seem perfect for the merge check-ins to the Main branch. I’ve found some issues trying to use it, for example:\n If multiple merges \u0026amp; check-ins from the same development are done and the first fails but the second doesn’t, you’ll still have pending merges to be done. You can try batching the builds, but I haven’t tried that. Issues with error notifications and pending code on dev VMs. If many check-ins are made, you’ll end up with lots of queued builds (and we only have one available agent per DevOps project). This can also be solved using the “Batch changes while a build is in progress”.  I think the CI option is working perfectly to validate code. As I’ve already said several times, choose the strategy that better suits your team and your needs. Experiment with CI and Gated check-in builds and decide what is better for you.\nSet up the new Azure DevOps tasks for Packaging and Model Versioning Almost all the tasks of the default build definition use PowerShell scripts that run on the Build VM. We can change 3 of those steps for newer tasks. In order to use these newer tasks, we need to install the “Dynamics 365 Unified Operations Tools”. We’ll be using them to set up our release pipeline too so consider doing it now.\nUpdate Model Version task This one is the easiest, just add it to your build definition under the current model versioning task, disable the original one and you’re done. If you have any filters in your current task, like excluding any model, you must add the filter in the Descriptor Search Pattern field using Azure DevOps pattern syntax.\nCreate Deployable Package task This task will replace the Generate packages from the current build definitions. To set it up we just need to do a pair of changes to the default values:\nX++ Tools Path This is your build VM’s physical bin folder, the AosService folder is usually on the unit K for cloud-hosted VMs. I guess this will change when we go VM-less to do the builds.\nUpdate!: the route to the unit can be changed for $(ServiceDrive), getting a path like $(ServiceDrive)\\AOSService\\PackagesLocalDirectory\\bin.\nLocation of the X++ binaries to package The task comes with this field filled in as $(Build.BinariesDirectory) but this didn’t work out for our build definitions, maybe the variable isn’t set up on the proj file. After changing this to $(Agent.BuildDirectory)\\Bin the package is generated.\nFilename and path for the deployable package The path on the image should be changed to $(Build.ArtifactStagingDirectory)\\Packages\\AXDeployableRuntime_$(Build.BuildNumber).zip. You can leave it without the Packages folder in the path, but if you do that you will need to change the Path to Publish field in the Publish Artifact: Package step of the definition.\nAdd Licenses to Deployable Package task This task will add the license files to an existing Deployable Package. Remember that the path of the deployable package must be the same as the one in the Create Deployable Package task.\nAzure hosted build for Dynamics 365 Finance \u0026amp; SCM The day we’ve been waiting for has come! The Azure hosted builds are in public preview since PU35!! We can now stop asking Joris when will this be available, because it already is! Check the docs!\nI’ve been able to write this because, thanks to Antonio Gilabert, we’ve been testing this at Axazure for a few months with access to the private preview. And of course thanks to Joris for inviting us to the preview!\nRiding the Azure Pipelines by Caza Pelusas\nWhat does this mean? We no longer need a VM to run the build pipelines! Nah, we still need! If you’re running tests or synchronizing the DB as a part of your build pipeline you still need the VM. But we can move CI builds to the Azure hosted agent!\nYou can also read my full guide on MSDyn365FO \u0026amp; Azure DevOps ALM.\nRemember this is a public preview. If you want to join the preview you first need to be part of the Dynamics 365 Insider Program where you can join the “Dynamics 365 for Finance and Operations Insider Community“. Once invited you should see a new LCS project called PEAP Assets, and inside its Asset Library you’ll find the nugets in the Nuget package section.\nAzure agents With the capacity to run an extra Azure-hosted build we get another agent to run a pipeline and can run multiple pipelines at the same time. But it still won’t be parallel pipelines, because we only get one VM-less agent. This means we can run a self-hosted and azure hosted pipeline at the same time, but we cannot run two of the same type in parallel. If we want that we need to purchase extra agents.\nWith a private Azure DevOps project we get 2GB of Artifacts space (we’ll see that later) and one self-hosted and one Microsoft hosted agent with 1800 free minutes:\nAzure hosted build: Azure DevOps project pricing\nWe’ll still keep the build VM, so it’s difficult to tell a customer we need to pay extra money without getting rid of its cost. Plus we’ve been doing everything with one agent until now and it’s been fine, right? So take this like extra capacity, we can divide the build between both agents and leave the MS hosted one for short builds to squeeze the 1800 free minutes as much as possible.\nHow does it work? There’s really no magic in this. We move from a self-hosted agent in the build VM to a Microsoft-hosted agent.\nThe Azure hosted build relies on nuget packages to compile our X++ code. The contents of the PackagesLocalDirectory folder, platform and the compiler tools have basically been put into nugets and what we have in the build VM is now on 3 nugets.\nWhen the build runs it downloads \u0026amp; installs the nugets and uses them to compile our code on the Azure hosted build along the standard packages.\nWhat do I need? To configure the Azure hosted build we need:\n  The 3 nuget packages from LCS: Compiler tools, Platform X++ and Application X++.\n  nuget.exe\n  A user with rights at the organization level to upload the nugets to Azure DevOps.\n  Some patience to get everything running 🙂\n  So the first step is going to the PEAP LCS’ Asset Library and downloading the 3 nuget packages:\nNugets for the Azure Hosted Build\nAzure DevOps artifact All of this can be done on your PC or in a dev VM, but you’ll need to add some files and a VS project to your source control so you need to use the developer box for sure.\nHead to your Azure DevOps project and go to the Artifacts section. Here we’ll create a new feed and give it a name:\nYou get 2GB for artifacts, the 3 nuget packages’ size is around 500MB, you should have no issues with space unless you have other artifacts in your project.\nNow press the “Connect to feed” button and select nuget.exe. You’ll find the instructions to continue there but I’ll explain it anyway.\nThen you need to download nuget.exe and put it in the Windows PATH. You can also get the nugets and nuget.exe in the same folder and forget about the PATH. Up to you. Finally, install the credential provider: download this Powershell script and run it. If the script keeps asking for your credentials and fails try adding -AddNetfx as a parameter. Thanks to Erik Norell for finding this and sharing in the comments of the original post!\nCreate a new file called nuget.config in the same folder where you’ve downloaded the nugets. It will have the content you can see in the “Connect to feed” page, something like this:\n\u0026lt;?xml version\\=\u0026#34;1.0\u0026#34; encoding\\=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;configuration\\\u0026gt; \u0026lt;packageSources\\\u0026gt; \u0026lt;clear /\\\u0026gt; \u0026lt;add key\\=\u0026#34;AASBuild\u0026#34; value\\=\u0026#34;https://pkgs.dev.azure.com/aariste/aariste365FO/\\_packaging/AASBuild/nuget/v3/index.json\u0026#34; /\\\u0026gt; \u0026lt;/packageSources\\\u0026gt; \u0026lt;/configuration\\\u0026gt; This file’s content has to be exactly the same as what’s displayed in your “Connect to feed” page.\nAnd finally, we’ll push (upload) the nugets to our artifacts feed. We have to do this for each one of the 3 nugets we’ve downloaded:\nnuget.exe push -Source \u0026#34;AASBuild\u0026#34; -ApiKey az \u0026lt;packagePath\u0026gt; You’ll get prompted for the user. Remember it needs to have enough rights on the project.\nOf course, you need to change “AASBuild” for your artifact feed name. And we’re done with the artifacts.\nPrepare Azure DevOps This new agent needs a solution to build our packages. This means we have to create an empty solution in Visual Studio and set the package of the project to our main package. Like this:\nVisual Studio solution\nIf you have more than one package or models, you need to add a project to this solution for each separate model you have.\nWe have to create another file called packages.config with the following content:\n\u0026lt;?xml version\\=\u0026#34;1.0\u0026#34; encoding\\=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;packages\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;7.0.5644.16778\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Application.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;10.0.464.13\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.CompilerPackage\u0026#34; version\\=\u0026#34;7.0.5644.16778\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\u0026gt; \u0026lt;/packages\u0026gt; The version tag will depend on when you’re reading this, but the one above is the correct one for PU35. We’ll need to update this file each time a new version of the nugets is published.\nAnd, to end with this part, we need to add the solution, the nuget.config and the packages.config files to TFVC. This is what I’ve done:\nAzure DevOps\nYou can see I’ve created a Build folder in the root of my DevOps project. That’s only my preference, but I like to only have code in my branches, even the projects are outside of the branches, I only want the code to move between merges and branches. Place the files and solution inside the Build folder (or wherever you decide).\nConfigure pipeline Now we need to create a new pipeline, you can just import this template from the newly created X++ (Dynamics 365) Samples and Tools Github project. After importing the template we’ll modify it a bit. Initially, it will look like this:\nAzure hosted build: Default imported pipeline\nAs you can see the pipeline has all the steps needed to generate the DP, but some of them, the ones contained in the Dynamics 365 tasks, won’t load correctly after the import. You just need to add those steps to your pipeline manually and complete its setup.\nPipeline root You need to select the Hosted Azure Pipelines for the Agent pool, and vs2017-win2016 as Agent Specification.\nGet sources Azure hosted build: Our mappings\nI’ve mapped 2 things here: our codebase in the first mapping and the Build folder where I’ve added the solution and config files. If you’ve placed these files inside your Metadata folder you don’t need the extra mapping.\nNuGet install Packages This step gets the nugets from our artifacts feeds and the installs to be used in each pipeline execution.\nAzure hosted build: nuget install\nThe command uses the config files we have uploaded to the Build folder, and as you can see it’s fetching the files from the $(build.sourcesDirectory)\\Build directory we’ve configured in the Get sources step. If you’ve placed those files in a diferent place you need to change the paths as needed.\nUpdate Model Version This is one of the steps that are displaying issues even though I got the Dynamics 365 tools installed from the Azure DevOps marketplace. If you got it right you probably don’t need to change anything. If you have the same issue as me, just add a new step and select the “Update Model Version” task and change the fields so it looks like this:\nAzure hosted build: Update Model Version\nBuild solution Build solution step\nIn the build solution step, you have a wildcard in the solution field: **\\\\*.sln. If you leave this wildcard it will build all the projects you have in the repo and, depending on the number of projects you have, the build could time out.\nI solve this by selecting a solution, that contains all the models I have, that I have placed in the Build folder in my repo, and update that solution if you add or remove any model.\nThanks to Ievgen Miroshnikov for pointing this out!\nThere could be an additional issue with the rnrproj files as Josh Williams points out in a comment. If your project was created pre-PU27 try creating a new solution to avoid problems.\nCreate Deployable Package This is another one of the steps that are not loading correctly for me. Again, add it and change as needed:\nAzure hosted build: Create Deployable Package\nAdd Licenses to Deployable Package Another step with issues. Do the same as with the others:\nAzure hosted build: Add Licenses to Deployable Package\nAnd that’s all. You can queue the build to test if it’s working. For the first runs you can disable the steps after the “Build solution” one to see if the nugets are downloaded correctly and your code built. After that try generating the DP and publishing the artifact.\nYou’ve configured your Azure hosted build, now it’s your turn to decide in which cases will you use the self-hosted or the azure hosted build.\nUpdate for version 10.0.18 Since version 10.0.18 we’ll be getting 4 NuGet packages instead of 3 because of the Microsoft.Dynamics.AX.Application.DevALM.BuildXpp NuGet size is getting near or over the max size which is 500MB and will come as 2 NuGet packages from now on.\nYou can read about this in the docs.\nThere just 2 small changes we need to do to the pipeline if we’re already using it, one to the packages.config file and another one to the pipeline.\npackages.config The packages.config file will have an additional line for the Application Suite NuGet.\n\u0026lt;?xml version\\=\u0026#34;1.0\u0026#34; encoding\\=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;packages\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;7.0.5968.16973\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Application.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;10.0.793.16\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.ApplicationSuite.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;10.0.793.16\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.CompilerPackage\u0026#34; version\\=\u0026#34;7.0.5968.16973\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;/packages\\\u0026gt; Pipeline We need to add a new variable to the pipeline variables called AppSuitePackage with the value Microsoft.Dynamics.AX.ApplicationSuite.DevALM.BuildXpp.\nNew Azure DevOps pipeline variable\nAnd then use it in the build step and change it to:\n/p:BuildTasksDirectory\\=\u0026#34;$(NugetsPath)\\\\$(ToolsPackage)\\\\DevAlm\u0026#34; /p:MetadataDirectory\\=\u0026#34;$(MetadataPath)\u0026#34; /p:FrameworkDirectory\\=\u0026#34;$(NuGetsPath)\\\\$(ToolsPackage)\u0026#34; /p:ReferenceFolder\\=\u0026#34;$(NuGetsPath)\\\\$(PlatPackage)\\\\ref\\\\net40;$(NuGetsPath)\\\\$(AppPackage)\\\\ref\\\\net40;$(MetadataPath);$(Build.BinariesDirectory);$(NuGetsPath)\\\\$(AppSuitePackage)\\\\ref\\\\net40\u0026#34; /p:ReferencePath\\=\u0026#34;$(NuGetsPath)\\\\$(ToolsPackage)\u0026#34; /p:OutputDirectory\\=\u0026#34;$(Build.BinariesDirectory)\u0026#34; Azure DevTest Labs powered builds The end of Tier-1 Microsoft-managed build VMs is near, and this will leave us without the capacity to synchronize the DB or run tests in a pipeline, unless we deploy a new build VM in our, or our customer’s, Azure subscription. Of course, there might be a cost concern with it, and there’s where Azure DevTest Labs can help us!\nThis post has been written thanks to Joris de Gruyter‘s session in the past DynamicsCon: Azure Devops Automation for Finance and Operations Like You’ve Never Seen! And there’s also been some investigation and (a lot of) trial-and-error from my side until everything has been working.\nConfiguring the build VM in Azure DevTest Labs\nIf you want to know more about builds, releases, and the Dev ALM of Dynamics 365 you can read my full guide on MSDyn365 \u0026amp; Azure DevOps ALM.\nBut first… What I’m showing in this post is not a perfect blueprint. There’s a high probability that if you try exactly the same as I do here, you won’t get the same result. But it’s a good guide to get started and do some investigation on your own and learn.\nAzure DevTest Labs Azure DevTest Labs is an Azure tool/service that allows us to deploy virtual machines and integrate them with Azure DevOps pipelines, and many other things, but what I’m going to explain is just the VM and pipeline part.\nWhat will I show in this post? How to prepare a Dynamics 365 Finance and Operations VHD image to be used as the base to create a build virtual machine from an Azure DevOps pipeline, build our codebase, synchronize the DB, run tests, even deploy the reports, generate the deployable package and delete the VM.\nGetting and preparing the VHD This is by far the most tedious part of all the process because you need to download 11 ZIP files from LCS’ Shared Asset Library, and we all know how fast things download from LCS.\nHow is LCS download speed?\nAnd to speed it up we can create a blob storage account on Azure and once more turn to Mötz Jensen‘s d365fo.tools and use the Invoke-D365AzCopyTransfer cmdlet. Just go to LCS, click on the “Generate SAS link” button for each file, use it as the source parameter in the command and your blob SAS URL as the destination one. Once you have all the files in your blob you can download them to your local PC at a good speed.\nOnce you’ve unzipped the VHD you need to change it from Dynamic to Fixed using this PowerShell command:\nConvert-VHD –Path VHDLOCATION.vhd –DestinationPath NEWVHD.vhd –VHDType Fixed The reason is you can’t create an Azure VM from a dynamically-sized VHD. And it took me several attempts to notice this 🙂\nCreate a DevTest Labs account To do this part you need an Azure account. If you don’t have one you can sign up for a free Azure account with a credit of 180 Euros (200 US Dollars) to be spent during 30 days, plus many other free services during 12 months.\nSearch for DevTest Labs in the top bar and create a new DevTest Lab. Once it’s created open the details and you should see something like this:\nAzure DevTest Labs\nClick on the “Configuration and policies” menu item at the bottom of the list and scroll down in the menu until you see the “Virtual machine bases” section:\nDevTest Labs custom image\nAnd now comes the second funniest part of the process: we need to upload the 130GB VHD image to a blob storage account! So, click the “Add” button on top and in the new dialog that will open click the “Upload a VHD using PowerShell”. This will generate a PowerShell script to upload the VHD to the DevTest Labs blob. For example:\n\u0026lt;# Generated script to upload a local VHD to Azure. WARNING: The destination will be publicly available for 24 hours, after which it will expire. Ensure you complete your upload by then. Run the following command in a Azure PowerShell console after entering the LocalFilePath to your VHD. #\u0026gt; Add-AzureRmVhd -Destination \u0026#34;https://YOURBLOB.blob.core.windows.net/uploads/tempImage.vhd?sv=2019-07-07\u0026amp;st=2020-12-27T09%3A08%3A26Z\u0026amp;se=2020-12-28T09%3A23%3A26Z\u0026amp;sr=b\u0026amp;sp=rcw\u0026amp;sig=YTeXpxpVEJdSM7KZle71w8NVw9oznNizSnYj8Q3hngI%3D\u0026#34; -LocalFilePath \u0026#34;\u0026lt;Enter VHD location here\u0026gt;\u0026#34; Generated script to upload a local VHD to Azure.\nWARNING: The destination will be publicly available for 24 hours, after which it will expire.\nEnsure you complete your upload by then.\nRun the following command in a Azure PowerShell console after entering\nthe LocalFilePath to your VHD.\nAdd-AzureRmVhd \\-Destination \u0026#34;https://YOURBLOB.blob.core.windows.net/uploads/tempImage.vhd?sv=2019-07-07\u0026amp;st=2020-12-27T09%3A08%3A26Z\u0026amp;se=2020-12-28T09%3A23%3A26Z\u0026amp;sr=b\u0026amp;sp=rcw\u0026amp;sig=YTeXpxpVEJdSM7KZle71w8NVw9oznNizSnYj8Q3hngI%3D\u0026#34; \\-LocalFilePath \u0026#34;\u0026lt;Enter VHD location here\u0026gt;\u0026#34; DevTest Labs custom image upload\nAn alternative to this is using the Azure Storage Explorer as you can see in the image on the left.\nYou should upload the VHD to the uploads blob.\nAny of these methods is good to upload the VHD and I don’t really know which one is faster.\nOnce the VHD is uploaded open the “Custom images” option again and you should see the VHD in the drop-down:\nDevTest Labs custom image\nGive the image a name and click OK.\nWhat we have now is the base for a Dynamics 365 Finance and Operations dev VM which we need to prepare to use it as a build VM.\nCreating the VM We’ve got the essential, a VHD ready to be used as a base to create a virtual machine in Azure. Our next step is finding a way to make the deployment of this VM predictable and automated. We will attain this thanks to Azure ARM templates.\nGo back to your DevTest Labs overview page and click the “Add” button, on the “Choose base” page select the base you’ve just created, and on the next screen click on the “Add or Remove Artifacts” link:\nAdd artifacts to the VM\nSearch for WinRM, select “Configure WinRM”, and on the next screen enter “Shared IP address” as the hostname box and click “Add”.\nNote: if when the VM runs the artifacts can’t be installed check whether the Azure VM Agent is installed on the base VHD. Thanks to Joris for pointing this out!\nConfigure Azure DevOps Agent Service Option A: use an artifact Update: thanks to Florian Hopfner for reminding me this because I forgot… If you choose Option A to install the agent service you need to do some things first!\nThe first thing we need to do is running some PowerShell scripts that create registry entries and environment variables in the VM, go to C:\\DynamicsSDK and run these:\nImport-Module $(Join-Path \\-Path \u0026#34;C:\\\\DynamicsSDK\u0026#34; \\-ChildPath \u0026#34;DynamicsSDKCommon.psm1\u0026#34;) \\-Function \u0026#34;Write-Message\u0026#34;, \u0026#34;Set-AX7SdkRegistryValues\u0026#34;, \u0026#34;Set-AX7SdkEnvironmentVariables\u0026#34; Set\\-AX7SdkEnvironmentVariables \\-DynamicsSDK \u0026#34;C:\\\\DynamicsSDK\u0026#34; Set\\-AX7SdkRegistryValues \\-DynamicsSDK \u0026#34;c:\\\\DynamicsSDK\u0026#34; \\-TeamFoundationServerUrl \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-AosWebsiteName $AosWebsiteName \u0026#34;AosService\u0026#34; The first one will load the functions and make them available in the command-line and the other two create the registry entries and environment variables.\nNow we need to add an artifact for the Azure DevOps agent service. This will configure the agent service on the VM each time the VM is deployed. Search for “Azure Pipelines Agent” and click it. You will see this:\nDevTest Labs Azure DevOps Agent\nWe need to fill some information:\nOn “Azure DevOps Organization Name” you need to provide the name of your organization. For example if your AZDO URL is https://dev.azure.com/blackbeltcorp you need to use blackbeltcorp.\nOn “AZDO Personal Access Token” you need to provide a token generated from AZDO.\nOn “Agent Name” give your agent a name, like DevTestAgent. And on “Agent Pool” a name for your pool, a new like DevTestPool or an existing one as Default.\nOn “Account Name” use the same user that we’ll use in our pipeline later. Remember this. And on “Account Password” its password. Using secrets with a KeyVault is better, but I won’t explain this here.\nAnd, finally, set “Replace Agent” to true.\nOption B: Configure Azure DevOps Agent in the VM To do this you have to create a VM from the base image you created before and then go to C:\\DynamicsSDK and run the SetupBuildAgent script with the needed parameters:\nSetupBuildAgent.ps1 \\-VSO\\_ProjectCollection \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-ServiceAccountName \u0026#34;myUser\u0026#34; \\-ServiceAccountPassword \u0026#34;mYPassword\u0026#34; \\-AgentName \u0026#34;DevTestAgent\u0026#34; \\-AgentPoolName \u0026#34;DevTestPool\u0026#34; \\-VSOAccessToken \u0026#34;YOUR\\_VSTS\\_TOKEN\u0026#34; WARNING: If you choose option B you must create a new base image from the VM where you’ve run the script. Then repeat the WinRM steps to generate the new ARM template which we’ll see next.\nARM template Then go to the “Advanced Settings” tab and click the “View ARM template” button:\nGet the ARM template\nThis will display the ARM template to create the VM from our pipeline. It’s something like this:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;newVMName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste001\u0026#34; }, \u0026#34;labName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aristeinfo\u0026#34; }, \u0026#34;size\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard\\_B4ms\u0026#34; }, \u0026#34;userName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;myUser\u0026#34; }, \u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\\[\\[\\[VmPassword\\]\\]\u0026#34; }, \u0026#34;Configure\\_WinRM\\_hostName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Public IP address\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;ariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_poolName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestPool\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;C\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: true } }, \u0026#34;variables\u0026#34;: { \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[concat(variables(\u0026#39;labVirtualNetworkName\u0026#39;), \u0026#39;Subnet\u0026#39;)\\]\u0026#34;, \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/virtualnetworks\u0026#39;, parameters(\u0026#39;labName\u0026#39;), variables(\u0026#39;labVirtualNetworkName\u0026#39;))\\]\u0026#34;, \u0026#34;labVirtualNetworkName\u0026#34;: \u0026#34;\\[concat(\u0026#39;Dtl\u0026#39;, parameters(\u0026#39;labName\u0026#39;))\\]\u0026#34;, \u0026#34;vmId\u0026#34;: \u0026#34;\\[resourceId (\u0026#39;Microsoft.DevTestLab/labs/virtualmachines\u0026#39;, parameters(\u0026#39;labName\u0026#39;), parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;\\[concat(parameters(\u0026#39;labName\u0026#39;), \u0026#39;/\u0026#39;, parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34; }, \u0026#34;resources\u0026#34;: \\[ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-15-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.DevTestLab/labs/virtualmachines\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmName\u0026#39;)\\]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;\\[resourceGroup().location\\]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[variables(\u0026#39;labVirtualNetworkId\u0026#39;)\\]\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;Dynamics365FnO10013AgentLessV2\u0026#34;, \u0026#34;customImageId\u0026#34;: \u0026#34;/subscriptions/6715778f-c852-453d-b6bb-907ac34f280f/resourcegroups/devtestlabs365/providers/microsoft.devtestlab/labs/devtestd365/customimages/dynamics365fno10013agentlessv2\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;\\[parameters(\u0026#39;size\u0026#39;)\\]\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;\\[parameters(\u0026#39;userName\u0026#39;)\\]\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\\[parameters(\u0026#39;password\u0026#39;)\\]\u0026#34;, \u0026#34;isAuthenticationWithSshKey\u0026#34;: false, \u0026#34;artifacts\u0026#34;: \\[ { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-winrm\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;hostName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Configure\\_WinRM\\_hostName\u0026#39;)\\]\u0026#34; } \\] }, { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-vsts-build-agent\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;vstsAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vstsPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentNameSuffix\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;poolName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_poolName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;RunAsAutoLogon\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;driveLetter\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;workDirectory\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;replaceAgent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#39;)\\]\u0026#34; } \\] } \\], \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[variables(\u0026#39;labSubnetName\u0026#39;)\\]\u0026#34;, \u0026#34;disallowPublicIpAddress\u0026#34;: true, \u0026#34;storageType\u0026#34;: \u0026#34;Premium\u0026#34;, \u0026#34;allowClaim\u0026#34;: false, \u0026#34;networkInterface\u0026#34;: { \u0026#34;sharedPublicIpAddressConfiguration\u0026#34;: { \u0026#34;inboundNatRules\u0026#34;: \\[ { \u0026#34;transportProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389 } \\] } } } } \\], \u0026#34;outputs\u0026#34;: { \u0026#34;labVMId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmId\u0026#39;)\\]\u0026#34; } } } NOTE: if you’re using option B you won’t have the artifact node for the VSTS agent.\nThis JSON file will be used as the base to create our VMs from the Azure DevOps pipeline. This is known as Infrastructure as Code (IaC) and it’s a way of defining our infrastructure in a file as it were code. It’s another part of the DevOps practice that should solve the “it works on my machine” issue.\nIf we take a look to the JSON’s parameters node there’s the following information:\n newVMName and labName will be the name of the VM and the DevTest Labs lab we’re using. The VM name is not really important because we’ll set the name later in the pipeline. size is the VM size, a D3 V2 in the example above, but we can change it and will do it later. userName \u0026amp; passWord will be the credentials to access the VM and must be the same we’ve used to configure the Azure DevOps agent. Configure_WinRM_hostName is the artifact we added to the VM template that will allow the pipelines to run in this machine.  To do it faster and for demo purposes I’m using a plain text password in the ARM template, changing the password node to something like this:\n\u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;yourPassword\u0026#34; }, I will do the same with all the secureString nodes, but you shouldn’t and should instead use an Azure KeyVault which comes with the DevTest Labs account.\nOf course you would never upload this template to Azure DevOps with a password in plain text. There’s plenty of resources online that teach how to use parameters, Azure KeyVault, etc. to accomplish this, for example this one: 6 Ways Passing Secrets to ARM Templates.\nOK, now grab that file and save it to your Azure DevOps repo. I’ve created a folder in my repo’s root called ARM where I’m saving all the ARM templates:\nARM templates on Azure DevOps\nPreparing the VM The VHD image you download can be used as a developer VM with no additional work, just run Visual Studio, connect it to your AZDO project and done. But if you want to use it as a build box you need to do several things first.\nRemember that the default user and password for these VHDs are Administrator and Pass@word1.\nDisable services First of all we will stop and disable services like the Batch, Management Reporter, SSAS, SSIS, etc. Anything you see that’s not needed to run a build.\nCreate a new SQL user Open SSMS (as an Administrator) and create a new SQL user as a copy of the axdbadmin one. Then open the web.config file and update the DB user and password to use the one you’ve just created.\nPrepare SSRS (optional) If you want to deploy reports as part of your build pipeline you need to go to SSMS again (and as an Admin again), and open a new query in the reporting DB to execute the following query:\nexec DeleteEncryptedContent PowerShell Scripts The default build definition that runs on a build VM uses several PowerShell scripts to run some tasks. I’m adding an additional script called PrepareForAgent.\nThe scripts can be found in the C:\\DynamicsSDK folder of the VM.\nPrepareForBuild This script comes with the VM and we need to modify it to avoid one thing: the PackagesLocalDirectory backup which is usually done in the first build. We need to get rid of this or we’ll waste around an hour per run until the files are copied.\nWe don’t need this because our VM will be new each time we run the pipeline!\nSo open the script, go to line 696 and look for this piece of code:\n# Create packages backup (if it does not exist). $NewBackupCreated \\= Backup-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation # Restore packages backup (unless a new backup was just created). if (!$NewBackupCreated) { Restore-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation \\-RestoreAllFiles:$RestorePackagesAllFiles } if (!$DatabaseBackupToRestore) { $DatabaseBackupPath \\= Get-BackupPath \\-Purpose \u0026#34;Databases\u0026#34; Backup-AX7Database \\-BackupPath $DatabaseBackupPath } else { # Restore a database backup (if specified). Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We need to modify it until we end up with this:\nif ($DatabaseBackupToRestore) { Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We just need the DB restore part and skip the backup, otherwise we’ll be losing 45 minutes in each run for something we don’t need because the VM will be deleted when the build is completed.\nOptional (but recommended): install d365fo.tools Just run this:\nInstall-Module -Name d365fo.tools We can use the tools to do a module sync, partial sync or deploy just our reports instead of all.\nCreate a new image Once we’ve done all these prepare steps we can log out of this VM and stop it. Do not delete it yet! Go to “Create custom image”, give the new image a name, select “I have not generalized this virtual machine” and click the “OK” button.\nThis will generate a new image that you can use as a base image with all the changes you’ve done to the original VHD.\nAzure DevOps Pipelines We’re ready to setup our new build pipeline in Azure DevOps. This pipeline will consist of three steps: create a new VM, run all the build steps, and delete the VM:\nFirst of all check that your pipeline runs on Azure pipelines (aka Azure-hosted):\nDevTest Labs Azure Pipelines\nThe create and delete steps will run on the Azure Pipelines pool. The build step will run on our DevTestLabs pool, or the name you gave it when configuring the artifact on DevTest Labs or the script on the VM.\nCreate Azure DevTest Labs VM Create a new pipeline and choose the “Use the classic editor” option. Make sure you’ve selected TFVC as your source and click “Continue” and “Empty job”. Add a new task to the pipeline, look for “Azure DevTest Labs Create VM”. We just need to fill in the missing parameters with our subscription, lab, etc.\nCreate VM Azure DevTest Labs\nRemember this step must run on the Azure-hosted pipeline.\nBuild This is an easy one. Just export a working pipeline and import it. And this step needs to run on your self-hosted pool:\nRuns on self-hosted pool\nOptional: use SelectiveSync (not recommended, see next option) You can replace the Database Sync task for a PowerShell script that will only sync the tables in your models:\nSelectiveSync.ps1\nThanks Joris for the tip!\nOptional: use d365fo.tools to sync your packages/models This is a better option than the SelectiveSync above. You can synchronize your packages or models only to gain some time. This cmdlet uses sync.exe like Visual Studio does and should be better than SelectiveSync.\nAdd a new PowerShell task, select Inline Script and this is the command:\nInvoke-D365DbSyncModule -Module \u0026#34;Module1\u0026#34;, \u0026#34;Module2\u0026#34; -ShowOriginalProgress -Verbose Optional: use d365fo.tools to deploy SSRS reports If you really want to add the report deployment step to your pipeline you can save some more extra time using d365fo.tools and just deploy the reports in your models like we’ve done with the DB sync.\nRun this in a new PowerShell task to do it:\nPublish-D365SsrsReport -Module YOUR\\_MODULE -ReportName \\* Delete Azure DevTest Labs VM It’s almost the same as the create step, complete the subscription, lab and VM fields and done:\nDelete VM\nAnd this step, like the create one, will run on the Azure-hosted agent.\nDependencies and conditions When all three steps are configured we need to add dependencies and conditions to some of them. For example, to make sure that the delete VM step runs when the build step fails, but it doesn’t when the create VM step fails.\nBuild The build step depends on the create VM step, and will only run if the previous step succeeds:\nBuild step dependencies and conditions\nDelete VM The delete step depends on all previous steps and must run when the create VM step succeeds. If the create step fails there’s no VM and we don’t need to delete it:\nDependencies and conditions on delete VM step\nThis is the custom condition we’ll use:\nand(always(), eq(dependencies.Job\\_1.status, \u0026#39;Succeeded\u0026#39;)) If you need to know your first step’s job name just export the pipeline to YAML and you’ll find it there:\nExport pipeline to YAML\nJob name on YAML\nIf this step fails when the pipeline is run, wait to delete the VM manually, first change the VM name in the delete step, save your pipeline and then use the dropdown to show the VMs in the selected subscription, and save the pipeline.\nRun the build And, I think, we’re done and ready to run our Azure DevTest Labs pipeline for Dynamics 365 Finance and Operations… click “Run pipeline” and wait…\nTadaaaa!!\nTimes The pipeline from the image above is one with real code from a customer but I can’t compare the times with the Azure-hosted builds because there’s no sync, or tests there. Regarding the build time the Azure-hosted takes one minute less, but it needs to install the nugets first.\nBut for example this is a comparison I did:\nAzure DevTest Labs B2ms vs B4ms\nIt takes around 1 hour to create the VM, build, do a full DB synch, deploy reports, run tests, generate a Deployable Package and, finally, delete the VM:\nIf you skip deploying the SSRS reports your build will run in 15 minutes less, that’s around 45 minutes.\nIf you use the partial sync process instead of a full DB sync it’ll be 5-7 minutes less.\nThis would leave us with a 35-40 minutes build.\nComparison 1 No DB Sync\nThe image above shows a simple package being compiled, without any table, so the selective sync goes really fast. The build times improve with VM size.\nComparison 2 Same code Full DB Sync\nThis one is compiling the same codebase but is doing a full DB sync. The sync time improves in the B4ms VM compared to the B2ms but it’s almost the same in the B8ms. Build times are better for larger VM sizes.\nComparison 3 Real code + full sync\nAnd in the image above we see a more realistic build. The codebase is larger and we’re doing a full DB sync.\nSimilar as the comparison before there a good enhancement between a B2ms and a B4ms, but not between a B4ms and B8ms.\nShow me the money! I think this is the interesting comparison. How did a Tier-1 MS-hosted build VM cost? Around 400€? How does it compare to using the Azure DevTest Labs alternative?\nThere’s only one fix cost when using Azure DevTest Labs: the blob storage where the VHD is uploaded. The VHD’s size is around 130GB and this should have a cost of, more or less, 5 euros/month. Keep in mind that you need to clean up your custom images when yours is prepared, the new ones are created as snapshots and also take space in the storage account.\nThen we have the variable costs that come with the deployment of a VM each build but it’s just absurd. Imagine we’re using a B4ms VM, with a 256GB Premium SSD disk, we would pay 0.18€/hour for the VM plus the proportional part of 35.26€/month of the SSD disk, which would be like 5 cents/hour?\nBut this can run on a B2ms VM too which is half the compute price of the VM, down to 9 cents per hour.\nIf we run this build once a day each month, 30 times, the cost of a B4ms would be like… 7€? Add the blob storage and we’ll be paying 12€ per month to run our builds with DB sync and tests.\nIs it cheaper than deploying a cloud-hosted environment, and starting and stopping it using the new d365fo.tools Cmdlets each time we run the build? Yes it is! Because if we deploy a CHE we’ll be paying the price of the SSD disk for the whole month!\nSome final remarks  I have accomplished this mostly through trial-and-error. There’s lots of enhancements and best practices to be applied to all the process, specially using an Azure Key Vault to store all the secrets to be used in the Azure DevOps Agent artifact and the pipeline. This in another clear example that X++ developers need to step outside of X++ and Dynamics 365 FnO. We’re not X++ only developers anymore, we’re very lucky to be working on a product that is using Azure. I’m sure there’s scenarios where using DevTest Labs to create a build VM is useful. Maybe not for an implementation partner, but maybe it is for an ISV partner. It’s just an additional option. The only bad thing to me is that we need to apply the version upgrades manually to the VHDs because they’re published only twice a year. As I said at the beginning of the post, it may have worked to me with all these steps, but if you try you maybe need to change some things. But it’s a good way to start.  Add and build .NET projects I bet that most of us have had to develop some .NET class library to solve something in Dynamics 365 Finance and Operations. You create a C# project, build it, and add the DLL as a reference in your FnO project. Don’t do that anymore! You can add the .NET project to source control, build it in your pipeline, and the DLL gets added to the deployable package!\nI’ve been trying this during the last days after a conversation on Yammer, and while I’ve managed to build .NET and X++ code in the same pipeline, I’ve found some issues or limitations.\nBuild .NET in your pipeline Note: what I show in this post is done using the Azure-hosted pipeline but it should also be possible to do it using a self-hosted agent (aka old build VM).\nThe build step of the pipeline invokes msbuild.exe which can build .NET code. If we check the logs of the build step we will see it:\nmsbuild.exe builds C# projects and our X++ ones too!\nRemember that X++ is part of the .NET family after all… a second cousin or something like it.\nBuild folder\nIf you’ve read the blog post about Azure-hosted builds you must’ve seen I’m putting the solution that references all my models in a folder called Build at the root of my source control tree (left image).\nThat’s just a personal preference that helps me keep the .config files and the solution I use to build all the models in a single, separate place.\nBy using a solution and pointing the build process to use it I also keep control of what’s being built in a single place.\nAdd a C# project to FnO Our first step will usually be creating a Finance and Operations project. Once it’s created we right-click on the solution and select “Add new project”. Then we select a Visual C# Class Library project:\nC# project in Dynamics 365\nNow we should have a solution with a FnO Project and a C# project (right image).\nTo demo this I’ll create a class called Calculator with a single method that accepts two decimal values as parameters and returns it’s sum. An add method.\npublic class Calculator { public decimal Add(decimal a, decimal b) { return a + b; } } public class Calculator { public decimal Add(decimal a, decimal b) { return a + b; } } Now compile the C# project alone, not the whole solution. This will create the DLL in the bin folder of the project. We have to do this before adding the C# project as a reference to the FnO project.\nRight click on the References node of the FnO project and select “Add Reference…”:\nAdd reference to FnO project\nA window will open and you should see the C# project in the “Projects” tab:\nAdd C# project reference to FnO project\nSelect it and click the Ok button. That will add the C# project as a reference to our FnO project, but we still need to do something or this won’t compile in our pipeline. We have to manually add the reference to the project that has been created in the AOT. So, right-click on the reference and select “Add to source control”:\nAdd the reference to source control\nIn the FnO project add a Runnable Class, we’ll call the C# library there:\nusing AASBuildNetDemoLibrary; class AASBuildNetTest { public static void main(Args \\_args) { var calc = new Calculator(); calc.Add(4, 5); } } using AASBuildNetDemoLibrary; class AASBuildNetTest { public static void main(Args \\_args) { var calc \\= new Calculator(); calc.Add(4, 5); } } Add the solution to source control if you haven’t, make sure all the objects are also added and check it in.\nBuild pipeline If I go to my Azure DevOps repo we’ll see the following:\nProjects and objects\nYou can see I’ve checked-in the solution under the Build folder, as I said earlier this is my personal preference and I do that to keep the solutions I’ll use to build the code under control.\nIn my build pipeline I make sure I’m using this solution to build the code:\nBuild Dynamics 365 solution\nRun the pipeline and when it’s done you can check the build step and you’ll see a line that reads:\nCopying file from \u0026#34;D:\\\\a\\\\9\\\\s\\\\Build\\\\AASBuildNetDemo\\\\AASBuildNetDemoLibrary\\\\bin\\\\Debug\\\\AASBuildNetDemoLibrary.dll\u0026#34; to \u0026#34;D:\\\\a\\\\9\\\\b\\\\AASDemo\\\\bin\\\\AASBuildNetDemoLibrary.dll\u0026#34;. And if you download the DP, unzip it, navigate to AOSService\\Packages\\files and unzip the file in there, then open the bin folder, you’ll see our library’s DLL there:\nVictory!\nThings I don’t like/understand/need to investigate I’ve always done this with a single solution and only one C# project. I have some doubts about how this will work with many C# projects, models, solutions, etc.\nFor example, if a model has a dependency on the DLL but it’s built before the DLL the build will fail. I’m sure there’s a way to set an order to solve dependencies like there is for FnO projects within a solution.\nOr maybe I could try building all the C#/.NET projects before, pack them in a nuget and use the DLLs later in the FnO build, something similar to what Paul Heisterkamp explained in his blog.\nAnyway, it’s your choice how to manage your C# projects and what solution fits your workflow the best, but at least you’ve got an example here 🙂\nSetup Release Pipelines We’ve seen how the default build definition is created and how we can modify it. Now we’ll see how to configure our release pipelines!\nThe release pipelines allow us to automatically deploy our Deployable Packages to a Tier 2+ environment. This is part of the Continuous Delivery (CD) strategy. We can only do this for the UAT environments, it’s not possible to automate the deployment to the production environment.\nSetting up Release Pipeline in Azure DevOps for Dynamics 365 for Finance and Operations To configure the release pipeline, we need:\n AAD app registration LCS project An Azure DevOps project linked to the LCS project above A service account  I recommend a service account to do this, with a non-expiring password and no MFA enabled. It must have enough privileges on LCS, Azure and Azure DevOps too. This is not mandatory and can be done even with your user (if it has enough rights) for testing purposes, but if you’re setting this up don’t use your user and go for a service account.\nAAD app creation The first step to take is creating an app registration on Azure Active Directory to upload the generated deployable package to LCS. Head to Azure portal and once logged in go to Azure ActiveDirectory, then App Registrations and create a new Native app:\nNext go to “Settings” and “Required permissions” to add the Dynamics Lifecycle Services API:\nIn the dialog that will open change to the “APIs my organization uses” tab and select “Dynamics Lifecycle Services”:\nSelect the only available permission in the next screen and click on the “Add permissions” button. Finally press the “Grant admin consent” button to apply the changes. This last step can be easily forgotten and the package upload to LCS cannot be done if not granted. Once done take note of the Application ID, we’ll use it later.\nCreate the release pipeline in DevOps Go to Azure DevOps, and to Pipelines -\u0026gt; Releases to create the new release. Select “New release pipeline” and choose “Empty job” from the list.\nOn the artifact box select the build which will be used for this release definition:\nPick the build definition you want to use for the release in “Source”, “Latest” in “Default version” and push “Add”.\nUpload to LCS The next step we’ll take is adding a Task with the release pipeline for Dynamics. Go to the Tasks tab and press the plus button. A list with extension will appear, look for “Dynamics 365 Unified Operations Tools”:\nIf the extension hasn’t been added previously it can be done in this screen. In order to add it, the user used to create the release must have admin rights on the Azure DevOps account, not only in the project in which we’re creating the pipeline.\nWhen the task is created we need to fill some parameters:![Release Dynamics Operations](./MSDyn365 \u0026amp; Azure DevOps ALM - ariste.info_files/Captura-de-pantalla-2019-02-03-a-les-0.43.11-1024x508.png#center.webp \u0026ldquo;MSDyn365 \u0026amp; Azure DevOps ALM 84\u0026rdquo;)\nApply deployable package This step is finally available for self-service environments! If you already set this for a regular environment you can still change the task to the new version.\nAzure DevOps asset deployment\nThe new task version 1 works for both type of environments: Microsoft managed (regular environments) and self-service environments. The task version 0 is the old one and will only work with regular environments. You can safely switch your deploy tasks to version 1.\nWhat’s different in task version 1? I guess that some work behind it that we don’t see to make it support self-service, but in the UI we only see a new field called “Name for the update“.\nName for the update field\nThis field is needed only for the self-service environments deployments, it will be ignored for regular ones, and corresponds to the field with the same name that appears on LCS when we apply an update to a sandbox environment:\nName for this update in LCS\nThe default field’s value is the variable $(Release.ReleaseName) that is the name of the release, but you can change it, for example I’ll be using a pattern like PREFIX BRANCH $(Build.BuildNumber) to have the same name we have for the builds and identifying what we’re deploying to prod quickier.\nCreating the LCS connection The first step in the task is setting up the link to LCS using the AAD app we created before. Press New and let’s fill the fields in the following screen:\nIt’s only necessary to fill in the connection name, username, password (from the user and Application (Client) ID fields. Use the App ID we got in the first step for the App ID field. The endpoint fields should be automatically filled in. Finally, press OK and the LCS connection is ready.\nIn the LCS Project Id field, use the ID from the LCS project URL, for example in https://lcs.dynamics.com/V2/ProjectOverview/1234567 the project is is 1234567.\nPress the button next to “File to upload” and select the deployable package file generated by the build:\nIf the build definition hasn’t been modified, the output DP will have a name like AXDeployableRuntime_VERSION_BUILDNUMBER.zip. Change the fixed Build Number for the DevOps variable $(Build.BuildNumber) like in the image below:\nThe package name and description in LCS are defined in “LCS Asset Name” and “LCS Asset Description”. For these fields, Azure DevOps’ build variables and release variables can be used. Use whatever fits your project, for example a prefix to distinguish between prod and pre-prod packages followed by $(Build.BuildNumber), will upload the DP to LCS with a name like Prod 2019.1.29.1, using the date as a DP name.\nSave the task and release definition and let’s test it. In the Releases select the one we have just created and press the “Create a release” button, in the dialog just press OK. The release will start and, if everything is OK we’ll see the DP in LCS when it finishes:\nThe release part can be automated, just press the lightning button on the artifact and enable the trigger:\nAnd that’s all! Now the build and the releases are both configured. Once the deployment package is published the CI scenario will be complete.\nMore automation! I’ve already explained in the past how to automate the builds, create the CI builds and create the release pipelines on Azure DevOps, what I want to talk about in this post is about adding a little bit more automation.\nBuilds In the build definition go to the “Triggers” tab and enable a scheduled build:\nThis will automatically trigger the build at the time and days you select. In the example image, every weekday at 16.30h a new build will be launched. But everyday? Nope! What the “Only schedule builds if the source or pipeline has changed” checkbox below the time selector makes is only triggering the build if there’s been any change to the codebase, meaning that if there’s no changeset checked-in during that day no build will be triggered.\nReleases First step done, let’s see what can we do with the releases:\nThe release pipeline in the image above is the one that launches after the build I’ve created in the first step. For this pipeline I’ve added the following:\nThe continuous deployment trigger has been enabled, meaning that after the build finishes this release will be automatically run. No need to define a schedule but you could also do that.\nAs you can see, the schedule screen is exactly the same as in the builds, even the changed pipeline checkbox is there. You can use any of these two approaches, CD or scheduled release, it’s up to your project or team needs.\nWith these two small steps you can have your full CI and CD strategy automatized and update a UAT environment each night to have all the changes done during that day ready for testing, with no human interaction!\nBut I like to add some human touch to it If you don’t like not knowing if an environment is being updated… well that’s IMPOSSIBLE because LCS will SPAM you to make sure you know what’s going on. But if you don’t want to be completely replaced by robots you can add approvals to your release flow:\nClicking the left lightning + person button on your release you can set the approvers, a person or a group (which is quite practical), and the kind of approval (all or single approver) and the timeout. You will also receive an email with a link to the approval form:\nAnd you can also postpone the deployment! Everything is awesome!\nExtra bonus! A little tip. Imagine you have the following release:\nThis will update 3 environments, but will also upload the same Deployable Package three times to LCS. Wouldn’t it be nice to have a single upload and that all the deployments used that file? Yes, but we can’t pass the output variable from the upload to other stages 🙁 Yes that’s unfortunately right. But we can do something with a little help from our friend Powershell!\nUpdate a variable in a release What we need to do is create a variable in the release definition and set its scope to “Release”:\nThen, for each stage, we need to enable this checkbox in the agent job:\nI explain later why we’re enabling this. We now only need to update this variable after uploading the DP to LCS. Add an inline Powershell step after the upload one and do this:\n# Populate store value to update pipeline $assetId\\= \u0026#34;$(GoldenUpload.FileAssetId)\u0026#34; Write\\-Output (\u0026#39;##vso\\[task.setvariable variable=localAsset\\]{0}\u0026#39; \\-f $assetId) #region variables  $ReleaseVariableName \\= \u0026#39;axzfileid\u0026#39; $releaseurl \\= (\u0026#39;{0}{1}/\\_apis/release/releases/{2}?api-version=5.0\u0026#39; \\-f $($env:SYSTEM\\_TEAMFOUNDATIONSERVERURI), $($env:SYSTEM\\_TEAMPROJECTID), $($env:RELEASE\\_RELEASEID) ) #endregion  #region Get Release Definition  Write\\-Host \u0026#34;URL: $releaseurl\u0026#34; $Release \\= Invoke\\-RestMethod \\-Uri $releaseurl \\-Headers @{ Authorization \\= \u0026#34;Bearer $env:SYSTEM\\_ACCESSTOKEN\u0026#34; } #endregion  #region Output current Release Pipeline  #Write-Output (\u0026#39;Release Pipeline variables output: {0}\u0026#39; -f $($Release.variables | #ConvertTo-Json -Depth 10)) #endregion  #Update axzfileid with new value $release.variables.($ReleaseVariableName).value \\= $assetId #region update release pipeline  Write\\-Output (\u0026#39;Updating Release Definition\u0026#39;) $json \\= @($release) | ConvertTo\\-Json \\-Depth 99 $enc \\= \\[System.Text.Encoding\\]::UTF8 $json\\= $enc.GetBytes($json) Invoke\\-RestMethod \\-Uri $releaseurl \\-Method Put \\-Body $json \\-ContentType \u0026#34;application/json\u0026#34; \\-Headers @{Authorization \\= \u0026#34;Bearer $env:SYSTEM\\_ACCESSTOKEN\u0026#34; } #endregion You need to change the following:\n Line 2: $assetId= “$(GoldenUpload.FileAssetId)”. Change $(GoldenUpload.FileAssetId) for your output variable name. Line 6: $ReleaseVariableName = ‘axzfileid’. Change axzfileid for your Release variable name.  And you’re done. This script uses Azure DevOps’ REST API to update the variable value with the file id, and we enabled the OAuth token checkbox to allow the usage of this API without having to pass any user credentials. This is not my idea obviously, I’ve done this thanks to this post from Stefan Stranger’s blog.\nNow, in the deploy stages you need to retrieve your variable’s value in the following way:\nDon’t forget the ( ) or it won’t work!\nAnd with these small changes you can have a release like this:\nWith a single DP upload to LCS and multiple deployments using the file uploaded in the first stage. With approvals, and delays, and emails, and everything!\nLCS DB API Call the LCS Database Movement API from your Azure DevOps Pipelines What for? Basically, automation. Right now the API only allows the refresh from one Microsoft Dynamics 365 for Finance and Operations environment to another, so the idea is having fresh data from production in our UAT environments daily. I don’t know which new operations the API will support in the future but another idea could be adding the DB export operation (creating a bacpac) to the pipeline and having a copy of prod ready to be restored in a Dev environment.\nDon’t forget that the API has a limit of 3 refresh operations per environment per 24 hours. Don’t do this on a CI build! (it makes no sense either). Probably the best idea is to run this nightly with all your tests, once a day.\nCalling the API I’ll use PowerShell to call the API from a pipeline. PowerShell has a command called Invoke-RestMethod that makes HTTP/HTTPS requests. It’s really easy and we just need to do the same we did to call the API in my post.\nGetting the token $projectId \\= \u0026#34;1234567\u0026#34; $tokenUrl \\= \u0026#34;https://login.microsoftonline.com/common/oauth2/token\u0026#34; $clientId \\= \u0026#34;12345678-abcd-432a-0666-22de4c4321aa\u0026#34; $clientSecret \\= \u0026#34;superSeCrEt12345678\u0026#34; $username \\= \u0026#34;youruser@tenant.com\u0026#34; $password \\= \u0026#34;strongerThan123456\u0026#34; $tokenBody \\= @{ grant\\_type \\= \u0026#34;password\u0026#34; client\\_id \\= $clientId client\\_secret \\= $clientSecret resource \\= \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; username \\= $username password \\= $password } $tokenResponse \\= Invoke\\-RestMethod \\-Method \u0026#39;POST\u0026#39; \\-Uri $tokenUrl \\-Body $tokenBody $token \\= $tokenResponse.access\\_token To get the token we’ll use this script. Just change the variables for the ones of your project, AAD App registration, user (remember it needs access to the preview) and password and run it. If everything is OK you’ll get the JSON response in the $tokenResponse variable and from there you can get the token’s value using dot notation.\nRequesting the DB refresh $projectId \\= \u0026#34;1234567\u0026#34; $sourceEnvironmentId \\= \u0026#34;fad26410-03cd-4c3e-89b8-85d2bddc4933\u0026#34; $targetEnvironmentId \\= \u0026#34;cab68410-cd13-9e48-12a3-32d585aaa548\u0026#34; $refreshUrl \\= \u0026#34;https://lcsapi.lcs.dynamics.com/databasemovement/v1/databases/project/$projectId/source/$sourceEnvironmentId/target/$targetEnvironmentId\u0026#34; $refreshHeader \\= @{ Authorization \\= \u0026#34;Bearer $token\u0026#34; \u0026#34;x-ms-version\u0026#34; \\= \u0026#39;2017-09-15\u0026#39; \u0026#34;Content-Type\u0026#34; \\= \u0026#34;application/json\u0026#34; } $refreshResponse \\= Invoke\\-RestMethod $refreshUrl\u0026amp;nbsp;\\-Method \u0026#39;POST\u0026#39; \\-Headers $refreshHeader This will be the call to trigger the refresh. We’ll need the token we’ve just obtained in the first step to use it in the header and the source and target environment Ids.\nIf it’s successful the response will be a 200 OK.\nAdd it to your pipeline Adding this to an Azure DevOps pipeline is no mistery. Select and edit your pipeline, I’m doing it on a nigthly build (it’s called continuous but it’s not…) that runs after the environment has been updated with code, and add a new PowerShell task:\nSelect the task and change it to “Inline”:\nThen just paste the script we’ve created in the Script field and done! You’ll get a refresh after the tests!\nYou can also run this on your release pipeline BUT if you do it after the deploy step remember to mark the “Wait for Completion” option or the operation will fail because the environment will already be servicing! And even then it could fail if the servicing goes over the timeout time. So… don’t run this on your release pipeline!\nAnd that’s all. Let’s which new operations will be added to the API and what we can do with them.\nUse d365fo.tools in your Azure Pipeline Thanks to Mötz’s comment pointing me to how to add d365fo.tools to a hosted pipeline I’ve created a pipeline which will install the tools and run the commands. It’s even easier to do than with the Invoke-RestMethod.\nBut first… Make sure that in your Azure Active Directory app registration you’ve selected “Treat application as a public client” under Authentication:\nThe task First we need to install d365fo.tools and then we can use its commands to call the LCS API:\nInstall\\-PackageProvider nuget \\-Scope CurrentUser \\-Force \\-Confirm:$false Install\\-Module \\-Name AZ \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$False \\-SkipPublisherCheck Install\\-Module \\-Name d365fo.tools \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$false Get\\-D365LcsApiToken \\-ClientId \u0026#34;{YOUR\\_APP\\_ID}\u0026#34; \\-Username \u0026#34;{USERNAME}\u0026#34; \\-Password \u0026#34;{PASSWORD}\u0026#34; \\-LcsApiUri \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; \\-Verbose | Set\\-D365LcsApiConfig \\-ProjectId 1234567 Invoke\\-D365LcsDatabaseRefresh \\-SourceEnvironmentId \u0026#34;958ae597-f089-4811-abbd-c1190917eaae\u0026#34; \\-TargetEnvironmentId \u0026#34;13cc7700-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; \\-SkipInitialStatusFetch As you can see it a bit easier to do the refresh using d365fo.tools. We get the token and pipeline the output to the Set-D365LcsApiConfig command which will store the token (and others). This also helps to not having to duplicate AppIds, users, etc. and as you can see to call the refresh operation we just need the source and target environment Ids!\nAutomating Prod to Dev DB copies The new LCS DB API endpoint to create a database export has been published! With it we now have a way of automating and scheduling a database refresh from your Dynamics 365 FnO production environment to a developer or Tier 1 VM.\nUsing the LCS DB API\nThe bacpac issue One of the main setbacks we currently have with prod DB refreshes is that it’s not a quick thing to do because you need to:\n Refresh a Tier 2+ environment with prod’s DB Export a bacpac from the Tier 2+ environment Restore the bacpac on a Tier 1 VM.  This happens because Tier 2+ environments use Azure SQL as the DB engine and Tier 1 VMs use SQL Server.\nThe time it takes to complete the process depends on the size of the database and the performance of the VM you’ll restore it to. But it’s not a fast process at all. For a 60GB database you’ll get a bacpac around 7GB that will take:\n 1 to 2 hours to refresh to UAT 2 to 4 hours for the bacpac to be exported At least 4 hours to restore it to a Tier 1 VM.  That’s between 7 and 11 hours until you have the DB on a developer machine. Once it’s there you can quickly get a BAK and share it. But you might need the time of a full working day to have that data available.\nSave us LCS DB API! Thanks to the new LCS DB API’s endpoint we can perform all these steps automatically, and with the help of d365fo.tools it’ll be even easier. But first…\nDue to the extensive time it takes to complete all the process, we first have to decide a schedule (daily, weekly, etc.) and then this schedule must be compatible with the release cadence to UAT/Prod, because only one operation at a time can be done.\nThere’s still another problem but I’ll talk about it after seeing the scripts.\nMy proposal To do the last part of the LCS DB API flow from prod to dev, we need a Tier 1 VM where the bacpac will be restored. My idea is using the build VM on Microsoft’s subscription and an Azure DevOps pipeline to run all the scripts that will restore the DB in that VM. It’s an underused machine and it fits perfectly to this purpose.\nI want to clarify why I’ve thought about doing this using the build VM. In most cases this VM will be doing nothing during the night, maybe only running some tests, and it’s during that period of time when I suggest doing all this. But be aware that depending on your DB size this won’t be possible or you’ll run out of space after 2 o 3 restores.\nSo think about deploying an extra VM and install an agent there to do this, whatever you do don’t mess with the build VM if you don’t know what you’re doing! Try this on a dev VM or anywhere else if you’re afraid of breaking something. Remember you’ll lose the capacity to generate DPs and run pipelines if this environments breaks!\nThis post is just an example of a possible solution, you need to decide what suits you best! End of the update.\nAs I said before I’ll be using Mötz Jensen‘s d365fo.tools, we could do everything without them but that would be a bit stupid because using the tools is easier, faster and makes everything clearer.\nI’ve separated all the steps in 3 Powershell scripts: execute the refresh, export the bacpac and restore the bacpac.\nRefresh database This will refresh the prod environmnet to a Tier 2+:\n$clientId \\= \u0026#34;ab12345-6220-4566-896a-19a4ad41783f\u0026#34; $userName \\= \u0026#34;admin@tenant\u0026#34; $passWord \\= \u0026#34;admin123456\u0026#34; $projectId \\= \u0026#34;1234567\u0026#34; $sourceEnvId \\= \u0026#34;958bc863-f089-4811-abbd-c1190917eaae\u0026#34; $targetEnvId \\= \u0026#34;13aa6872-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; Get-D365LcsApiToken \\-ClientId $clientId \\-Username $userName \\-Password $passWord \\-LcsApiUri \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; \\-Verbose | Set\\-D365LcsApiConfig \\-ProjectId $projectId Invoke-D365LcsDatabaseRefresh \\-SourceEnvironmentId $sourceEnvId \\-TargetEnvironmentId $targetEnvId \\-SkipInitialStatusFetch Export database This part will trigger the bacpac export from the Tier 2+ environment which we’ve just refreshed:\n$sourceEnvId \\= \u0026#34;958bc863-f089-4811-abbd-c1190917eaae\u0026#34; $targetEnvId \\= \u0026#34;13aa6872-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; Get-D365LcsApiConfig | Invoke-D365LcsApiRefreshToken | Set\\-D365LcsApiConfig Invoke-D365LcsDatabaseExport \\-SourceEnvironmentId $targetEnvId \\-BackupName $bacpacName Restore bacpac And the final step will download the bacpac and restore it to a new database:\n$currentDate \\= Get-Date \\-Format yyyymmdd $bacpacName \\= \u0026#34;UAT{0}\u0026#34; \\-f $currentDate $downloadPath \\= \u0026#34;D:\\\\UAT{0}.bacpac\u0026#34; \\-f $currentDate $newDBName \\= \u0026#34;AxDB\\_{0}\u0026#34; \\-f $currentDate Get-D365LcsApiConfig | Invoke-D365LcsApiRefreshToken | Set\\-D365LcsApiConfig $backups \\= Get-D365LcsDatabaseBackups $fileLocation \\= $backups\\[0\\].FileLocation Invoke-D365AzCopyTransfer \\-SourceUri $fileLocation \\-DestinationUri $downloadPath Import-D365Bacpac \\-ImportModeTier1 \\-BacpacFile $downloadPath \\-NewDatabaseName $newDBName Using it in an Azure DevOps pipeline Azure DevOps pipeline\nThis is it. Create a Powershell script, place it in the Build VM and call it in your pipeline. This is only valid for the agent hosted in the build VM. Everything can probably be run in an Azure hosted agent, but I’ll not cover it here because I think that using the build VM, where we can restore the DB, is more useful to us.\nTiming These 3 scripts will call the LCS DB API to refresh, export and restore the DB. But there’s the timing issue.\nRefreshing the database takes some time and exporting it too. You need to find a way to control the status of the operations. The LCS DB API offers an operation you can use to get the status of the ongoing operation. Using d365fo.tools:\nGet-D365LcsDatabaseRefreshStatus \\-OperationActivityId 123456789 \\-EnvironmentId \u0026#34;99ac6587-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; You can choose to control that inside your Powershell scripts, but if we use the agent on the build VM that means we cannot use it for anything else until everything is done.\nThat’s why I separated the process in 3 steps. You can manually schedule 3 pipelines, one for each step at the times you know each stage ends. Then you can choose the order: export, restore, refresh or refresh, export, restore.\nYou could also use Windows Task Scheduler and forget about AZDO Pipelines, but we’re not doing that because we love pipelines.\nAnd that’s all, we finally have a way of moving data without having to do it manually, we can schedule it, but we need to take some decisions on how we’ll do things. And I’ll leave that up to you 🙂\nSecure your Azure Pipelines with Azure Key Vault But creating a pipeline with a password in plain sight was not very secure. How could we add extra security to a pipeline? Once again we can turn to an Azure tool to help us, the Azure Key Vault.\nAzure Key Vault A Key Vault is a service that allows us to safely store certificates or secrets and later use them in our applications and services. And like many other Azure services it has a cost but it’s really low and, for a normal use, you will be billed like a cent or none a month. Don’t be stingy with security!\nYou might already know about Azure Key Vault because we can use it in Microsoft Dynamics 365 for Finance and Operations under System Administration. For example it’s how the company certificates for the Spanish SII or Brazilian NF-e are stored and later retrieved to call the web services.\nSecuring your Azure DevOps Pipelines Thanks to the Azure Key Vault task (which is open source like many other tasks) getting a secret from a Key Vault has no secret (badum tssss).\nCreate a Key Vault Go to your Azure subscription and look for Key Vaults in the top search bar. If you don’t have an Azure subscription you can get one free with a credit of 170€/200$ for 30 days and try this or other things.\nIn the Key Vault page click on “Create key vault” and fill the fields\nYou can go through other tabs but I will just click “Review \u0026amp; Create” to create the vault.\nAdd the task to DevOps Now go to Azure DevOps and create a new pipeline or edit an existing one. Add a task to the agent job and look for azure key vault:\nIt’s possible that you might need to get the task from the marketplace first, if so remember you need to have enough right on the organization and not only the AZDO project you’re in. Now go to the task and select your subscription:\nOnce selected click the “Authorize” button. This will create a service principal in your subscription, we’ll use it later. After authorizing you just need to select the key vault you’ve created in the first step. And back to Azure.\nSetup and secret creation Go to your key vault, “Access policies” and click “Add Access Policy”:\nWhen we authorized the task to access our Azure subscription it created a service principal now we need to select it to list and get the secrets to be able to use them in our pipeline. Click on “Select principal”:\nIn the search bar type your subscription’s name, the principal should start with it and end with the same ID of your subscription. Select it and click the “Select” button at the bottom:\nNow click on the “Secret permissions” lookup and under “Secret Management Operations” select Get and List:\nIf you want to also use certificates or keys you should do the same. Finally click the “Add” button and don’t forget to click “Save”!! Otherwise nothing will be saved:\nNow we can create a secret in the key vault. Go to secrets and click on “Generate/Import”, complete the fields and finally click on the “Create” button:\nUsing the secrets in your pipelines We’re ready to use the secret in our pipeline. I will add a PowerShell task to call the LCS DB API using d365fo.tools but I’ll change all the variables to the secrets:\n# Write your PowerShell commands here. Install\\-PackageProvider nuget \\-Scope CurrentUser \\-Force \\-Confirm:$false Install\\-Module \\-Name AZ \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$False \\-SkipPublisherCheck Install\\-Module \\-Name d365fo.tools \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$false Get\\-D365LcsApiToken \\-ClientId \u0026#34;$(myAppId)\u0026#34; \\-Username \u0026#34;$(myUserName)\u0026#34; \\-Password \u0026#34;$(mySecretPassword)\u0026#34; \\-LcsApiUri \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; \\-Verbose | Set\\-D365LcsApiConfig \\-ProjectId $(myProjectId) Get\\-D365LcsDatabaseBackups As you can see now even the AAD App Id is masked.\nWhat the Azure Key Vault task does is getting the secrets from Azure and storing them in variables when the pipeline runs:\nThen we can access it’s value with the $(variableName) notation in the PowerShell script. If you try to print the secrets’ values using the Write-Host command all you’ll get will be three asterisks, so you can see that using the Key Vault is more than safe. If we check the result of running the Get-D365LcsDatabaseBackups command we’ll see how good is this:\nThe ProjectId value is not printed because it was one of our secret values!\nAnd this is how you can add extra security to your Dev ALM!\n","permalink":"https://nuxulu.com/posts/2021-05-01-dynamics-365-for-finance-operations-and-azure-devops/","summary":"Dynamics 365 for Finance \u0026amp; Operations and Azure DevOps Azure DevOps Azure DevOps will be the service we will use for source control. Microsoft Dynamics 365 for Finance and Operations supports TFVC out of the box as its version-control system.\nBut Azure DevOps does not only offer a source control tool. Of course, developers will be the most benefited of using it, but from project management to the functional team and customers, everybody can be involved in using Azure DevOps.","title":"Dynamics 365 for Finance \u0026 Operations and Azure DevOps"},{"content":"","permalink":"https://nuxulu.com/posts/2021-04-30-github-git-cheat-sheet/","summary":"","title":"Github Git Cheat Sheet"},{"content":"Open Windows PowerShell in Admin mode\nNavigate to the PowerShell scripts\ncd K:\\AosService\\PackagesLocalDirectory\\Plugins\\AxReportVmRoleStartupTask\\ Execute the below commands:\n For deploying all SSRS reports  ./DeployAllReportsToSSRS.ps1 -PackageInstallLocation “K:\\AosService\\PackagesLocalDirectory”  For deploying the specific reports  ./DeployAllReportsToSSRS.ps1 -Module ApplicationSuite -ReportName \u0026lt;ReportName\u0026gt; -PackageInstallLocation “K:\\AosService\\PackagesLocalDirectory” Example:\n./DeployAllReportsToSsrs.ps1 -Module MaxCustomization -ReportName MaxCheque_US.Report -PackageInstallLocation \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; ./DeployAllReportsToSsrs.ps1 -Module ApplicatoinSuite -ReportName Cust* -PackageInstallLocation \u0026#34;C:\\AosService\\PackagesLocalDirectory\u0026#34; ","permalink":"https://nuxulu.com/posts/2021-01-27-deploy-ssrs-reports-in-dynamics-365-finance-scm-using-powershell/","summary":"Open Windows PowerShell in Admin mode\nNavigate to the PowerShell scripts\ncd K:\\AosService\\PackagesLocalDirectory\\Plugins\\AxReportVmRoleStartupTask\\ Execute the below commands:\n For deploying all SSRS reports  ./DeployAllReportsToSSRS.ps1 -PackageInstallLocation “K:\\AosService\\PackagesLocalDirectory”  For deploying the specific reports  ./DeployAllReportsToSSRS.ps1 -Module ApplicationSuite -ReportName \u0026lt;ReportName\u0026gt; -PackageInstallLocation “K:\\AosService\\PackagesLocalDirectory” Example:\n./DeployAllReportsToSsrs.ps1 -Module MaxCustomization -ReportName MaxCheque_US.Report -PackageInstallLocation \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; ./DeployAllReportsToSsrs.ps1 -Module ApplicatoinSuite -ReportName Cust* -PackageInstallLocation \u0026#34;C:\\AosService\\PackagesLocalDirectory\u0026#34; ","title":"Deploy SSRS reports in Dynamics 365 Finance, SCM using Powershell"},{"content":"A new article for you the community. More specifically, all project managers, functional and technical consultants, as well as customer-side decision-makers on the deployment of Dynamics 365 Finance and Operations.\nI told myself that it would be important to offer you a global article on all the testing capabilities (manual or automatic) for Dynamics 365 Finance and Operations. It is the result of quite a long work but which I hope will help you to improve your code delivery and upgrade processes.\nAs you know, updates to version 10.x are monthly and therefore require operational tests on a regular basis ! I don’t know for you, but for some customers that I know it can take almost 1 week or more for 3 consultants…\nIn addition, if you want to be in an agile process of continuous delivery in production, it is often necessary to take time with the Key Users or the IT team to ensure that we do not have a regression in any part of the ERP. The more development you add, the longer and more difficult it will be to test all of your processes: this will therefore lower the quality of your deployments and repetitions of bugs in the chain: which will necessarily have a cost but also psychological in trust of the tool and your team.\nFor my part, I know that this part is often the least urgent in a process, however if this is done from the start of your GoLive and in continuity, you will gain enormously, therefore automating your tests as much as possible to allow time your team to manage more strategic cases (training, designs) that a machine will not be able to automate. This is the motto of the PowerAutomate tool of the PowerPlatform :\n “Take Care of what’s important. Automate the rest”\n Before going into the details of each testing feature that you can use with Dynamics 365 for Finance and Operations, it is important to clarify a few things, such as :\n  You CAN’T test everything… well automatically in fact. As you will see in this article, testing some SSRS Reports, layouts, interfaces or other tools will be very complicated to achieve, and a machine is not a human… So keep in mind that all these features will help you to automate as much as possible long tests process, but you will need to do something… Like of course analyzing Plan test reports, see errors log, correct them, tests manual process : but at the end, maybe you will gain 90% of your testing time ! So clearly I invite you to use all the elements that I will present to you.\n  I will not go very deeper in each parts, because it will be too long ! So of course, you can go directly in each Microsoft documentations to go further in details. But keep in mind, that all documentations are presented separately and it seemed important to me to report everything to you in a single document.\n  Data integration testing\nDo not use RSAT (as you will see after) for integration tests, instead rely on the data management framework (also known as DIXF). The Data task automation framework enables you to configure and automate the testing of your data integration scenarios.\n   Now, we can go !\nSo, in Dynamics 365 Finance and Operations, you have 3 parts of testing.\nWhile the functional validation of an ERP application can’t be fully data agnostic, there are multiple phases and approaches for testing. These testing phases include:\n  SysTest framework\n  ATL frameowrk\n  Regression Suite Automation Tool (RSAT)\n  Overview   SysTest framework – The SysTest framework is reliable for writing unit tests. Because unit tests are generally testing a method or function, they should always be data agnostic and dependent only on the input data that is provided as part of the test.\n  ATL framework – Microsoft has an ATL framework that is an abstraction on the SysTest framework and makes functional test writing much more simple and reliable. This framework should be used for writing component tests or simple integration tests.\n  RSAT – The RSAT is used for integration tests and business cycle tests. The business cycle tests, also called the regression validation tests, are dependent on existing data. However, these tests can become data agnostic if you consider additional factors.\n  Where unit tests and component tests are low level and can fully be data agnostic (not dependent on existing dataset), the business cycle or regression validation tests are dependent on some existing data. This data includes setup, configuration settings (parameters), and master data (customer, vendors, items, etc.), but never transaction data. Make sure that during the test, if any of these are being changed, that they are reverted back as part of the final test.\n  Select master data based on certain criteria instead of selecting a particular record. For example, if you want to select an item based on its dimension values and stock availability, filter the product list with those values, select the first item, and copy the number to be used for future tests. If it’s a simple master data line such as customer, vendor, or item, it can be created as part of the automation and used in future tests through chaining.\n  Enter the unique identifiers, such as invoice numbers, through the number sequence or by using Microsoft Excel functions such as =TEXT(NOW(),\u0026ldquo;yyyymmddhhmm\u0026rdquo;). This function will provide a unique number every minute, which allows you to track when the action happened. This can be used for variables such as product receipt numbers and vendor invoice numbers. These tests continue to work on the same database again and again, without requiring any restoration.\n  Always set the Edit mode of the environment to Read or Edit as the first test case because the default option is Auto. The Auto options always uses the previous setting and can cause unreliable tests. You can change it in the TEST account that will be used in RSAT (User Option)\n  Only validate after you filter on a particular transaction instead of generic validation. For example, for the number of records, filter for the transaction number or the transaction date so that the validation excludes all other transactions.\n  If you are checking a customer balance or budget check, save the value first and then add your transaction value to validate the expected result instead of validating a fixed expected value.\n    ![rsat-data-agnostic-testing-01.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/rsat-data-agnostic-testing-01.png#center)\n 1/ LCS \u0026lt;=\u0026gt; BPM \u0026lt;=\u0026gt; AzureDevOps (via Task Recorder) You aren\u0026rsquo;t required to use the Business process modeler (BPM) tool in LCS. However, BPM is the recommended tool if you want to enable the management and distribution of test libraries across projects and tenants. These capabilities are especially useful for Microsoft partners and independent software vendors (ISVs). BPM enables the distribution of test libraries as part of LCS solutions. If you are not using BPM, you can manually create test cases in Azure DevOps and attach developer recording files to your Azure DevOps test cases. You can create developer recording files directly from the Task recorder pane.\nOn my side, I will show you how to create BPM in LCS before going to record all my process with the Task Recorders.\nSo, go to your LCS project first. I will assume also that you have already connect LCS and Azure DevOps together.\nYou can after go to the Business Process Modeler part.\n![Screenshot 2020-04-20_21-19-53-857.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_21-19-53-857.png#center)\nAs you will notice here, you have your own BPM and also some done by Microsoft. Of course, in your own company/customer, it’s somehow complicated to have a standard \u0026amp; global but it will be helpful to check the Microsoft BPM to see how it works.\n![Screenshot 2020-04-20_21-21-34-605.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_21-21-34-605.png#center)\nOn my side I will create a complete new one to handle testing purpose. “Normally” before a GoLive it’s a task very highly recommended to do with your Key Users, Project Manager and Functional Consultant.\n![Screenshot 2020-04-20_21-25-13-784.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_21-25-13-784.png#center)\nSo as you can see, I’ve created a very basic one, just for an example. My process is only to model my customer creation process.\nAfter you can add other child flows, dependent processes etc…\nAlso in BPM, you can have a VISIO model of Flow charts, of all your process in the ERP, has defined in LCS. It can be also a good way to add requirements in AzureDevOps before jumping to do customization. Clearly it’s high level definition, but useful also to not forget some specific process, as well as doing some tutorials practices based on that : as you will see also in Task Recorder, you can do your training documentation with screenshots. That’s why if you do it at the very beginning stage of your implementation, all these process conception will help you not just for testing purpose after or before GoLive !!\nIf you want to learn more, you’ll to go there in the Microsoft documentation below :\nLearn more on BPM in LCS\nNow that I’ve done my BPM in LCS, i will need to go to FinOps instance to record all my process flow.\nQuick tips : Download the Extension to have the featue for Google Chrome to take screenshots (good way for training documentation) :\nhttps://chrome.google.com/webstore/detail/d365-for-finance-and-oper/inifapcodikhojbnbafaalgbgkfmnlob/related?hl=en-GB\nGo like in an UAT instance, where you have some DEMO data in it or maybe already a copy of your production/live database. Of course, the mandatory data is to have all your reference and master data in it in order to have a whole complete process. (like for my customer flow : Customer Group, Tax Group etc…)\nWhen you are in FinOps, go to the top right !\n![Screenshot 2020-04-20_21-43-54-968.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_21-43-54-968.png#center)\nClick on “Create recording”\nGive it a name and description if you want. (good if you have multiple different type of flows to create a customer) - activate take screenshots option if you also intend to generate documentation, in addition to being able to recover the trace file for automated tests.\n![Screenshot 2020-04-20_21-48-41-193.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_21-48-41-193.png#center)\nWell, next, you need to do your process :)\nAfter end your task recorder, and keep in a safe place your .AXTR files ; like in a Sharepoint folder.\n![Screenshot 2020-04-20_21-54-08-943.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_21-54-08-943.png#center)\nKeep in mind that Task Records will need to be adjust in your lifecycle of your project, maybe if you change your process or maybe because you have added a new custom field.\nAnd here are few more best practices :\nFollow these guidelines when authoring and recording your test cases, especially if you are planning to automate test execution. The process and tools described in this article apply to business process acceptance tests. They are not meant to replace component and unit testing that is typically owned by developers.\n  Author a limited number of test cases that, when combined, cover complete end-to-end processes.\n  Focus on business processes that have been customized.\n  An individual test case (recording) should cover one or two business tasks only, typically executed by one person. This simplifies task recording maintenance. Do not combine a complete end-to-end business process such as \u0026ldquo;Procure to Pay\u0026rdquo; or \u0026ldquo;Order to Cash\u0026rdquo; into one large task recording. For example, instead of having RFQ \u0026gt; Purchase Order \u0026gt; Product Receipt \u0026gt; Vendor Invoice \u0026gt; Vendor Payment as one test case, divide the process into three or four test cases. You will have the opportunity to combine these tests into an ordered test suite later.\n  A test case should have at least one validation. Try to validate critical fields that cover the impact of other fields. For example: Validation of totals on sales or purchase orders cover the unit price/quantity/discount/tax \u0026hellip;etc.\n  Avoid printing a report in a test case. If a test case needs to print a report, it should be selected on screen.\n  80+% of test cases should be of transactions or source documents. Master data should be limited to up to 20% of test cases only.\n  Saying that, let’s go again in our BPM library that we have created earlier. We will attach our task recorder file in it.\nFor that, just click on upload.\n![Screenshot 2020-04-20_22-01-29-423.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-01-29-423.png#center)\nNow we will sync it directly to our AzureDevOps Project, to create all our test plan and create every unit test case for every process that you have attached a task recorder file.\nYou will need to click on the 3 dots, right after the Collapse button and click on the two like me :\n![Screenshot 2020-04-20_22-03-46-975.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-03-46-975.png#center)\nNow you can go to your Azure DevOps / VSTS tenant.\nYou will need an account that have a MSDN subscription or at least a Basic Licence + test plan in order to do automate testing. So take 1 or 2 licences, essentially for the account that will configure all test plan. The basic licence is at 5 euro/month/user and the basic + test plan is more than 40 euro/month/user so not the same cost, but compared to the gain in effort by continuing to carry out only manual tests, believe me you will necessarily largely recover that…\nI have created a Test Plan called : RSAT for the tool Regression Suite Automation Tool that I will explain after. But you can call it more with a friendly name, like “Regression test for 10.0.9 upgrade”. Of course it’s like a folder of every test case suite that we will attach after, to have a global overview of every tests cases.\nIt will be also here that you can see Chart, progression testing report, based on a time purpose, priority, logs. You can copy/past a whole test plan, every week-month, like for upgrade process.\nAttach like me the test cases that are normally been in VSTS, since you have synced in LCS before in the BPM.\n![Screenshot 2020-04-20_22-13-33-236.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-13-33-236.png#center)\nYou will see in a test case, like the Customer flow that I’ve done before some informations. The tag “LCS:TestCases” in order to quickly see all synced tests with BPM in LCS. You will see every steps actions, and in attachment you will have the .AXTR file needed for automate the test with RSAT after.\n![Screenshot 2020-04-20_22-17-06-855.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-17-06-855.png#center)\n 2/ RSAT - Regression Suite Automation Tool Overview\nThe Regression suite automation tool (RSAT) significantly reduces the time and cost of user acceptance testing (UAT). UAT is typically required before you take a Microsoft application update, or before you apply custom code and configurations to your production environment. RSAT lets functional power users record business tasks by using Task recorder and then convert the recordings into a suite of automated tests, without having to write source code. For more information about Task recorder, see Task recorder resources.\nRSAT is fully integrated with Microsoft Azure DevOps for test execution, reporting, and investigation. Test parameters are decoupled from test steps and stored in Microsoft Excel files.\nRSAT usage is described here in this schema :\n![end-to-end.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/end-to-end.png#center)\nFirst, you will need to install it :)\nWindows 10 and needed also Excel app.\nDownload RSAT\nBut before going to RSAT directly, go back to your Azure DevOps, you will need a Personal Access Token.\n![Sans titre.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Sans+titre.png#center)\n![Screenshot 2020-04-20_23-51-45-050.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-51-45-050.png#center)\nCreate a new one for RSAT, and make him as expiration date in 1 year, to be safe :)\nAfter you need to configure it, to link of course on which environment you need to do your automate testing and. also the Azure DevOps project in which you have setup all test cases.\n![RSAT-1.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-1.png#center)\nChange your Azure DevOps URL, put your Personal Access Token generated before, you will now use your Project and the Test Plan created in Azure DevOps before.\n![RSAT-2.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-2.png#center)\nAfter it’s a little more complicated.\nPut your hostname, the URL of your FinOps instance without HTTPS.\nFor the SOAP Hostname, it’s the same but with aossoap between the firstpart of your hostname and .sandbox part. Like me below :\n![RSAT-3.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-3.png#center)\nPut your admin user name email address. The account that will launch every test in your instance. And the company name / legal entity. Keep in mind that we can change on every test the legal entity in a parameter file.\nFor the Thumbprint, it’s a certificate to generate on your computer. Click on new. Copy the number. You will need a technical guy to put it in the environment where you plan to execute the TEST.\nIn the wif.config located in K:\\AosService\\WebRoot\n-\u0026gt; Don’t forget to apply on each AOS Server !\n\u0026lt;authority name=\u0026quot;CN=127.0.0.1\u0026quot;\u0026gt;\r\u0026lt;keys\u0026gt;\r\u0026lt;add thumbprint=\u0026quot;xxxxxxxxxxxxxxxxxxxxxxxxx\u0026quot; /\u0026gt;\r\u0026lt;/keys\u0026gt;\r\u0026lt;validIssuers\u0026gt;\r\u0026lt;add name=\u0026quot;CN=127.0.0.1\u0026quot; /\u0026gt;\r\u0026lt;/validIssuers\u0026gt;\r\u0026lt;/authority\u0026gt;  Save a Working Directory folder and also Default Browser to Google Chrome. Don’t forget after configuration to click on Save As, in order to save the configuration and maybe share it for other consultants in your project.\nLoad your test plan and click after on Generate Test Execution and Parameters files.\n![RSAT-3.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-3(1).png#center)\n![You need to have all your test cases here and now you can check on each the Parameter files and change everything needed.](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-4.png#center)\nYou need to have all your test cases here and now you can check on each the Parameter files and change everything needed.\n![RSAT-5.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-5.png#center)![RSAT-6.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-6.png#center)![RSAT-7.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-7.png#center)\nYou can pick one, like the Customer RSAT process. And click on “RUN” ; before a small warning , click on YES.\n![RSAT-8.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-8.png#center)\nNormally, Google Chrome will launch automatically and test everything without any actions on your hand !!\nSo maybe a good way is to install RSAT on a network server machine or a VM in Azure and launch every tests on night, without to keep your local machine / PC turned on : because it can takes some time, depending on the amount of test you have.\nAt the end, you can see the result directly in RSAT on each of them.\nBut the best way is after to upload the result directly on Azure DevOps. So that everyone, especially Technical Guys can get the log files in there if they are some errors.\nFor that, click on Upload part and “all modified automation files”\n![RSAT-9.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/RSAT-9.png#center)\nNow go to your Azure DevOps Test Plan.\nYou can see all Recent test runs\n![Screenshot 2020-04-21_00-30-44-231.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-21_00-30-44-231.png#center)\nIf we go to the one of RSAT that goes well.\n![Screenshot 2020-04-21_00-32-06-314.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-21_00-32-06-314.png#center)\nYou can have all the Log needed, tracked by RSAT.\nOn something that went wrong… but in unit test with SysTestFramework that we will see after, the developer can have the .AXTR file generated for her/him.\nAlso we can generate automatically a Bug/Work Item based on a failed test and alert me :)\n![Screenshot 2020-04-21_00-31-37-242.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-21_00-31-37-242.png#center)\nFor Project Manager, we have also a report that we can change if you want !\n![Screenshot 2020-04-21_00-33-18-567.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-21_00-33-18-567.png#center)\nTo conclude RSAT, yes we can deeper. I don’t have setup this kind of thing in this example, but you can of course :\n  Create derivated test case : The Regression suite automation tool (RSAT) lets you to use the same task recording with multiple test cases, so that you can run a task with different data configurations. To do this, select a test case in the Regression suite automation tool and then select New \u0026gt; Create Derived Test Case. This creates a child test case in Azure DevOps. The resulting derived test case is linked to its parent test case in Azure DevOps. It has an Excel parameters file attached but no recording file. The derived test case will appear in the Regression suite automation tool grid under the same test suite with the Derived column selected. Derived test cases are named after their parent test case with a numeric suffix.\n  Chaining : One of the key features of the Regression Suite Automation Tool is the chaining of test cases, that is, the ability of a test to pass values to other tests. Test cases are executed according to their defined order in the Azure DevOps test plan, which can also be updated in the test tool itself. It is important to correctly order the tests if you want to pass variables from one test case to the other.\nTo save the value of a variable while recording the test in Task Recorder, right-click the field and select Task recorder \u0026gt; Copy, as shown below. This will save the variable in the recording file. This variable can be used in subsequent tests.\n  Validate expected values : An important component of a test case is validation of expected values. You can define validation parameters during the authoring of your test cases using Task Recorder. While recording, right-click on a control and select CurrentValue under the Task Recorder \u0026gt; Validate menu. This action becomes a validation step that you can use with the Regression suite automation tool. The control value will become a validation variable in the automatically generated Excel parameters file. The menu item is shown in the following image.\n  ![When RSAT generates the Excel parameter file for a test case, validation steps are added as shown in the image below. You can enter the expected value to use during execution of the test case.](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/validate-test-case.png#center)\nWhen RSAT generates the Excel parameter file for a test case, validation steps are added as shown in the image below. You can enter the expected value to use during execution of the test case.\n![rsat-validate-variables.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/rsat-validate-variables.png#center)\nLearn more RSAT\n 3/ SysTestFramework and ATL : Acceptance test library ![54.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/54.png#center)\nKey concepts   Use SysTest Framework to author unit/component test code.\n  Test isolation\n  Test module creation to manage test code and FormAdaptors.\n  Import Task Recorder recordings into Visual Studio to generate test code.\n  Overview of the ATL framework\n  Integrate a Test module with a build machine.\n  Clearly this part is mostly for developers, but I think it’s useful also for project manager or functional consultant to know which unit test in the code can be done, despite the RSAT tool that we see before. In fact, all task recorders can be a good start to include it in the Development machine to generate simple test case, but I will show also other Framework that developers can use to be more confident before pushing a new development in a TEST environment…\nSo, first best practice, before going to start writing tests, you will need a new model for that ! To achieve that, go to your Visual Studio and create a new one like me :\n![Screenshot 2020-04-20_22-43-39-536.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-43-39-536.png#center)\nImportant after, select your reference package. Like me, I select my main package of custom codes - solution to TEST\n![Screenshot 2020-04-20_22-44-11-534.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-44-11-534.png#center)\nCheck : Create new project, it will here where you will place all your unit tests - and don’t of course make it as the default model for new projects.\n![Screenshot 2020-04-20_22-44-49-881.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-44-49-881.png#center)\nAfter the model is created, change the reference packages, to include ALL FormsAdaptators models, and the main one : Test essentials model !\nAlso put your main VS project as the Startup object !\n![Screenshot 2020-04-20_22-46-10-271.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-46-10-271.png#center)\nSo now, you have your TEST model, all referenced packages included, include your main one of course. You can now built custom test unit code with SysTestFramework;\nOn my side, I will use the Task Recorder Add-in in VS, but of course, you can built your own one without addin. Especially to test custom method on a custom class etc…\nTo generate test class automatically, click on Addins / Import Task Recording\n![Screenshot 2020-04-20_22-56-02-444.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_22-56-02-444.png#center)\nImport the file that you have in the AzureDevOps Unit Test case that we see before (like me : Recording.xml) : as you can see BPM / LCS and Azure DevOps is not only for RSAT !\nSelect of course your new model for testing purpose.\n![Screenshot 2020-04-20_23-01-07-796.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-01-07-796.png#center)\nYou will have a new generate class, like me\nChange just the top of the class, like adding a SysTestCategory, it will be helpful by doing some filters in the BUILD pipeline of Azure DevOps. Select the legal entity where you want to achieve your test and also add an AutoRollback statement in order to erase all your data after testing process.\nThis class is of course simulate each UI testing, as well as data / business process testing.\n![Screenshot 2020-04-20_23-03-17-041.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-03-17-041.png#center)\nYou can of course directly after run \u0026amp; check if your TEST is OK in your DEV environment by going in the Test Explorer view.\n![Screenshot 2020-04-20_23-04-06-977.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-04-06-977.png#center)\nThis test can be now included in your version control to be used as the BUILD pipeline step in order to automatically runs every night all your SysTest Framework test.\nLearn more on SysTest Framework\nSince 10.0.2 I think, we have now a new Test Framework in order to achieve more testing class, as far as I seen it’s really much better \u0026amp; easier than SysTestFramework.\nThe Acceptance test library (ATL) is an X++ test library that offers the following benefits:\n  It lets you create consistent test data.\n  It increases the readability of test code.\n  It provides improved discoverability of the methods that are used to create test data.\n  It hides the complexity of setting up prerequisites.\n  It supports high performance of test cases.\n  Since I don’t want to rewrite all the Microsoft documentation and since it’s very well good documented, I highly recommend to have a look on it for developer.\nLearn more on ATL Framework\n 4/ Change your BUILD \u0026amp; Release pipeline in AzureDevOps Now go to your AzureDevOps project, on the BUILD pipeline part. We will change the pipeline to include automate testing from SysTest Framework or ATL that we saw before.\nOn my side, I’ve got a BUILD Main, only one. So of course, change \u0026amp; adapt on your needs. Enable the 3 tasks at the end : Test Setup, Execute and End Tests.\n![Screenshot 2020-04-20_23-22-37-415.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-22-37-415.png#center)\nFor these 3 steps, don’t need to change large setup, Just use Task version 2.* and the only change is on the Variable part, to include the SysTestCategory that I had before, remember :) also added my main model.dll that host all my test class.\n![Screenshot 2020-04-20_23-22-51-535.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-22-51-535.png#center)![Screenshot 2020-04-20_23-23-24-739.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-23-24-739.png#center)![Screenshot 2020-04-20_23-23-38-414.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-23-38-414.png#center)![Screenshot 2020-04-20_23-24-24-388.png#center](./Automate your tests for Dynamics 365 Finance and Operations — PowerAzure365_files/Screenshot+2020-04-20_23-24-24-388.png#center)\nSo here we are you are a Pro of Automate testing in Dynamics 365 Finance and Operations.\nTo conclude, we can also use PowerAutomate with AzureDevops : doing a morning a test plan report email ? Also create an adaptive card for deployment approval on release pipeline after seeing the test report ?\nWell as you can see, you have now all in your hands features to achieve a lot automate testing.\n","permalink":"https://nuxulu.com/posts/2021-01-01-automate-your-tests-for-dynamics-365-finance/","summary":"A new article for you the community. More specifically, all project managers, functional and technical consultants, as well as customer-side decision-makers on the deployment of Dynamics 365 Finance and Operations.\nI told myself that it would be important to offer you a global article on all the testing capabilities (manual or automatic) for Dynamics 365 Finance and Operations. It is the result of quite a long work but which I hope will help you to improve your code delivery and upgrade processes.","title":"AUTOMATE YOUR TESTS FOR DYNAMICS 365 FINANCE AND OPERATIONS"},{"content":"Microsoft provides a versioned set of capabilities that you can currently use to copy databases between environments, and to list and download database backups.\nwhat you can do with Database movement API so far:\n List database backups Create database refresh Create a database export Get operation activity status  More supported actions will be added in later releases.\nThe endpoint uses impersonation authentication base, please follow to register a new application by using the Azure portal\n1. Postman environment setup Open Postman -\u0026gt; manage environments -\u0026gt; Add\ntenant_id: {tenant_id get from AAD} client_id: {get from the application that you have created before} client_secret: {get from the application that you have created before} username: {LCS user name with owner permission} password: {LCS user password} grant_type: password resource: https://lcsapi.lcs.dynamics.com projectId: {Your LCS Project ID} bearerToken: {this will be populated when authentication} {:.border}\n2. Authentication with Postman You get the authentication bearer with POST method and https://login.microsoftonline.com/{{tenant_id}}/oauth2/token endpoint.\nIn the request Body, please do following:\n{:.border}\nClick Send, and you will have access token to start using the API.\n{:.border}\n3. Cosnume the APIs with Postman To call the Database Movement API, you attach the access token as a bearer token to the authorization header in your HTTP request. So in Postman, modify the headers tab like bellow\nAuthorization:Bearer {{bearerToken}} x-ms-version:\u0026#39;2017-09-15\u0026#39; Content-Type:application/json {:.border}\n3.1. List database backups GET https://lcsapi.lcs.dynamics.com/databasemovement/v1/databases/project/{{projectId}} And we’ll get a JSON with a list of the DB backups on our LCS Asset Library:\n{ \u0026#34;DatabaseAssets\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;12314234-862e-4a6a-800d-0c64e982284a\u0026#34;, \u0026#34;ProjectId\u0026#34;: 123123, \u0026#34;OrganizationId\u0026#34;: 123124, \u0026#34;Name\u0026#34;: \u0026#34;backup\u0026#34;, \u0026#34;FileName\u0026#34;: \u0026#34;ATbackup.bacpac\u0026#34;, \u0026#34;FileDescription\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FileLocation\u0026#34;: \u0026#34;https://uswedpl1catalog.blob.core.windows.net/product-ax7productname/******\u0026#34;, \u0026#34;ModifiedDateTime\u0026#34;: \u0026#34;2020-08-17T09:52:50.077\u0026#34;, \u0026#34;CreatedDateTime\u0026#34;: \u0026#34;2020-08-17T09:52:45.297\u0026#34;, \u0026#34;CreatedByName\u0026#34;: null, \u0026#34;ModifiedByName\u0026#34;: null } ], \u0026#34;IsSuccess\u0026#34;: true, \u0026#34;OperationActivityId\u0026#34;: \u0026#34;5053e0dd-66e3-4832-a9f8-1e2d621562e1\u0026#34;, \u0026#34;ErrorMessage\u0026#34;: null, \u0026#34;VersionEOL\u0026#34;: \u0026#34;9999-12-31T23:59:59.9999999\u0026#34; } 3.2. Create database refresh POST https://lcsapi.lcs.dynamics.com/databasemovement/v1/refresh/project/{projectId}/source/{sourceEnvironmentId}/target/{targetEnvironmentId} { \u0026#34;IsSuccess\u0026#34;: true, \u0026#34;OperationActivityId\u0026#34;: \u0026#34;55eb4327-9346-4c7b-82bd-fe8ef15112c6\u0026#34;, \u0026#34;ErrorMessage\u0026#34;: null, \u0026#34;VersionEOL\u0026#34;: \u0026#34;9999-12-31T23:59:59.9999999\u0026#34; } 3.3. Create a database export POST https://lcsapi.lcs.dynamics.com/databasemovement/v1/export/project/{projectId}/environment/{environmentId}/backupName/{backupName} { \u0026#34;IsSuccess\u0026#34;: true, \u0026#34;OperationActivityId\u0026#34;: \u0026#34;55eb4327-9346-4c7b-82bd-fe8ef15112c6\u0026#34;, \u0026#34;ErrorMessage\u0026#34;: null, \u0026#34;VersionEOL\u0026#34;: \u0026#34;9999-12-31T23:59:59.9999999\u0026#34; } 3.4. Get operation activity status GET https://lcsapi.lcs.dynamics.com/databasemovement/v1/fetchstatus/project/{projectId}/environment/{environmentId}/operationactivity/{operationactivityId} { \u0026#34;IsSuccess\u0026#34;: true, \u0026#34;OperationActivityId\u0026#34;: \u0026#34;6a90b45f-1764-4077-b924-3f4671540237\u0026#34;, \u0026#34;ErrorMessage\u0026#34;: null, \u0026#34;VersionEOL\u0026#34;: \u0026#34;9999-12-31T23:59:59.9999999\u0026#34;, \u0026#34;ProjectId\u0026#34;: \u0026#34;12345\u0026#34;, \u0026#34;EnvironmentId\u0026#34;: \u0026#34;5362377c-bc37-4f92-b30e-fe0c1e664cc0\u0026#34;, \u0026#34;ActivityId\u0026#34;: \u0026#34;55eb4327-9346-4c7b-82bd-fe8ef15112c6\u0026#34;, \u0026#34;CompletionDate\u0026#34;: null, \u0026#34;OperationStatus\u0026#34;: \u0026#34;InProgress\u0026#34; } ","permalink":"https://nuxulu.com/posts/2020-08-17-testing-dynamics-365-finance-database-movement-api-with-postman/","summary":"Microsoft provides a versioned set of capabilities that you can currently use to copy databases between environments, and to list and download database backups.\nwhat you can do with Database movement API so far:\n List database backups Create database refresh Create a database export Get operation activity status  More supported actions will be added in later releases.\nThe endpoint uses impersonation authentication base, please follow to register a new application by using the Azure portal","title":"Testing Dynamics 365 Finance Database Movement API with Postman"},{"content":"Today, we published the 2020 release wave 2 plans for Microsoft Dynamics 365 and Microsoft Power Platform, a compilation of new capabilities that will be released between October 2020 and March 2021. This second release wave of the year offers hundreds of new features and enhancements, demonstrating our continued investment to power digital transformation for our customers and partners.\nHighlights from Dynamics 365  Dynamics 365 Sales includes updates for more simplified experiences; including collaboration tools, a new mobile experience for quick access to customer information, and new enhancements to forecasting to natively create and manage bottom-up sales forecast processes. Dynamics 365 Sales Insights continues investments in digital selling across multiple areas: sales acceleration, conversation intelligence, relationship intelligence, and advanced forecasting and pipeline intelligence with predictive lead and opportunity scorings to help sales teams uncover top deals. Dynamics 365 Customer Service expands agent productivity capabilities enabling agents to engage in multiple sessions simultaneously. Omnichannel for Customer Service is enhanced with additional extensibility options to enable integration with mobile applications, Microsoft bot framework, and outbound messaging channels. Dynamics 365 Customer Service Insights adds new capabilities to help agents using similar case suggestions to resolve customer issues quickly and easily. A new analytical view for customer service managers helps them focus on key support areas that need attention. These highlights will also be included directly in the core Customer Service Hub app so that users can get insights in context without having to switch between applications. Dynamics 365 Remote Assist expands its range of scenarios beyond calls, allowing technicians to perform activities such as capture service and repairs data, perform surveys and walk-throughs independently, and derive service insights from their service operations. Availability to non-AR enabled devices with modified functionality further empowers technicians to solve problems faster the first time in more environments. Dynamics 365 Field Service continues to add intelligence capabilities including a new Field Service dashboard for monitoring key KPIs and work order completion metrics. There are many user experience enhancements to enable proactive service delivery. The Field Service mobile app is enhanced with capabilities such as push notifications and real-time location sharing. This release wave also includes scheduling enhancements such as multi-day manual scheduling and enhanced skill-based matching. Dynamics 365 Marketing improves the customer journey design experience, for example layout options, zoom, and performance improvements. Integration with Microsoft Teams live events and meetings allows users to create and host live events as a webinar provider. Segmentation is enhanced with a new natural language experience to create and consume customer segments, helping eliminate the specialized skills needed to build complex segments. Dynamics 365 Customer Insights continues to enhance data ingestion and unification, segmentation, and extensibility capabilities using Microsoft Power Platform to enable and extend line-of-business experiences. For example, users can gain deeper customer insights with Microsoft Power BI, build custom apps with Microsoft Power Apps, and trigger workflows based on insights and signals using Microsoft Power Automate. Dynamics 365 Human Resources expands leave and absence, and benefits management capabilities to transform the employee experience. Employees and managers will be able to manage leave and absence directly from Microsoft Teams. We continue to build an HCM ecosystem enabling integrations to recruiting and payroll partners. Dynamics 365 Commerce continues to expand capabilities enabling marketers and non-developers to easily create and manage e-commerce sites with built-in experimentation capabilities. We are improving in-store and curbside pickup scenarios to help customers thrive in the face of the COVID impact. In addition, we are making it easier to increase engagement and conversions online and in-store with AI-powered \u0026ldquo;shop similar looks\u0026rdquo; recommendations and intelligent search experiences through Bing for Commerce. Dynamics 365 Connected Store adds a number of new capabilities such as integration with Dynamics 365 Commerce, front-line worker task assignment and tracing with Microsoft Teams, integrated workflows with Power Platform, intelligent command center, store analytics, and store insights solutions such as anomaly detection, inventory recommendations, and shift management recommendations. Dynamics 365 Fraud Protection adds integration with Dynamics 365 Commerce and a new \u0026ldquo;manual review\u0026rdquo; capability that allows customers to use the Fraud Protection rules experience to flag transactions for review, and then allow expert human agents to consume and adjudicate those transactions. Dynamics 365 Finance continues to focus on automating common tasks to reduce the number of processes that finance users complete manually. We are introducing Asset leasing to enhance the core capabilities of Finance and continue to expand our global coverage delivering localizations for five additional countries/regions (Bahrain, Hong Kong, Kuwait, Oman, and Qatar) extending the number of supported countries/regions to 42. Dynamics 365 Supply Chain Management expands planning optimization for manufacturing to perform supply and production planning in near real-time with in-memory services. Enhancements to Product Information Management include engineering change management and production versioning capabilities. Cost Management includes new features that will enable global companies maintain multiple cost accounting ledgers by allowing dual currency and dual valuation. Enhancements to the job card device include a new user experience and a new feature to enable reporting serial numbers. Dynamics 365 Project Operations unifies operational workflows to provide the visibility, collaboration, and insights needed to drive success across teams from sales to finance. Project Operations connects your sales, resourcing, project management, and finance teams within a single application to win more deals, accelerate delivery, empower employees, and maximize profitability. Dynamics 365 Guides prioritizes updates in this release wave on intelligent workflows. By taking advantage of data and AI innovations, work instructions can be configured to adjust on the fly based on operator inputs. In addition, insights will make it easier to use time-tracking data and connect that data to your business. Dynamics 365 Business Central investments for this release wave include service enhancements to meet the demands of a rapidly growing customer base, improved performance, handling of file storage, geographic expansion together with support for Group VAT, top customer requested features, and deeper integration with Microsoft Teams.  Highlights from Power Platform  Power Apps includes significant improvements for Power Apps developers of all skill levels, improving the sophistication and usability of apps that are created across the web and mobile devices. Makers will be able to create Power Apps directly within Microsoft Teams in order to easily customize the Teams experience. Makers will also be able to add custom pages to model-driven apps using the app designer, bringing together the best of canvas and model capabilities, including creating custom layouts and components. Power Apps portals adds Microsoft Power Virtual Agents as a component in the Power Apps portals studio, as well as support for code components created using Power Apps component framework. AI Builder introduces new AI scenarios for receipt scanning and translation, and improvements to connect to remote training data. The AI builder home page and model details page are updated to improve discoverability and integration with Power Apps and Power Automate. Power BI is investing in three key areas that drive a data culture: amazing data experiences, modern enterprise BI, and insights where decisions are made. Power BI Desktop includes many new capabilities for users to create content quickly and easily, enabling authors to empower their users, enterprise grade content creation, and AI-infused authoring experiences. Power BI Mobile adds split view support for iPad and Power BI Service integrates with Azure Synapse to automatically create and manage materialized views on larger Power BI models as well as enhanced integration with SharePoint lists to build additional custom reports. We continue to enhance our Data protection capabilities enabling customers to classify and label sensitive data. Power Automate enhancements will combine the best of WinAutomation with the cloud-based AI builder and connector-based capabilities in automated flows. This new version will offer customers a way to automate everything from Office apps to legacy terminal applications that haven\u0026rsquo;t been updated in decades. In addition, enhancements to automated flows running in the cloud will include richer automation and approval experiences. Power Virtual Agents brings expanded capabilities in the authoring experience including tools to create richer content, Adaptive Cards capabilities, topic suggestions from documents, improved Power Automate integration, voice integration with smart speakers, theming to customize the look and feel of the bot, and much more.  For a complete list of new capabilities, please check out the Dynamics 365 and Power Platform 2020 release wave 2 plans.\nEarly access period Starting August 3, 2020, customers and partners will be able to validate the latest features in a non-production environment. These features include user experience enhancements that will be automatically enabled for users in production environments during October 2020. Take advantage of the early access period, try out the latest updates in a non-production environment, and get ready to roll out updates to your users with confidence. To see the early access features, check out the Dynamics 365 and Power Platform pages. For questions, please visit the Early Access FAQ page.\nWe\u0026rsquo;ve done this work to help you\u0026mdash;our partners, customers, and users\u0026mdash;drive the digital transformation of your business on your terms. Get ready and learn more about latest product updates and plans, and share your feedback in the community forum for Dynamics 365 or Power Platform.\n","permalink":"https://nuxulu.com/posts/2020-08-15-dynamics365/","summary":"Today, we published the 2020 release wave 2 plans for Microsoft Dynamics 365 and Microsoft Power Platform, a compilation of new capabilities that will be released between October 2020 and March 2021. This second release wave of the year offers hundreds of new features and enhancements, demonstrating our continued investment to power digital transformation for our customers and partners.\nHighlights from Dynamics 365  Dynamics 365 Sales includes updates for more simplified experiences; including collaboration tools, a new mobile experience for quick access to customer information, and new enhancements to forecasting to natively create and manage bottom-up sales forecast processes.","title":"Microsoft Dynamics 365"},{"content":" In GOLD, export GOLD bak file In MIG, stop all services: Batch, DIXF, MR In MIG, backup AXDB MIG in MIG, AxDB to AxDB_Orgi In MIG, Create AxDB_New In MIG,resotre GOLD to AxDB_New AxDB_Gold to AxDB Start all the services and resett IIS in LCS DB Sync  ALTER DATABASE AxDB SET Single_user With Rollback immediate alter database AxDB Modify name = AxDB_Orig alter database AxDB_Orig SET MULTI_USER alter database AxDB_Gold Modify name = AxDB select * from [USERINFO] ","permalink":"https://nuxulu.com/posts/2020-07-14-database-movement-tier1-to-tier1/","summary":" In GOLD, export GOLD bak file In MIG, stop all services: Batch, DIXF, MR In MIG, backup AXDB MIG in MIG, AxDB to AxDB_Orgi In MIG, Create AxDB_New In MIG,resotre GOLD to AxDB_New AxDB_Gold to AxDB Start all the services and resett IIS in LCS DB Sync  ALTER DATABASE AxDB SET Single_user With Rollback immediate alter database AxDB Modify name = AxDB_Orig alter database AxDB_Orig SET MULTI_USER alter database AxDB_Gold Modify name = AxDB select * from [USERINFO] ","title":"Database movement from Tier 1 to Tier 1"},{"content":"","permalink":"https://nuxulu.com/posts/2020-07-08-ms-600-extending-sharepoint/","summary":"","title":"MS-600 Extending Sharepoint"},{"content":"For the upcoming Dynamics 365 Finance Updates, Visual Studio 2017 and .NET runtime 4.7.2 required for PU36/10.0.12 or higher; New VMs deployed with PU36/10.0.12 or higher will have Visual Studio 2017 as well as .NET runtime 4.7.2 already installed.\nFor your dev/test/build Tier1 VMs, Microsoft recommends just redeploying a new VM, and you will be all set!\nIf you cannot deploy new VMs, please follow below steps to update .NET runtime, VS 2017 before installing PU36/10.0.12 or higher, this will be applied for cloud-hosted environment\n1. Download VS Professional 2017 15.9 here and install it 2. The .NET runtime download is available here by clicking on the Download .NET Framework 4.7.2 Runtime and running the installation, restart required. 3. Go to Dynamics Lifecycle Services and download PU36/10.0.12 or any higher package which is a part of the Platform and application binary, the VSIX file is located in the DevToolsService\\Scripts folder. {:.border}\nYou need to install Microsoft.Dynamics.Framework.Tools.Installer and then Microsoft.Dynamics.Framework.Tools.InternalDevTools. After the installation, open the VS2017 if you should see Dynamics 365 menu extension like below\n{:.border}\n4. If you try to access the client before applying the package you will get the error like below in event viewer and unable to access the environment, so you need to apply an update first. AX is shutting down due to an error. Serialization version mismatch detect, make sure the runtime dlls are in sync with the deployed metadata. Version of file \u0026lsquo;194\u0026rsquo;. Version of dll \u0026lsquo;193\u0026rsquo;. {:.error}\n5. In LCS, navigate to your cloud-hosted environment, and apply Platform and application binary package PU36/10.0.12 or higher. After completed upgrading, you will be able to access the environment client. Overall, there is no change in the compiler, metadata; this is only an update to the Visual Studio extensions and NET runtime for the tier1 VMs.\n","permalink":"https://nuxulu.com/posts/2020-06-30-how-to-upgrade-to-visual-studio-2017-.net-4.7.2/","summary":"For the upcoming Dynamics 365 Finance Updates, Visual Studio 2017 and .NET runtime 4.7.2 required for PU36/10.0.12 or higher; New VMs deployed with PU36/10.0.12 or higher will have Visual Studio 2017 as well as .NET runtime 4.7.2 already installed.\nFor your dev/test/build Tier1 VMs, Microsoft recommends just redeploying a new VM, and you will be all set!\nIf you cannot deploy new VMs, please follow below steps to update .NET runtime, VS 2017 before installing PU36/10.","title":"How to upgrade to Visual Studio 2017, .NET 4.7.2 for PU36/10.0.12 and higher"},{"content":"Example two: A new Forms response is submitted then Flow promts for an approval request before a new record is created in Vendors table In the last example, we created a simple Flow to create a new Vendor Account from submitted Forms responses. The process was made as simple as it could be: When a new Forms response if submitted, a new Vendor Account will be created on D365FO client.\nTo make the process more reliable, we will now add an approval step to the Flow. It means before a new Vendor Account is created, the user will have the right to Approve or Reject the request.\nStep 1: Add a new step to the Flow The flow that we have is like this.\n{:.border}\nWe will add a step right before the new record is created.\n{:.border}\nWe will go with Approval \u0026gt; Start and wait for an approval.\n{:.border}\nThis step will offer several types of approval in which the most common ones are All users must approve or First approve/reject from any user. To make it simple, we will go with First approval because in this example, we will have only one user.\nAs straightforward as they seem, the fields explain themselves. We will start an approval request, set a title for it and assign it to a user (only users in the same organisation as you).\n{:.border}\nNote 2-1\n Almost all fields including Title, Details, etc. are fully customisable by using Dynamic content.\n The flow now will look like this.\n{:.border}\nStep 2: Testing out Trigger a test run of the flow.\n{:.border}\n{:.border}\nThe test run is ready, waiting for the input from MS Forms.\n{:.border}\nSubmit a new response.\n{:.border}\nThe flow will run. The process will show that an approval is being waited from the assigned user.\n{:.border}\nAn approval request will be sent to the mailbox of the assigned user. That user can check the request in Office.com \u0026gt; Outlook.\n{:.border}\nUpon approval, the flow will finish its pending step and a new record will be created.\n{:.border}\nExample three: From the created record in Vendors table, add related records in other tables In the last example, we added an Approval step to the Flow which will allow users to give Approve or Reject action toward the request coming from the Form.\nWe can actually utilise the Flow one step further: To create additional records on different tables that might depend on the created record in VendTable.\nTo make it easier to understand, the process is like this:\n Create a new Vendor Account by using input from the Form. Create a new Bank Account with the Bank Account ID exactly similar to the Vendor Account. Add some contact details to the Vendor Account.  Which entities should we target to? They are:\n For Vendor Bank Account: VendorBankAccounts For Vendor Contact Details: PartyContacts  You can play around in Visual Studio to get used to looking for an Entity that might fulfil the requirements of your intergration. Check out the next part of this article to get some quick tips into that.\nAssuming that we now have a Flow as we configured in the last two examples, we can add some more steps like these underneath.\nStep 1: Add a new step to add a new record to VendorBankAccounts and PartyContacts Given the case that we had a Vendor Account and Bank Account for that Vendor Account, the records would be like this.\nThe relation should be: VendTable.AccountNum = VendBankAccount.VendAccount\nOn the client, the information should be illustrated like this.\nAnd, when checking the Bank Account details, we would find.\nNow, to do so, we will add a new step right after the creation of VendTable record.\n Instance = Environment URL (same as any other steps) Entity name = VendorBankAccounts (check my post about good tips with Visual Studio to know how to find the Data Entity) Supplier account = Supplier account from last step Bank account= Supplier account from last step (Because I would like to have same Vendor Account and Bank Account)  When saying \u0026ldquo;from the last step\u0026rdquo;, I meant this.\nIn MS FLow, any steps that are already executed will have outputs. We, in this step, are taking the Supplier Account from the last step of creating a new record in VendTable . This dynamic content was generated when the new record is created.\nNote 3-1\n Keep in mind that if the last step is not \u0026ldquo;Creating a new record\u0026rdquo;, chance is that you cannot take the output from it because there\u0026rsquo;s simply none.\n Click on Show advanced options so we can add more information to the Vendor Bank Account.\nAdd more details to the bank account:\n Name = Bank account name chosen from the Form using Dynamic Content (we don\u0026rsquo;t want to copy any more values from the VendTable record - we actually will specify a bank account name when composing a response on the Form) Bank account number = Bank account number chosen from the Form using Dynamic Content  We finished with Vendor Bank Account.\nWe will do just the same with the record in PartyContacts.\n Party ID = Party ID from VendTable record Contact number/address = Email from the Form (in this case, I would like to add an email address - it is totally up to you to opt to add phone number, Skype, Twitter and some other contact detail that D365FO is supporting) Purpose = I am \u0026ldquo;hard-coding\u0026rdquo; this \u0026quot;Business\u0026quot; (the supported purpose of Contact can be found on Vendor Card on D356FO client - this purpose should mainly be used when sending emails, i.e. D365FO want to send Purchase Order Confirmation to any email addresses with purpose \u0026quot;Invoice\u0026quot;)  Step 2: Testing out Form input.\nResults on D365FO client.\nThere you go, simple integration between MS Forms, MS Automate and D365FO client. I am seeing that Microsoft is improving such by adding more actions on MS Automate. I believe we will be able to do many more jobs with the built-in utilities of MS Automate.\nHow to determine the Data Entity that should be used in the Power Automate The names of the target entity on D365FO client and being found in Power Automate should be different. You can always find them out by testing several import/export but, with the help of Visual Studio, you will be able to save a lot of time. This, however, requires a certain extent of experience working with data integration on D365FO.\nGiven the case that we would like to import data to All Vendors, we will start our search in Workspaces \u0026gt; Data Management \u0026gt; Data Entities. Apply several filters here and there, and looking for potential ones that we usually do, we will finally end our search at the VendVendorV2Entity\n{:.border}\nMaking our way to Visual Studio, it is not a difficult task for us to find out the Entity.\n{:.border}\nCheck out the Properties, we will need to get details in the Public group.\n{:.border}\nIn Public:\n Is Public: If it is Yes, the entity can be found in Power Automate; otherwise, No. Public Collection Name: Entity name on Power Automate.  The information should be displayed here:\n{:.border}\nFurthermore, by checking the Entity\u0026rsquo;s fields, we can get the information of mapping between the Entity and the physical table VendTable.\n{:.border}\nGive yourself some time to play around between MS Forms, MS Automate and Visual Studio. You will finally see the relations between them.\n","permalink":"https://nuxulu.com/posts/2020-06-28-dynamics-365-finops-data-integration-using-microsoft-forms-part-2/","summary":"Example two: A new Forms response is submitted then Flow promts for an approval request before a new record is created in Vendors table In the last example, we created a simple Flow to create a new Vendor Account from submitted Forms responses. The process was made as simple as it could be: When a new Forms response if submitted, a new Vendor Account will be created on D365FO client.","title":"Dynamics 365 FinOps Data Integration using Microsoft Forms (Part 2)"},{"content":"With any D365FO users, Data Management workspace and Data Entities should have become one of the most used and well-known tools in the system. We all agree how powerful such integration framework is, especially when it comes to a large number of records that we would need to import into different tables.\nThe classic method that we have been utilizing is to include all source data in a single (or multiple, much depending on the purpose and order of data integration) Excel sheet, trigger an import execution, pass any validation layers that present and ultimately have the data available on D365FO. Now, I would like to propose another method that might come useful in some circumstances: Using Microsoft Forms.\n{:.border}\nWhen This method should be at its finest when you are planning to outsource the input to any external users.\nImagine you are a company who is using D365FO to leverage your daily work. You now want to add many local vendors to your system in form of new Vendor Accounts. It is fine to collect all information from them (i.e. Vendor Company’s name, their address, their contact details and so on), put everything in an Excel sheet and import it. Yet, chance is that it would take (a lot of) time for you to finish this task. You would then be very likely to try coming up with some methods to cut down time for it.\nThat demand should raise two questions:\n How can we let the vendors enter their information by themselves? And, how can we then have all such information in our system?  What What should be needed?\n Microsoft Forms: To establish a process in which you will publish a prepared form. Any vendors out there will have the access to it and they can fill any fields on in. Microsoft Power Automate aka. Microsoft Flow: To set up a protocol to push the input information (form responses) to the target (D365FO Data Entities). (Optional) Access to Visual Studio on your virtual machine: To help you determine the target Data Entities more easily and efficiently.  How The flow should go like this.\n{:.border}\nTo make it short, when a response is submitted in Microsoft Form, the Flow in MS Power Automate is triggered automatically. An approval request will be sent to your Outlook mailbox and if it is approved, a new record will be created in the target data entity.\nLet go through three examples in the next articles in the series (from the simplest to a more complicated) so we will see the logic behind this process.\nAs easy as it sounds, in this example, we will create a new Vendor Account from Microsoft Forms with only a small number of fields being populate.\nNote\n To help simplify the example, the number of fields is minimised. In other words, we will import just enough fields so the new Vendor Account is valid.\n  Given that, only Vendor Account and Vendor Group are chosen to be imported.\n Step 1: Create a MS Form To do so, we will access Microsoft Forms and create a new form. To make the new form usable, we will create two questions asking for inputs of Vendor Account and Vendor Group.\n{:.border}\nNote all information on the form is customisable.\nNote 1-1\n We can make the questions\u0026hellip;any questions that suit the purpose of the form. Say, in reality, the question can be \u0026quot;What is your company's name?\u0026quot; if you want to send this form to any external vendor users. The question does not change the usability of the returned response\u0026rsquo;s value. It can still be mapped to VendAccount not matter what the question is.\n Note 1-2\n The second question is being set in form of a Choice question. The given choices should match with available Vendor Group values found on your D365FO client. If this question is a Text one, chance is that its response might not match with any available Vendor Group values, thus, will return an error during Flow runtime.\n  Vendor Groups can be found under Account Payable \u0026gt; Vendors \u0026gt; Vendor Groups. In this example, TopVendor and Others are two available Vendor Groups in my D365FO client.\n {:.border}\nStep 2: Create a MS Flow To do so, we will access Microsoft Power Automate and create a new flow. We will go with an Automated one in this case.\n{:.border}\nSearch for form and choose the trigger When a new response is submitted. Hit Create to create a new flow.\n{:.border}\nChoose the form that we just created using the dropdown list.\n{:.border}\nPress New step, search for form and go with Get response details.\n{:.border}\nIn the new step, choose the Form ID. As soon as we move the cursor to the Response ID field, the FLow will suggest the Dynamics content that we can use. In this case, there should be only one dynamic content, which is List of response notifications Response ID. We will go with it by choosing it.\n{:.border}\nAfter doing so, the flow should look like this.\n{:.border}\nNow, the reponse details will be sent from the Form to the Flow when a new response is submitted. Next, we will set up the step to create a new Vendor Account.\nCreate a new step, search for dynamics and go with Dynamics 365 for Finance and Operations.\n{:.border}\nChoose Create record action.\n{:.border}\nChoose the Dynamics 365 FinOps Instance from the dropdown list. If your client does not appear here, choose Enter custom value. For privacy, environments' name will be censored in our example.\n{:.border}\nThe Instance should be the link to your D365FO client. Set Entity name VendorsV2.\n{:.border}\nNote 1-3\n Later in this series, we will see how to find the exact Entity name using Visual Studio. Also, we will see in which condition, the Entity can be used in Power Automate.\n Populate the fields that we want to import. In this example, we will populate only 3 fields: Group (VendorGroupID), Company (DataAreaId) and Supplier Account (VendorAccountNumber).\nDynamic content will be suggested automatically when the cursor is placed in each field.\n{:.border}\nWe can map Form fields with Automate fields or hardcode in these fields:\n Group = Response of What is the Vendor Group? question. Company = (hardcode) usmf. Supplier Account = Response of What is the Vendor Account? question. Name = Supplier Account = Response of What is the Vendor Account? question (Show advanced option on the Flow to find the field Name (VendorOrganisationName)). These fields should be required when creating a new Vendor Account on D365FO client.  {:.border}\nWe are done here with the set up.\nStep 3: Testing out On Forms, choose the form that we just created, click on Preview so we can submit a form response.\n{:.border}\nCheck out All Vendors (VendTableListPage) on the client to find a new Vendor Account is created.\n{:.border}\nNote 1-4\n Use Test option in Power Automate to follow the process of the Flow. This, in other words, is so-called \u0026quot;Run Flow with Debug\u0026quot; option.\n ","permalink":"https://nuxulu.com/posts/2020-05-24-dynamics-365-finops-data-integration-using-microsoft-forms-part-1/","summary":"With any D365FO users, Data Management workspace and Data Entities should have become one of the most used and well-known tools in the system. We all agree how powerful such integration framework is, especially when it comes to a large number of records that we would need to import into different tables.\nThe classic method that we have been utilizing is to include all source data in a single (or multiple, much depending on the purpose and order of data integration) Excel sheet, trigger an import execution, pass any validation layers that present and ultimately have the data available on D365FO.","title":"Dynamics 365 FinOps Data Integration using Microsoft Forms (Part 1)"},{"content":"You will got an error like this when doing Database Synchronization after changing the data type of the field on the table:\nInvalidOperationException: Table \u0026hellip; : Converting Field \u0026lsquo;..\u0026rsquo; of Type \u0026lsquo;..\u0026rsquo; to \u0026lsquo;..\u0026rsquo; is not support. Please drop the original field, sync the table and add new field with same name if needed. {:.error}\nSolution:\n Drop the table and delete references from SQLDictionary  DROP TABLE AXDB.dbo.NAMEOFTABLE DELETE FROM AXDB.dbo.SQLDICTIONARY WHERE TABLEID IN ( SELECT TABLEID FROM SQLDICTIONARY WHERE NAME = \u0026#39;NAMEOFTABLE\u0026#39; AND FIELDID = 0 )  Restart IIS In VS 2015, run DB sync again.  Thank you for reading.\n","permalink":"https://nuxulu.com/posts/2020-05-10-database-sync-after-data-type-on-field-has-been-changed/","summary":"You will got an error like this when doing Database Synchronization after changing the data type of the field on the table:\nInvalidOperationException: Table \u0026hellip; : Converting Field \u0026lsquo;..\u0026rsquo; of Type \u0026lsquo;..\u0026rsquo; to \u0026lsquo;..\u0026rsquo; is not support. Please drop the original field, sync the table and add new field with same name if needed. {:.error}\nSolution:\n Drop the table and delete references from SQLDictionary  DROP TABLE AXDB.dbo.NAMEOFTABLE DELETE FROM AXDB.","title":"Database Sync after data type on field has been changed"},{"content":"1. Download Dynamics 365 finance and operations VHD files   Go to the LCS main page and select Shared asset library or go to Shared Asset Library.\n  Select the asset type Downloadable VHD.\n  Find the VHD you are looking for based on the desired Finance and Operation version. The VHD is divided into multiple file parts that you need to download. For example, the asset files that start with \u0026ldquo;VHD - 10.0.5\u0026rdquo; are the different files you need in order to install version 10.0.5.\n  Download all files (parts) associated with the desired VHD to a local folder.\n  After the download is complete, run the executable file that you downloaded, accept the software license agreement, and choose a file path to extract the VHD to.\n  This creates a local VHD file that you can use to run a local virtual machine.\n  Sign in to the VM by using the following credentials:\n User name: Administrator Password: pass@word1    Provision the administrator user.\n  2. Rename VM   Rename and restart the machine before you start development or connect to Azure DevOps.\n  Update the server name in SQL Server\n  To be able to login, Start SQL Server with administrator or using the user axdbadmin has password AOSWebSite@12\n  Run following query\nsp_dropserver [old_name] sp_addserver [new_name], local   Restart SQL service\n    Open Reporting Services Configuration Manager for SQL Server 2016, then Select Database, select Change Database, and use the new server name.\n  Update the Azure Storage Emulator\n  From the Start menu, open Microsoft Azure Storage Emulator - v4.0, and run the following commands.\nAzureStorageEmulator.exe start\n If you got an error Port conflict with existing application, please check this post.\n   This command verifies that the emulator is running.\nAzureStorageEmulator.exe status\n  Update the server name\nAzureStorageEmulator.exe init -server new_name\nFor more information about Azure storage emulator please follow https://docs.microsoft.com/en-us/azure/storage/common/storage-use-emulator\n  Update financial reporting\n  Open a Microsoft Windows PowerShell command window as an admin, and run the following command. This command contains the default passwords that might have to be updated. Be sure to replace new_name with the new name.\ncd \u0026lt;update folder\u0026gt;\\MROneBox\\Scripts\\Update .\\ConfigureMRDatabase.ps1 -NewAosDatabaseName AxDB -NewAosDatabaseServerName new_name -NewMRDatabaseName ManagementReporter -NewAxAdminUserPassword AOSWebSite@123 -NewMRAdminUserName MRUser -NewMRAdminUserPassword MRWebSite@123 -NewMRRuntimeUserName MRUSer -NewMRRuntimeUserPassword MRWebSite@123 -NewAxMRRuntimeUserName MRUser -NewAxMRRuntimeUserPassword MRWebSite@123   3. Location of packages, source code, and other AOS configurations On a VM, you can find most of the application configuration by opening the web.config file of AOSWebApplication.\n  Start IIS.\n  Go to Sites \u0026gt; AOSWebApplication.\n  Right-click, and then click Explore to open File Explorer.\n  Open the web.config file in Notepad or another text editor. The following keys are of interest to many developers and administrators:\n  Aos.MetadataDirectory – This key points to the location of the packages folder that contains platform and application binaries, and also source code. (Source code is available only in development environments.) Typical values are: c:\\packages, c:\\AosServicePackagesLocalDirectory, and J:AosServicePackagesLocalDirectory.\n  DataAccess.Database – This key holds the name of the database.\n  Aos.AppRoot – This key points to the root folder of the Application Object Server (AOS) web application.\n    4. Redeploying or restarting the runtime on the VM To restart the local runtime and redeploy all the packages, follow these steps.\n  Open File Explorer, and go to C:\\CustomerServiceUnit.\n  Right-click AOSDeploy.cmd, and then click Run as administrator.\n  This process might take a while. The process is completed when the cmd.exe window closes. If you just want to restart AOS (without redeploying the runtime), run iisreset from an administrator Command Prompt window, or restart AOSWebApplication from IIS.\n5. Update to the latest version Please check this document https://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/deployment/install-deployable-package\nThat\u0026rsquo;s it, thank you for reading.\n","permalink":"https://nuxulu.com/posts/2020-04-06-getting-onebox-vhd-dynamics-365-finance-and-operations-virtual-machine/","summary":"1. Download Dynamics 365 finance and operations VHD files   Go to the LCS main page and select Shared asset library or go to Shared Asset Library.\n  Select the asset type Downloadable VHD.\n  Find the VHD you are looking for based on the desired Finance and Operation version. The VHD is divided into multiple file parts that you need to download. For example, the asset files that start with \u0026ldquo;VHD - 10.","title":"Getting onebox VHD Dynamics 365 finance and operations virtual machine"},{"content":"The Microsoft Azure storage emulator is a tool that emulates the Azure Blob, Queue, and Table services for local development purposes. You can test your application against the storage services locally without creating an Azure subscription or incurring any costs. When you\u0026rsquo;re satisfied with how your application is working in the emulator, switch to using an Azure storage account in the cloud.\nIn Dynamics 365 finance and operations onebox environment, we also use Microsoft Azure Storage Emulator for same purposes. There is a common problem when you try to start its service or using Data entity Import/Export\n{:.border}\nPort conflict with existing application {:.error}\nReason By default, Azure storage emulator is using port number 10000, 10001, 10002 for Blob, Queue and Table services respectively. And there is a progress/system takes those ports already, so you are not able to start its service. We can simply use this command in CMD to see which one has conflict\nnetstat -p tcp -ano | findstr :10001\n{:.border}\nThe process ID 4 is currently taking port 10001.\nResolution You can either terminate the conflict program or change the default port for Azure Storage Emulator.\nTo determine the conflict program we can use this command in CMD Run following command to check which application/process has the same port, then simply go to task manager and terminate it.\ntasklist /fi \u0026quot;pid eq 4\u0026quot;\n{:.border}\nTo change the default port for Azure storage Emulator Go to your storage emulator default installation folder, which is located at C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\Storage Emulator, open the AzureStorageEmulator.exe.config by notepad and edit the port number to different number\n{:.border}\nWhen done, try to start its service again using\nAzureStorageEmulator.exe start\n{:.border}\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2020-04-05-azure-storage-emulator-port-conflict-with-existing-application/","summary":"The Microsoft Azure storage emulator is a tool that emulates the Azure Blob, Queue, and Table services for local development purposes. You can test your application against the storage services locally without creating an Azure subscription or incurring any costs. When you\u0026rsquo;re satisfied with how your application is working in the emulator, switch to using an Azure storage account in the cloud.\nIn Dynamics 365 finance and operations onebox environment, we also use Microsoft Azure Storage Emulator for same purposes.","title":"Azure Storage Emulator 'Port conflict with existing application', Dynamics 365 finance and operations"},{"content":"class MaxGeneratePO { public static void main(Args _args) { int i = 0; // number of purchase orders  NumberSeq numberSeq; PurchTable purchTable; PurchLine purchLine; InventDim inventDim; while (i \u0026lt;= 3) { ttsBegin; MaxGeneratePO createPO = new MaxGeneratePO(); numberSeq = NumberSeq::newGetNum(PurchParameters::numRefPurchId()); numberSeq.used(); purchTable.PurchId = numberSeq.num(); purchTable.initValue(); purchTable.initFromVendTable(VendTable::find(\u0026#39;US-101\u0026#39;)); if (!purchTable.validateWrite()) { throw Exception::Error; } purchTable.insert(); inventDim.clear(); purchLine.clear(); purchLine.initValue(); purchLine.PurchId = purchTable.PurchId; purchLine.ItemId = \u0026#39;D0002\u0026#39;; inventDim.InventSiteId = \u0026#34;1\u0026#34;; inventDim.InventLocationId = \u0026#34;11\u0026#34;; purchLine.InventDimId=InventDim::findOrCreate(inventDim).inventDimId ; purchLine.createLine(true, true, true, true, true, true); purchLine.PurchQty = 5; purchLine.PurchUnit = \u0026#34;ea\u0026#34;; purchLine.PurchPrice = createPO.randomAmount(); // get random amount nubmer  purchLine.LineAmount = purchLine.calcLineAmount(); purchLine.update(); //PO confirm  PurchFormLetter purchFormLetter; PurchFormLetter purchFormLetterPack; purchFormLetter = PurchFormLetter::construct(DocumentStatus::PurchaseOrder); purchFormLetter.update(purchTable, strFmt(\u0026#34;ConNum_%1\u0026#34;, purchTable.PurchId), systemDateGet(), PurchUpdate::All, AccountOrder::None, NoYes::No, NoYes::no); //Product receipt  createPO.proceed(purchTable.PurchId, purchLine.ItemId,purchLine.InventDimId,purchLine.PurchQty,strFmt(\u0026#34;RptNum_%1\u0026#34;, purchTable.PurchId)); //Post PO  createPO.postPOInvoice(purchTable.PurchId, strFmt(\u0026#34;RptNum_%1\u0026#34;, purchTable.PurchId)); info(strFmt(\u0026#34;Purchase order \u0026#39;%1\u0026#39; has been created\u0026#34;, purchTable.PurchId)); ttsCommit; i++; } } public boolean proceed(PurchId _purchId, ItemId _itemId,inventDimId _inventDimId, PurchQty _qty, PackingSlipId _productReceiptNumber) { return this.generateProductReceipt(_purchId, this.addToPurchLineList(_purchId, _itemId, _inventDimId, _qty), _productReceiptNumber); } public boolean generateProductReceipt(PurchId _purchId, List _purchLineList, PackingSlipId _productReceiptNumber) { boolean ret = true; PurchFormLetter purchFromLetter; PurchTable purchTable = PurchTable::find(_purchId); try { ttsbegin; purchFromLetter = PurchFormLetter::construct(DocumentStatus::PackingSlip); purchFromLetter.createFromLines(true); purchFromLetter.parmLineList(_purchLineList.pack()); purchFromLetter.update(purchTable, _productReceiptNumber, DateTimeUtil::getToday(DateTimeUtil::getUserPreferredTimeZone()), PurchUpdate::All); ttscommit; } catch { ret = false; } return ret; } public List addToPurchLineList(PurchId _purchId, ItemId _itemId,inventDimId _inventDimId, PurchQty _qty) { List purchLineList = new List(Types::Record); PurchLine purchLine = PurchLine::findItemIdInventDimId(_purchId, _itemId, _inventDimId); if(purchLine \u0026amp;\u0026amp; _qty \u0026gt; 0) { purchLine.PurchReceivedNow = _qty; purchline.modifiedField(fieldNum(PurchLine, PurchReceivedNow)); purchLineList.addEnd(purchLine); } return purchLineList; } public void postPOInvoice(PurchId purchId, PackingSlipId packingSlipId) { TmpFrmVirtual tmpFrmVirtualVend; PurchFormLetter_Invoice purchFormLetter; VendPackingSlipJour vendPackingSlipJour; SysQueryRun chooseLinesQuery; SysQueryRun chooseLinesPendingInvoiceQuery; container conTmpFrmVirtual; List selectedList = new List(Types::Record); select firstonly vendPackingSlipJour where vendPackingSlipJour.PurchId == purchId \u0026amp;\u0026amp; vendPackingSlipJour.PackingSlipId == packingSlipId; if (vendPackingSlipJour) { tmpFrmVirtualVend.clear(); tmpFrmVirtualVend.TableNum = vendPackingSlipJour.TableId; tmpFrmVirtualVend.RecordNo = vendPackingSlipJour.RecId; tmpFrmVirtualVend.NoYes = NoYes::Yes; tmpFrmVirtualVend.Id = vendPackingSlipJour.PurchId; tmpFrmVirtualVend.insert(); } chooseLinesQuery = new SysQueryRun(queryStr(PurchUpdate)); chooseLinesQuery.query().addDataSource(tableNum(VendInvoiceInfoTable)).enabled(false); // chooseLinesPendingInvoiceQuery needs to be initialized, although it will not be used  chooseLinesPendingInvoiceQuery = new SysQueryRun(queryStr(PurchUpdatePendingInvoice)); chooseLinesPendingInvoiceQuery.query().dataSourceTable(tableNum(PurchTable)).addRange(fieldNum(PurchTable,PurchId)).value(queryValue(\u0026#39;\u0026#39;)); purchFormLetter = PurchFormLetter::construct(DocumentStatus::Invoice); purchFormLetter.chooseLinesQuery (chooseLinesQuery); purchFormLetter.parmQueryChooseLinesPendingInvoice(chooseLinesPendingInvoiceQuery); purchFormLetter.purchTable (PurchTable::find(PurchId)); purchFormLetter.transDate (systemDateGet()); purchFormLetter.parmParmTableNum (strFmt(\u0026#34;%1\u0026#34;,packingSlipId)); //This is invoice number  purchFormLetter.printFormLetter (NoYes::No); purchFormLetter.sumBy (AccountOrder::Auto); purchFormLetter.specQty (PurchUpdate::PackingSlip); while select tmpFrmVirtualVend { selectedList.addEnd(tmpFrmVirtualVend); conTmpFrmVirtual = selectedList.pack(); } purchFormLetter.selectFromJournal(conTmpFrmVirtual); purchFormLetter.reArrangeNow(true); purchFormLetter.run(); } public int randomAmount() { RandomGenerate randomGenerate; randomGenerate = RandomGenerate::construct(); randomGenerate.parmSeed(new Random().nextInt()); return RandomGenerate.randomInt(100, 800); } } Thank you for reading.\n","permalink":"https://nuxulu.com/posts/2020-01-25-create-po-confirm-receipt-post/","summary":"class MaxGeneratePO { public static void main(Args _args) { int i = 0; // number of purchase orders  NumberSeq numberSeq; PurchTable purchTable; PurchLine purchLine; InventDim inventDim; while (i \u0026lt;= 3) { ttsBegin; MaxGeneratePO createPO = new MaxGeneratePO(); numberSeq = NumberSeq::newGetNum(PurchParameters::numRefPurchId()); numberSeq.used(); purchTable.PurchId = numberSeq.num(); purchTable.initValue(); purchTable.initFromVendTable(VendTable::find(\u0026#39;US-101\u0026#39;)); if (!purchTable.validateWrite()) { throw Exception::Error; } purchTable.insert(); inventDim.clear(); purchLine.clear(); purchLine.initValue(); purchLine.PurchId = purchTable.PurchId; purchLine.ItemId = \u0026#39;D0002\u0026#39;; inventDim.InventSiteId = \u0026#34;1\u0026#34;; inventDim.InventLocationId = \u0026#34;11\u0026#34;; purchLine.InventDimId=InventDim::findOrCreate(inventDim).inventDimId ; purchLine.","title":"Create Purchase Orders - Confirm - Product receipt - Post PO using X++"},{"content":"When you consume a custom data entity, you get an error\nerrorSystem.ArgumentOutOfRangeException : Length cannot be less than zero” and it works fine for standard data entities. {:.error}\n{:.border}\nThe reason is the temporary XML file where the metadata stored which mismatches with the metadata from https://\u0026lt;yourenvironment\u0026gt;.cloudax.dynamics.com/data/$metadata\nYou can follow this post to understand how to create the XML file.\nIf you try to regenerate the metadata by saving the ODataClient.tt file, the XML will be accumulated, and the error keeps happening. The resolution here is simply delete it and regenerate metadata again by saving the ODataClient.tt file\n{:.border}\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2019-12-17-dynamics-365-finance-and-operations-odata-consuming-length-cannot-be-less-than-zero/","summary":"When you consume a custom data entity, you get an error\nerrorSystem.ArgumentOutOfRangeException : Length cannot be less than zero” and it works fine for standard data entities. {:.error}\n{:.border}\nThe reason is the temporary XML file where the metadata stored which mismatches with the metadata from https://\u0026lt;yourenvironment\u0026gt;.cloudax.dynamics.com/data/$metadata\nYou can follow this post to understand how to create the XML file.\nIf you try to regenerate the metadata by saving the ODataClient.","title":"Dynamics 365 finance and operations ODATA consuming - Length cannot be less than zero"},{"content":"In this article, we will go through how to enable Power BI embedded in Dynamics 365 finance and operations version 10 in a cloud-hosted environment (customer managed). From the previous version of FinOps, Power BI embedded uses workspace collections at Azure to display the report, since the workspace collections have been deprecated, Microsoft also disables power BI embedded in cloud-hosted environments. You can only enable Power BI embedded in multiple boxes environments like UAT and production.\n1. Create workspace collections Although the workspace collections have been deprecated, it does not show in the Azure portal and you could not create it by using the Azure portal, but you can create and manage by using Azure CLI.\nOpen PowerShell and run following\n// Allow policy set-executionpolicy remotesigned //Install AzureRM Install-Module -Name AzureRM -AllowClobber // Login to Azure using credentials Login-AzureRmAccount // select the subscription ID Select-AzureRmSubscription -SubscriptionId $subscriptionId $ResourceGroupName = “MaxWorkspaceCollections” $Location = \u0026#34;Southeast Asia\u0026#34; // Create workspace collections New-AzPowerBIWorkspaceCollection -ResourceGroupName $ResourceGroupName -WorkspaceCollectionName $WorkSpaceCollectionName -Location $Location // Obtain the access keys Get-AzPowerBIWorkspaceCollection -ResourceGroupName $ResourceGroupName -name \u0026#34;\u0026lt;yourWorkspacename\u0026gt;\u0026#34; 2. Create AxWD Azure SQL Database We must use the Azure SQL Database for the AxDW in Dynamics 365 finance and operations cloud-hosted environment. Please follow this document to create Azure SQL DB.\nWe need at least 5 GB storage for Database, for Pricing tier, I will recommend using from S1, and name the database AxDB\nThis is my DB property\n{:.border}\nI’m using Premium tier because I want to use the Columnstore Clustered indexes in Database; it helps performance a little bit faster. Once you have the Azure SQL Database, we can use SSMS to connect to the database and create a user for that DB. You need to get the user and password information in LCS, where the cloud-hosted provisioned.\n{:.border}\nUse Master CREATE LOGIN axdwadmin WITH PASSWORD = \u0026#39;\u0026#39;; CREATE LOGIN axdwruntimeuser WITH PASSWORD = \u0026#39;\u0026#39;; Use AxDw CREATE USER axdwadmin FROM LOGIN axdwadmin; CREATE USER axdwruntimeuser FROM LOGIN axdwruntimeuser; ALTER ROLE db_owner ADD MEMBER axdwadmin; ALTER ROLE db_datareader ADD MEMBER axdwruntimeuser; 3. Configuring to enable Analytical Workspaces and Reports What you are having now\nPower BI Embedded Service details from step 1\nWorkspace Collection Name: The name of the Workspace Collection created when deploying the Power BI Embedded Service Access Key1: The secret key #1 used to access the Power BI Embedded service Access Key2: The secret key #2 used to access the Power BI Embedded service Entity Store Database Service details from step 2\nDatabase name: AxDW Server name: Azure SQL Server name (*.database.windows.net) Server admin login: Username supplied in the SQL Server settings Password: Account password supplied when configuring SQL Server Run Notepad in administrator mode, open web.config from K:\\AOSService\\webroot in Dynamics 365 for finance and operations environment.\nUpdate the configuration settings:\n\u0026lt;add key=\u0026#34;BiReporting.DW\u0026#34; value=\u0026#34;[Database name]\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;BiReporting.DWServer\u0026#34; value=\u0026#34;[Server name]\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;BiReporting.DWRuntimeUser\u0026#34; value=\u0026#34;[Server Admin login]\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;BiReporting.DWRuntimePwd\u0026#34; value=\u0026#34;[Password]\u0026#34; /\u0026gt; Update the Power BI Embedded Service configuration settings:\n\u0026lt;add key=\u0026#34;PowerBIEmbedded.AccessKey\u0026#34; value=\u0026#34;[Access Key1]\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;PowerBIEmbedded.AccessKey2\u0026#34; value=\u0026#34;[Access Key2]\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;PowerBIEmbedded.ApiUrl\u0026#34; value=\u0026#34;https://api.powerbi.com\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;PowerBIEmbedded.IsPowerBIEmbeddedEnabled\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;PowerBIEmbedded.WorkspaceCollectionName\u0026#34; value=\u0026#34;[Workspace Collection Name]\u0026#34; /\u0026gt; After that, you need to restart IIS and Dynamics 365 for finance and operations batch service\n4. Refresh data entity store to AxDW In Dynamics 365 for finance and operations, navigate to System administration \u0026gt; Setup \u0026gt; Entity Store, select all the entity store, and click refresh.\nAllow this process to complete in the background (~5 - 10mins). You can monitor the status of the background process using the Batch jobs.\nOnce the Progress is made, you can check the event message in the same form to make sure the refreshing working properly\n{:.border}\n5. PowerBI configuration (this is optional) Configuring power BI for the workspace; please follow the document here\nThis is what you should have after the setup\n{:.border}\nCheck the result If you want to check the result, go to Workspaces \u0026gt; Customer credit and collections \u0026gt; Analytics all companies\n{:.border}\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2019-12-18-enable-power-bi-embedded-in-cloud-hosted-dynamics-365-finance-and-operations-8.0-+/","summary":"In this article, we will go through how to enable Power BI embedded in Dynamics 365 finance and operations version 10 in a cloud-hosted environment (customer managed). From the previous version of FinOps, Power BI embedded uses workspace collections at Azure to display the report, since the workspace collections have been deprecated, Microsoft also disables power BI embedded in cloud-hosted environments. You can only enable Power BI embedded in multiple boxes environments like UAT and production.","title":"Enable Power BI Embedded in cloud hosted Dynamics 365 finance and operations 8.0 +"},{"content":"1. Authentication We need an authentication to connect Dynamics 365 finance and operations from .Net application by using service principal.\nHow to create an app registration in Azure https://docs.microsoft.com/en-us/azure/active-directory/develop/app-registrations-training-guide-for-app-registrations-legacy-users\nYou also need to add required permissions for Dynamics 365 finance and operations, please follow\nhttps://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/data-entities/services-home-page#register-a-web-application-with-aad\nThis is what you should have\n{:.border}\n2. Register your external application a. In the FinOps application, go to System administration \u0026gt; Setup \u0026gt; Azure Active Directory applications. b.\tSelect New. c.\tFill in the fields for the new record:  In the Client Id field, enter the application ID that you registered in Azure AD. In the Name field, enter a name for the application. In the User ID field, select an appropriate service account user ID. For this example, we have selected the Admin user. However, as a better practice, you should provision a dedicated service account that has the correct permissions for the operations that must be performed. When you\u0026rsquo;ve finished, select Save.  You\u0026rsquo;ve now finished setting up the prerequisites. After the external application retrieves an Azure AD authentication token, it should now be able to use the token in an authorization HTTP header to make subsequent service calls via OData or SOAP, for example.\n3. OData client configuration  Download the project here https://github.com/microsoft/Dynamics-AX-Integration Open ServiceSamples solution. We cannot use existing proxies and classes, so I need to regenerate them again. Under ODataUtility project, delete ODataClient.tt and ODataClient.ttinclude. To regenerate Odata client, right click on ODataUtility project \u0026gt; Add \u0026gt; New item, search for OData in Online and rename it to ODataClient.tt. Open ODataClient.tt, in MetadataDocumentUri add \u0026ldquo;https://.cloudax.dynamics.com/data/$metadata\u0026rdquo;  If you save the ODataClient.tt file, it will generate Odata proxies and classes. From PU12 onward, there are so many entities so you will get an error after compiling\nCombined length of user strings used by the program exceeds allowed limit. Try to decrease use of string literals. {:.error}\nThere are 2 ways to fix it\na. Using temporary file The simplest way to fix is add TempFilePath, ensure that you have write permission for this path, this is what you got\n{:.border}\nSave the ODataClient.tt and there will be Test1.xml file created to store metadata, there should be a bug while generating the xml file, one more step you need to do is replace the double quote to single quote (a global replace of \u0026quot;\u0026quot; with \u0026quot; ), and this is what you have\n{:.border} You can build the project without problem.\nb. Remove unused entities Another work-around that works too, edit the ODataClient.ttinclude so that it parses only some of the entities that you want to use, this reducing the error. For instance, change the following foreach statement\n Original code:  {% highlight csharp %} foreach (IEdmEntitySet entitySet in container.EntitySets()) { IEdmEntityType entitySetElementType = entitySet.EntityType(); string entitySetElementTypeName = GetElementTypeName(entitySetElementType, container);\nstring camelCaseEntitySetName = entitySet.Name;\rif (this.context.EnableNamingAlias)\r{\rcamelCaseEntitySetName = Customization.CustomizeNaming(camelCaseEntitySetName);\r}\r } {% endhighlight %}\n Modified code:  {% highlight csharp %} foreach (IEdmEntitySet entitySet in container.EntitySets()) { IEdmEntityType entitySetElementType = entitySet.EntityType(); string entitySetElementTypeName = GetElementTypeName(entitySetElementType, container);\nstring camelCaseEntitySetName = entitySet.Name;\r//start of manual fix //only process entity names that containin a specific string, to reduce the string size\rif (((camelCaseEntitySetName.Contains(\u0026quot;CUST\u0026quot;)) || (camelCaseEntitySetName.Contains(\u0026quot;VEND\u0026quot;))) || (camelCaseEntitySetName.Contains(\u0026quot;SALES\u0026quot;)))\r{\r// emd of manual fix\rif (this.context.EnableNamingAlias)\r{\rcamelCaseEntitySetName = Customization.CustomizeNaming(camelCaseEntitySetName);\r}\r}\r } {% endhighlight %}\n4. Authentication configuration Under AuthenticationUtility project, you need to modify ClientConfiguration.cs follow, you can get all related information for the first step.\n{:.border}\nFrom now, you can freely test the integrations under ODataConsoleApplication project. If you are testing with a custom data entity and you get this error\nSystem.ArgumentOutOfRangeException : Length cannot be less than zero. {:.error}\nPlease check this post.\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2019-12-16-consuming-dynamics-365-finance-and-operations-odata-services-from-net/","summary":"1. Authentication We need an authentication to connect Dynamics 365 finance and operations from .Net application by using service principal.\nHow to create an app registration in Azure https://docs.microsoft.com/en-us/azure/active-directory/develop/app-registrations-training-guide-for-app-registrations-legacy-users\nYou also need to add required permissions for Dynamics 365 finance and operations, please follow\nhttps://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/data-entities/services-home-page#register-a-web-application-with-aad\nThis is what you should have\n{:.border}\n2. Register your external application a. In the FinOps application, go to System administration \u0026gt; Setup \u0026gt; Azure Active Directory applications.","title":"Consuming Dynamics 365 Finance and Operations OData services from .NET"},{"content":"In Dynamics 365 finance and operations, Business events provide a mechanism that lets external systems receive notifications from FinOps applications. In this way, the systems can perform business actions in response to business events.\nThere are 3 types of business events: Application business events, Workflow business events, and Alerts as business events. You can also implement a new business event.\nBusiness events can be consumed using Microsoft Flow and Azure messaging services, and we use endpoint to manage the destinations for sending business events to, Microsoft supports many endpoints: Azure Service Bus Queue, Azure Service Bus Topic, Azure Event Grid, Azure Event Hub, HTTPS, Microsoft Flow. In this article I will show how to send business event to HTTPs endpoint that leveraging on Azure functions.\nThe scenario: Once a free text invoice is posted, Business event will be triggered and send messages to the HTTPs endpoint.\n1. HTTPs and Azure function Ideally, I will create a new Azure function which has HTTPs endpoint to subscribe the business events in FinOps. To create Azure function please follow this https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-first-azure-function\nHere is the simple line of code\n{% highlight csharp %} using System.Net; using Microsoft.AspNetCore.Mvc; using Microsoft.Extensions.Primitives; using Newtonsoft.Json; public static async TaskRun(HttpRequest req, ILogger log) { log.LogInformation(\u0026ldquo;Dynamics 365 finance and operations notifications\u0026rdquo;); string requestBody = await new StreamReader(req.Body).ReadToEndAsync(); log.LogInformation(requestBody); return (ActionResult)new OkObjectResult($\u0026ldquo;Hello world\u0026rdquo;); } {% endhighlight %}\nThis basically listens the JSON messages from Business events and display the Json message at Console.\nFrom the Azure function you can get the HTTPs endpoint, save it for later reference.\n{:.border}\n2. Application registration int AAD We need an application to authenticate with FinOps and Azure function HTTPs. Go to Azure portal \u0026gt; AAD \u0026gt; App registrations \u0026gt; New registration\n{:.border}\n Name of the application. Depend on you have multitenant or not Dynamics 365 finance and operations URL  API Permissions Go to the newly created application \u0026gt; API permissions and adding permission as below\n{:.border}\nSecrets Go to Certificates \u0026amp; secrets menu item and create a new client secret\n{:.border}\nAfter this you will have Application Id and Application Secret, save it for later.\n3. Key Vaults In Azure portal create a new keyVault to store the HTTPs endpoint URL information\n{:.border}\nAccess policy Click next to create access policy (you also can set up this later after creating Key Vault)\n{:.border}\nSelect all the permissions in Key, Secret and Certificate, In select principal choose the application you have created before.\n{:.border}\nKey Vault secret Go to the newly created Key vault \u0026gt; secrets \u0026gt; generate a new one\n{:.border}\nValue is the endpoint URL for D365 to call the one we got from the first step. After this step you will have the Key vault DNS name https://maxfokeyvault.vault.azure.net/ and Key Vault secret name D365VaultSecretName That information will be needed for Business events configuration in Dynamics 365 finance and operations\n4. Creating HTTPs endpoint Go to System administrator \u0026gt; Business events \u0026gt; Business events catalog, Click on Endpoints in Endpoint type choose HTTPS and click Next. Fill all the required information that you got from above steps.\nClick on Business events catalog, look for business event Id CustFreeTextInvoicePostedBusinessEvent, check the record and click Active; from there choose legal entity and the Endpoint that we have just created.\n{:.border}\nAfter that, if you check on Active events tab, there will be a new record created. That\u0026rsquo;s it, now I will create a free text invoice and post it, this is what I got from the console log in Azure.\n{:.border}\nWith the JSON messages, you can deserialize it and save to Cosmos DB or do whatever in Azure function.\n5. About platform changes Before PU26, Business event run in batch, following menu System admin \u0026gt; Business events \u0026gt; Start business events batch job\n{:.border} {:.border} {:.border}\nIn BusinessEventsParameters, the value will be Enabled = 0 , BatchEnabled = 1.\nAfter PU26, the sending business events will be triggered directly from FinOps, you won\u0026rsquo;t find the menu System admin \u0026gt; Business events \u0026gt; Start business events batch job\nIn BusinessEventsParameters, the value will be Enabled = 1 , BatchEnabled = 0. If you have just upgraded from PU26 to higher version, the Business events are not getting triggered because the value might not be changed in BusinessEventsParameters and there is no batch. You should check this table according to the situation.\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2019-11-05-business-events-and-https-endpoint/","summary":"In Dynamics 365 finance and operations, Business events provide a mechanism that lets external systems receive notifications from FinOps applications. In this way, the systems can perform business actions in response to business events.\nThere are 3 types of business events: Application business events, Workflow business events, and Alerts as business events. You can also implement a new business event.\nBusiness events can be consumed using Microsoft Flow and Azure messaging services, and we use endpoint to manage the destinations for sending business events to, Microsoft supports many endpoints: Azure Service Bus Queue, Azure Service Bus Topic, Azure Event Grid, Azure Event Hub, HTTPS, Microsoft Flow.","title":"Business events and HTTPs endpoint"},{"content":"Got inspired by this topic, I\u0026rsquo;d like to write this article to show you how to post multiple records in single request by using Postman.\nGenerally, batch requests are supported in the OData service, The easiest way is you can use the C# code approach from github and the excel add-ins in Dynamics 365 for finance and operations use Odata batch to communicate in a single request but how can we leverage it in Postman.\nUsing Excel add-in then add 2 customer groups records and submit to Dynamics 365 for finance and operations, while you are doing that using fiddler on the same box to see how the odata batch framework works. This can be done from POSTMAN too and you need to use header and body as you see in Fiddler.\nFor basic setting up Dynamics 365 for finance and operations and Postman please follow this offical document\n1. Get Dynamics 365 for finance and operations authorization This is a result\n2. Create a new POST request in Postman with header URL: {{resource}}/data/$batch Content-Type: multipart/mixed;boundary=batch_d63a-e9be-2927\n3. Modify the Body in Postman I will place 2 records for customer groups\nFull text here\n--batch_d63a-e9be-2927 Content-Type: multipart/mixed; boundary=changeset_2499-90ab-7b93 --changeset_2499-90ab-7b93 Content-Type: application/http Content-Transfer-Encoding: binary POST CustomerGroups?cross-company=true HTTP/1.1 Content-ID: 1 Accept: application/json;q=0.9, */*;q=0.1 OData-Version: 4.0 Content-Type: application/json OData-MaxVersion: 4.0 {\u0026#34;CustomerGroupId\u0026#34;:\u0026#34;1060\u0026#34;,\u0026#34;Description\u0026#34;:\u0026#34;Wholesales customers\u0026#34;,\u0026#34;PaymentTermId\u0026#34;:\u0026#34;Net30\u0026#34;,\u0026#34;IsSalesTaxIncludedInPrice\u0026#34;:\u0026#34;No\u0026#34;,\u0026#34;dataAreaId\u0026#34;:\u0026#34;usmf\u0026#34;} --changeset_2499-90ab-7b93-- --batch_d63a-e9be-2927 Content-Type: multipart/mixed; boundary=changeset_b573-33b2-85ff --changeset_b573-33b2-85ff Content-Type: application/http Content-Transfer-Encoding: binary POST CustomerGroups?cross-company=true HTTP/1.1 Content-ID: 2 Accept: application/json;q=0.9, */*;q=0.1 OData-Version: 4.0 Content-Type: application/json OData-MaxVersion: 4.0 {\u0026#34;CustomerGroupId\u0026#34;:\u0026#34;1070\u0026#34;,\u0026#34;Description\u0026#34;:\u0026#34;Wholesales customers1\u0026#34;,\u0026#34;PaymentTermId\u0026#34;:\u0026#34;Net30\u0026#34;,\u0026#34;IsSalesTaxIncludedInPrice\u0026#34;:\u0026#34;No\u0026#34;,\u0026#34;dataAreaId\u0026#34;:\u0026#34;usmf\u0026#34;} --changeset_b573-33b2-85ff-- Click send and this is what you got from response\nFull Response\n{% highlight json %} \u0026ndash;batchresponse_45e87829-5a26-480e-8aaa-8a08c7a82c60 Content-Type: multipart/mixed; boundary=changesetresponse_27ed7621-d939-40b7-9f8b-be0421ff0cea \u0026ndash;changesetresponse_27ed7621-d939-40b7-9f8b-be0421ff0cea Content-Type: application/http Content-Transfer-Encoding: binary Content-ID: 1 HTTP/1.1 201 Created ETag: W/\u0026ldquo;JzEsNjg3MTk0Nzk4MzUn\u0026rdquo; Location: https: //fodevb2819a3b6966913ddevaos.cloudax.dynamics.com/data/CustomerGroups(dataAreaId=\u0026lsquo;usmf\u0026rsquo;,CustomerGroupId=\u0026lsquo;1060\u0026rsquo;) Content-Type: application/json; odata.metadata=minimal OData-Version: 4.0 { \u0026ldquo;@odata.context\u0026rdquo;: \u0026ldquo;https://fodevb2819a3b6966913ddevaos.cloudax.dynamics.com/data/$metadata#CustomerGroups/$entity\u0026quot;, \u0026ldquo;@odata.etag\u0026rdquo;: \u0026ldquo;W/\u0026quot;JzEsNjg3MTk0Nzk4MzUn\u0026quot;\u0026rdquo;, \u0026ldquo;dataAreaId\u0026rdquo;: \u0026ldquo;usmf\u0026rdquo;, \u0026ldquo;CustomerGroupId\u0026rdquo;: \u0026ldquo;1060\u0026rdquo;, \u0026ldquo;ClearingPeriodPaymentTermName\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;DefaultDimensionDisplayValue\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;CustomerAccountNumberSequence\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;IsSalesTaxIncludedInPrice\u0026rdquo;: \u0026ldquo;No\u0026rdquo;, \u0026ldquo;Description\u0026rdquo;: \u0026ldquo;Wholesales customers\u0026rdquo;, \u0026ldquo;WriteOffReason\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;PaymentTermId\u0026rdquo;: \u0026ldquo;Net30\u0026rdquo;, \u0026ldquo;TaxGroupId\u0026rdquo;: \u0026quot;\u0026rdquo; } \u0026ndash;changesetresponse_27ed7621-d939-40b7-9f8b-be0421ff0cea\u0026ndash; \u0026ndash;batchresponse_45e87829-5a26-480e-8aaa-8a08c7a82c60 Content-Type: multipart/mixed; boundary=changesetresponse_541a7d21-af21-4d66-b410-fb4165599b54 \u0026ndash;changesetresponse_541a7d21-af21-4d66-b410-fb4165599b54 Content-Type: application/http Content-Transfer-Encoding: binary Content-ID: 2 HTTP/1.1 201 Created ETag: W/\u0026ldquo;JzEsNjg3MTk0Nzk4MzYn\u0026rdquo; Location: https: //fodevb2819a3b6966913ddevaos.cloudax.dynamics.com/data/CustomerGroups(dataAreaId=\u0026lsquo;usmf\u0026rsquo;,CustomerGroupId=\u0026lsquo;1070\u0026rsquo;) Content-Type: application/json; odata.metadata=minimal OData-Version: 4.0 { \u0026ldquo;@odata.context\u0026rdquo;: \u0026ldquo;https://fodevb2819a3b6966913ddevaos.cloudax.dynamics.com/data/$metadata#CustomerGroups/$entity\u0026quot;, \u0026ldquo;@odata.etag\u0026rdquo;: \u0026ldquo;W/\u0026quot;JzEsNjg3MTk0Nzk4MzYn\u0026quot;\u0026rdquo;, \u0026ldquo;dataAreaId\u0026rdquo;: \u0026ldquo;usmf\u0026rdquo;, \u0026ldquo;CustomerGroupId\u0026rdquo;: \u0026ldquo;1070\u0026rdquo;, \u0026ldquo;ClearingPeriodPaymentTermName\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;DefaultDimensionDisplayValue\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;CustomerAccountNumberSequence\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;IsSalesTaxIncludedInPrice\u0026rdquo;: \u0026ldquo;No\u0026rdquo;, \u0026ldquo;Description\u0026rdquo;: \u0026ldquo;Wholesales customers1\u0026rdquo;, \u0026ldquo;WriteOffReason\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;PaymentTermId\u0026rdquo;: \u0026ldquo;Net30\u0026rdquo;, \u0026ldquo;TaxGroupId\u0026rdquo;: \u0026quot;\u0026rdquo; } \u0026ndash;changesetresponse_541a7d21-af21-4d66-b410-fb4165599b54\u0026ndash; \u0026ndash;batchresponse_45e87829-5a26-480e-8aaa-8a08c7a82c60\u0026ndash; {% endhighlight %}\nCheck the data in Dynamics 365 for finance and operations\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2019-10-15-insert-multiple-records-in-single-request-using-odata-from-postman/","summary":"Got inspired by this topic, I\u0026rsquo;d like to write this article to show you how to post multiple records in single request by using Postman.\nGenerally, batch requests are supported in the OData service, The easiest way is you can use the C# code approach from github and the excel add-ins in Dynamics 365 for finance and operations use Odata batch to communicate in a single request but how can we leverage it in Postman.","title":"Insert multiple records in single request using Odata from Postman"},{"content":"In this article, I will show the procedure for the SSRS Reports development and customization in Dynamics 365 finance and operations (Version 8.1 and above).\nThe scenario is you would like to extend the customer account statement report with 2 main tasks\n Create custom design for the report Expand the standard report data sets  Same with AX 2012 version, there is no change on how you developer a new SSRS report with query based and report data provider. But if you want to extend or modify the standard one you need to understand how to use extensions in general, Event handlers and Chain of Command.\nThe steps\n1. Adding a new field The main temp table is CustAccountStatementExtTmp, right click and create an extension; I\u0026rsquo;m going to add a new string field MaxTxT\n2. Duplicate the report Duplicate the CustAccountStatementExt report in the Application explorer \u0026gt; AOT \u0026gt; Reports \u0026gt; CustAccountStatementExt as shown in below screen shot:\nRename the report and provide any appropriate name: MaxCustAccountStatementExt\n3. Modify the report design, right click on report dataset and choose restore to refresh the new field Open report designer and add that field into a table\n4. Create a new Extension class that extends the standard report controller class. class MaxCustAccountStatementExtController_Ext extends CustAccountStatementExtController{}\n{% highlight csharp %} //Add construct public static MaxCustAccountStatementExtController_Ext construct() { return new MaxCustAccountStatementExtController_Ext(); } {% endhighlight %}\nCopy the main method from the standard controller class and add references to the new Controller class\n{% highlight csharp %} public static void main(Args _args) { SrsPrintMgmtFormLetterController controller = new MaxCustAccountStatementExtController_Ext(); controller.parmReportName(PrintMgmtDocType::construct(PrintMgmtDocumentType::CustAccountStatement).getDefaultReportFormat()); controller.parmArgs(_args); MaxCustAccountStatementExtController_Ext::startControllerOperation(controller, _args); } protected static void startControllerOperation(SrsPrintMgmtFormLetterController _controller, Args _args) { _controller.startOperation(); } {% endhighlight %}\nOptional method, determine which default design for report\n{% highlight csharp %} protected void outputReport() { SRSCatalogItemName reportDesign; reportDesign = ssrsReportStr(MaxCustAccountStatementExt,Report); this.parmReportName(reportDesign); this.parmReportContract().parmReportName(reportDesign); formletterReport.parmReportRun().settingDetail().parmReportFormatName(reportDesign); super(); } {% endhighlight %}\n5. Create new report handler class class MaxCustAccountStatementHandler {}\nWe have two different ways to Populate the data in the Report handler class:\n Add a temp table Inserting event, row-by-row calculations.  {% highlight csharp %} [DataEventHandlerAttribute(tableStr(CustAccountStatementExtTmp), DataEventType::Inserting)] public static void CustAccountStatementExtTmpInsertEvent(Common c, DataEventArgs e) { CustAccountStatementExtTmp tempTable = c; tempTable.MaxTxT = \u0026ldquo;Hello world\u0026rdquo;; } {% endhighlight %}\n Add a data processing post-handler, inserting operations that use a single pass over the result set of the standard solution.  {% highlight csharp %} [PostHandlerFor(classStr(CustAccountStatementExtDP), methodstr(CustAccountStatementExtDP, processReport))] public static void TmpTablePostHandler(XppPrePostArgs arguments) { CustAccountStatementExtDP dpInstance = arguments.getThis() as CustAccountStatementExtDP; CustAccountStatementExtTmp tmpTable = dpInstance.getCustAccountStatementExtTmp(); ttsbegin; while select forUpdate tmpTable { tmpTable.MaxTxT = \u0026ldquo;Hello world\u0026rdquo;; tmpTable.update(); } ttscommit; } {% endhighlight %}\n6. Add a delegate handler method to start to use your custom report. In this example, extend the getDefaultReportFormatDelegate method in the PrintMgtDocTypeHandlerExt class by using the following code.\n{% highlight csharp %} class MaxPrintMgtDocTypeHandlersExt { [SubscribesTo(classstr(PrintMgmtDocType), delegatestr(PrintMgmtDocType, getDefaultReportFormatDelegate))] public static void getDefaultReportFormatDelegate(PrintMgmtDocumentType _docType, EventHandlerResult _result) { switch (_docType) { case PrintMgmtDocumentType::CustAccountStatement: _result.result(ssrsReportStr(MaxCustAccountStatementExt, Report)); break; } } } {% endhighlight %}\n7. Create extension for the existing menu items Navigating to the CustAccountStatementExt output menu item and create extension.\nAlso make sure to set the value of the Object property to MaxCustAccountStatementExtController_Ext to redirect user navigation to the extended solution.\nThat\u0026rsquo;s all, this is what you should have\n8. Update the Print management settings to use the custom business document Go to Account payable \u0026gt; Inquiries and reports \u0026gt; Setup \u0026gt; Forms \u0026gt; Form setup Click Print Management, find the document configuration settings, and then select the custom design\n9. Run report and test the result ","permalink":"https://nuxulu.com/posts/2019-10-10-extend-the-standard-reports-in-dynamics-365-finance-and-operations/","summary":"In this article, I will show the procedure for the SSRS Reports development and customization in Dynamics 365 finance and operations (Version 8.1 and above).\nThe scenario is you would like to extend the customer account statement report with 2 main tasks\n Create custom design for the report Expand the standard report data sets  Same with AX 2012 version, there is no change on how you developer a new SSRS report with query based and report data provider.","title":"Extend the standard reports in Dynamics 365 finance and operations"},{"content":"The following reports provide details about technical objects available in Dynamics 365 for Finance Operations version 10.0. This version was released in May 2019 and has a build number of 10.0.2.\n","permalink":"https://nuxulu.com/posts/2019-09-28-technical-referece-report-data-entites-d365/","summary":"The following reports provide details about technical objects available in Dynamics 365 for Finance Operations version 10.0. This version was released in May 2019 and has a build number of 10.0.2.","title":"Technical reference - Data entity - Dynamics 365 finance and operations"},{"content":"You can either execute the script for cloud-hosted, onebox VHD, or UAT environment. This is not applied with one-box Microsoft hosted environment.\nOpen PowerShell, run following script\nK:\\AOSService\\webroot\\bin\\Microsoft.Dynamics.AX.Deployment.Setup.exe -bindir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; metadatadir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; -sqluser \u0026#34;axdbadmin\u0026#34; -sqlserver \u0026#34;.\u0026#34; -sqldatabase \u0026#34;AxDB\u0026#34; -setupmode \u0026#34;sync\u0026#34; -syncmode \u0026#34;fullall\u0026#34; -isazuresql \u0026#34;false\u0026#34; -sqlpwd \u0026#34;************\u0026#34; -logfilename \u0026#34;C:\\Temp\\dbsync.log\u0026#34; For example\nK:\\AOSService\\webroot\\bin\\Microsoft.Dynamics.AX.Deployment.Setup.exe -bindir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; metadatadir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; -sqluser \u0026#34;axdbadmin\u0026#34; -sqlserver \u0026#34;.\u0026#34; -sqldatabase \u0026#34;AxDB\u0026#34; -setupmode \u0026#34;sync\u0026#34; -syncmode \u0026#34;fullall\u0026#34; -isazuresql \u0026#34;false\u0026#34; -sqlpwd \u0026#34;AOSWebSite@123\u0026#34; -logfilename \u0026#34;C:\\Temp\\dbsync.log\u0026#34; AX DB user you can get from LCS, if you want to sync Azure DB please change -isazuresql to True\n","permalink":"https://nuxulu.com/posts/2019-08-05-database-sync-d365fo/","summary":"You can either execute the script for cloud-hosted, onebox VHD, or UAT environment. This is not applied with one-box Microsoft hosted environment.\nOpen PowerShell, run following script\nK:\\AOSService\\webroot\\bin\\Microsoft.Dynamics.AX.Deployment.Setup.exe -bindir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; metadatadir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; -sqluser \u0026#34;axdbadmin\u0026#34; -sqlserver \u0026#34;.\u0026#34; -sqldatabase \u0026#34;AxDB\u0026#34; -setupmode \u0026#34;sync\u0026#34; -syncmode \u0026#34;fullall\u0026#34; -isazuresql \u0026#34;false\u0026#34; -sqlpwd \u0026#34;************\u0026#34; -logfilename \u0026#34;C:\\Temp\\dbsync.log\u0026#34; For example\nK:\\AOSService\\webroot\\bin\\Microsoft.Dynamics.AX.Deployment.Setup.exe -bindir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; metadatadir \u0026#34;K:\\AosService\\PackagesLocalDirectory\u0026#34; -sqluser \u0026#34;axdbadmin\u0026#34; -sqlserver \u0026#34;.\u0026#34; -sqldatabase \u0026#34;AxDB\u0026#34; -setupmode \u0026#34;sync\u0026#34; -syncmode \u0026#34;fullall\u0026#34; -isazuresql \u0026#34;false\u0026#34; -sqlpwd \u0026#34;AOSWebSite@123\u0026#34; -logfilename \u0026#34;C:\\Temp\\dbsync.","title":"Dynamics 365 for finance and operations database synchronization using command line"},{"content":"When we write a code, or make a customization to Dynamics 365 For Operation which is using X++ should make use of Exception Handling to provide some context for the message or a different more useful message. In this article, I will be exploring how I can come up with a uniform way to catch multiple types of exception that can be raised in X++.\n1. Exception type There are many several types of exception and the type differs depending on what caused the error. Much of exception types are determined by the kernel and are not normally thrown by application code. All exception types, however, can be caught, and it is the developers’ responsibility to decide which exceptions need to be handled.\nThe exception type is identified using the system-based enumeration called an exception. Because it is a system Enum, it cannot be modified, so users cannot add new exception types.\nThe following table shows the exception literals that are the values of the Exception enumeration.\n2. Key commands The Try command signifies the start of a block of code that you want to control with the X++ exception handling system. Any exceptions that are thrown in that block of code can be caught and handled accordingly. The block of code inside the Try statement must be contained between brackets ( { } ).\nCatch statements come after the block of code and define what code is executed when each exception is thrown.\nYou do not have to define a catch statement for every possible exception; however, each try statement must have at least one catch statement.\nA Retry command tells the system to go back to the Try statement and attempt to execute the code again. Any data that was loaded before the Try command will remain as it was, but any data retrieved or modified after the Try statement will be refreshed.\nWhen a deadlock exception is thrown, all locks on tables that this process holds are released, which may allow the other process or processes that are also deadlocked to continue.\nBy calling a retry, the process can attempt to update the record again and may now be able to complete. It is a best practice that a retry uses a counter so that the number of retries can be controlled, and a user does not become stuck in a loop.\nThe final keyword is now available to follow the try and catch keywords. The semantics are identical to the semantics in C#. The statements provided in the final clause is executed irrespective of whether the try block threw any exceptions.\n3. Code Statement We will use these lines of code example here for testing Exception handling\n{% highlight csharp %} class CustCreateCustomer { public static void main(Args _args) { CustCreateCustomer custCreateCustomer = new CustCreateCustomer(); custCreateCustomer.run(); }\npublic void run()\r{\rDialog dlg = new Dialog(\u0026quot;Create new customer\u0026quot;);\rDialogField dlgCust;\rDialogField dlgGrp;\rCustTable custTable;\rdlgCust = dlg.addField(extendedTypeStr(CustVendAc), \u0026quot;Customer account\u0026quot;);\rdlgGrp = dlg.addField(extendedTypeStr(CustGroupId));\rif (dlg.run())\r{\rtry\r{\rcustTable.AccountNum = dlgCust.value();\rcustTable.CustGroup = dlgGrp.value();\rif (!custTable.validateWrite())\r{\rthrow error(\u0026quot;Please enter all required fields.\u0026quot;);\r}\relse\r{\rcustTable.insert();\r}\r}\rcatch (Exception::Error)\r{\rerror(\u0026quot;An error occurred. Please try again\u0026quot;);\r}\r}\r}\r } {% endhighlight %}\nThis code will try to create a customer after revived inputted value from users, this code also handling errors when user do not input enough information. A Throw statement is used to throw an error that can be caught by a Catch statement. When the system throws an exception ttsabort is called automatically, and so does not have to be called in a Catch statement.\n4. Optimistic Concurrency Exceptions The optimistic concurrency check (OCC) is a performance enhancing function within Microsoft Dynamics 365 For Operation. It presumes that any record retrieved from the database is not updated until it is proven to be updated by the database. This means that fewer locks must be placed on records in the database. This allows for faster access for other users.\nThis also means that one user can update a record after another user has retrieved it from the database. This can cause data inconsistency. If the second user then also tries to update the record, an UpdateConflict exception is thrown. The system does this by comparing the recVersion field on the record buffer at runtime and the actual record in the database. The recVersion field value is changed every time that an update is successfully made to a record.\nThere are two main table update exceptions, UpdateConflict and DeadLock. An update conﬂict occurs due to the optimistic concurrency failing, whereas a deadlock is the classic database scenario where both transactions have each locked a table that the other needs.\nUpdate conﬂicts are normally handled within the insert, delete, and update methods of a table. The BOM table is a good example of this. You may also hard to find many examples where this has been used. We use this pattern only if we deem it to be required. The code within the table\u0026rsquo;s update method also updates other records, so it has been written to handle update conﬂicts.\nThe following code is an example of how to handle the UpdateConflict exception that might be thrown.\n{% highlight csharp %} public void update() { #OCCRetryCount try { ttsbegin; // code that updates records in other tables super(); // do the update // other code that updates records in other tables ttscommit; } //Deadlock catch (Exception::Deadlock) { retry; } //UpdateConflict catch (Exception::UpdateConflict) { if (appl.ttsLevel() == 0) { if (xSession::currentRetryCount() \u0026gt;= #RetryNum) { throw Exception::UpdateConflictNotRecovered; } else { retry; } } else { throw Exception::UpdateConflict; } } } {% endhighlight %}\nIf a conflict due to OCC occurs, the system throws the UpdateConflict exception and it is caught by the catch statement.\nThe other new element here is ttsLevel. Since transactions can be nested, we do want the exception to falling through to the parent transaction if one exists. If ttsabort is issued (directly or due to a throwing error) at any level, the whole transaction will be rolled back; we can\u0026rsquo;t roll back just the level where the error is thrown.\nThe code checks the current TTS level. If it is not Zero, in other words, it is still in a TTS transaction, it throws another UpdateConflict exception to the next Catch list of the next outer Try statement in scope. This continues until it is no longer inside a TTS transaction. We must make sure that when the code is retired, all data is refreshed.\nIt is important that we don\u0026rsquo;t retry indefinitely, as this may cause the client to hang. To control this, we use xSession::currentRetryCount() to get the number of retries and check this against the #RetryNum macro. The macro defines the standard number of retries deemed appropriate by Microsoft, which is five. then the UpdateConflictNotRecovered exception is thrown. This means the whole transaction is aborted and stops retrying.\n5. Conclusion We do not, in any case, want an error to be thrown that stops the form from opening. Also, if there is an error, we need to decide whether the user actually needs to know that an error occurred. It may be enough for our purposes that the fields don\u0026rsquo;t appear, and we can use the debugger to trace through the code to determine why.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-08-07-exception-handling-in-dynamics-365-for-finance-and-operation/","summary":"\u003cp\u003eWhen we write a code, or make a customization to Dynamics 365 For Operation which is using X++ should make use of Exception Handling to provide some context for the message or a different more useful message. In this article, I will be exploring how I can come up with a uniform way to catch multiple types of exception that can be raised in X++.\u003c/p\u003e\n\u003ch3 id=\"1-exception-type\"\u003e1. Exception type\u003c/h3\u003e\n\u003cp\u003eThere are many several types of exception and the type differs depending on what caused the error. Much of exception types are determined by the kernel and are not normally thrown by application code. All \u003cstrong\u003eexception types\u003c/strong\u003e, however, can be caught, and it is the developers’ responsibility to decide which exceptions need to be handled.\u003c/p\u003e\n\u003cp\u003eThe exception type is identified using the system-based enumeration called an exception. Because it is a system Enum, it cannot be modified, so users cannot add new exception types.\u003c/p\u003e\n\u003cp\u003eThe following table shows the exception literals that are the values of the Exception enumeration.\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e","title":"Exception Handling in Dynamics 365 For Finance and Operation"},{"content":"1. Overview Management Reporter in New AX Management Reporter is now Financial reports.\nAs you know Management Reporter is a real-time financial reporting application that is designed to empower information workers to quickly and easily create, generate, secure, and publish financial statements, such as Profit and Loss statements, balance sheets, and cash flow reports.\nIn Dynamics 365 For Operation, I can access these reports within AX means directly from the web client in the browser. This feature allows me to run financial statements, such as a balance sheet and income statements.\nThe 22 default reports that are included, default financial reports including income statements, balance sheet reports, cash flow statements, and general ledger reports, which can be modified depending on your company’s requirements.\nHere is the list\n2. Access You can find the financial reporting menu in Dynamics 365 For Operation by visiting\nGeneral Ledger \u0026gt; Inquiries and reports \u0026gt; Financial reporting.\nAlthough, there are some privileges and duties relating to this. These report functions are available to users that have their appropriate privileges and duties assigned to them already. In order to view, edit, and manage reports in Management Reporter you need to add security administrator role to business Users. Besides, if you only need users can run the financial reports then you have to add the appropriate privileges and duties assigned to them.\nBefore creating and generating financial reports for a legal entity, you must be done set up for that legal entity:\n Fiscal calendar Ledger Chart of Accounts Currency  3. Management Reporter Benefits With Management Reporter, business users can:\n Create their own boardroom-quality reports without IT assistance. Take advantage of secure report distribution through the centralized Report Library. Gain strong support for regulatory compliance (Change the report date, currency, view in Summary view or a detailed view, Add either dimension filters or attribute filters). Report design flexibility likes Save dimension combinations, and reuse the dimensions for multiple reports. Management Report Components. Financial report collaboration:  Schedule reports so that they are automatically generated on a daily, weekly, monthly, or annual basis. Export to the read-only XPS format, which provides better document security through digital signatures. Export to a Microsoft Excel worksheet. To share reports, you can create email messages that contain links to the reports.    4. Management Report Components Some important components I want to mention here:\n Report Designer Create report building blocks that can be combined to define and generate a report. The report wizard guides less experienced users through the design process. Advanced users can create new report building blocks or modify existing building blocks to meet their requirements. Desktop Viewer Used to organize and view reports and supporting files. It also stores the report library. Web Viewer Displays Management Reporter reports in a web browser. The Web Viewer does not require an installation of Management Reporter server components. Report schedules A user can schedule a single report or a group of reports to generate regularly. Management Reporter database This SQL database stores the components, known as building blocks, which are used to generate reports. It also stores report definitions and previously generated reports. Application service Controls access to the data provider and provides connectivity to clients. Process service Generates the reports that are created and queued by the Management Reporter client.  5. Generate financial reports We are currently at Financial reports screen as picture below\nAs you can see there\u0026rsquo;s nowhere on the screen or anything that indicates that there\u0026rsquo;s the Management Reporter available, but if we click this edit button and entering your email account and password. After you log in, it will open the Report Definitions Management Reporter window for us.\nOn the left, here we have a list of all the same reports that we were looking at in AX. Let\u0026rsquo;s look at the income statement - default report, So I have a couple parameter fields we need to change before we can generate our report. Please prefer below picture\n6. Conclusion Row, Column, Tree, Report definitions work essentially the same as they did in Dynamics AX 2012. I can still use row, column, and tree definitions with multiple reports, and options for the Report definition tabs.\nHere are some of the functions that are not available in Financial Reporting for Dynamics 365 for Operations:\n Publishing/distributing reports to SharePoint and/or SharePoint Online locations is not available. Report Viewer/Library has been removed. Import data from an Excel file is not available. XBRL (extensible Business Reporting Language) is no longer available. Comments are not currently available in the web client. Running a consolidation between companies in different instances of AX is not available. The Wizard has been removed. Management Reporter includes direct integration with the Microsoft Dynamics AX general ledger so that there is no need to create a custom connection to the data source.  Nevertheless, with custom reporting structures, ledger accounts, and dimensions mapping, creation and the capabilities to publish reports to multiple channels and formats from the desktop, and embedding transactional and analytical reports into a customizable dashboard, Financial Reporting in Dynamics 365 For Operation is all set to simplify the financial reporting process, providing instant insight into your financials.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/management-reporter-in-dynamics-365-for-operation/","summary":"\u003ch3 id=\"1-overview-management-reporter-in-new-ax\"\u003e1. Overview Management Reporter in New AX\u003c/h3\u003e\n\u003cp\u003eManagement Reporter is now \u003cstrong\u003eFinancial reports\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAs you know Management Reporter is a real-time financial reporting application that is designed to empower information workers to quickly and easily create, generate, secure, and publish financial statements, such as Profit and Loss statements, balance sheets, and cash flow reports.\u003c/p\u003e\n\u003cp\u003eIn Dynamics 365 For Operation, I can access these reports within AX means directly \u003cem\u003efrom the web client in the browser\u003c/em\u003e. This feature allows me to run financial statements, such as a balance sheet and income statements.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003e22 default\u003c/strong\u003e reports that are included, default financial reports including income statements, balance sheet reports, cash flow statements, and general ledger reports, which can be modified depending on your company’s requirements.\u003c/p\u003e\n\u003cp\u003eHere is the list\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e","title":"Management reporter in new AX, Dynamics 365 For Operation"},{"content":" There are many explanations that we can find in internet about C# OOP, but here in my article I will give a very simple example. In this article, I will use a “House (like the houses we live in) “as a real-time example for easy understanding of OOP Concept in C#.\n 1. Class Class is a like a Blueprint.\nBlueprint is outline drawing of our actual plan. For example if we plan to build our new house, the Engineer will explain our new house plan with a blue print as shown in the image below. Once we finalized the plan the engineer will start building the house with same plan.\nSame like blueprint class is an outline of program. Using the class we can write our own method and declare the variables and using the objects we can access the class Method and Variables. The class will be complete with Variables, Methods and Objects.\nFor more easy understanding of OOP with real world example here I have explained a class with House.\nWe can imagine a House as an example for a Class. In a house, we will have rooms like living room, bedroom, kitchen and items like TV, fridge etc. House owner can access and use all the rooms and rooms' items. Same like this in a Class will have a group of Methods and Variables.\nRooms and Rooms\u0026rsquo;items are example for Methods and Variables. So now, we have a complete house with all rooms and rooms\u0026rsquo;items. House owner can access and use all the rooms and Rooms' Items. To access and use a Class, methods and variables here we use Objects. Objects are instance of a class. We will see details about objects in the next section.\nWhat will happen if there are no rooms and items in a House? It will be empty and no one can use the house until it has all the rooms and Items. See the below image as an example for the empty house.\nNow this empty house is a Class. So what is the use of a Class without Methods and variable. Now let’s see an example for a Complete House with Rooms and items.\nSo here, we have a complete house. Similarly, the Class will be complete with group of Variables, Methods and Objects. We can see details of all this in next sections of this article.\nClass and objects are the base concept of OOP – Object Oriented Programming.\nClass should be started with the Keyword class and next we give the name for our class we can give any meaning full name as Class Name, next we will have the Open and close brackets.\n{% highlight csharp %} using System; public class ClassA { } {% endhighlight %}\n2. Object As we have already seen that, House Owner will access and use all the Rooms of the House and its Items. Similarly, to access all Class Method and Variable we use Objects. We can create one or many object for a same Class. Example we can say for a house there might be one or many owners.\nobjHouseOwner is the Object for a class which will be used to access all variable and Method of a class.\n{% highlight csharp %} namespace ConsoleApplication1 { class Program { static void Main(string[] args) { Program pro = new Program(); } } } {% endhighlight %}\n3. Variable Variables are used to store our value. Our value can be numeric or characters for example to store a Phone no we can use int type variable and to store our name we can use string type variable with name for each variable.\nVariables can be local or Global. For Example, we can say if we buy a new TV , TV Service man will come and setup the TV in our home. He will give his contact number for future contacts. Usually what we do is take a memo paper and write his contact number and keep it in a common place or in a wallet of ours.\nIf we keep the memo in a Common place everyone who is visiting our house can see that contact number. Global or public variables are similar to this. If we declared the variable as Global, All the Methods inside the class can access the variable. If we store the memo Only in our wallet, we can see the contact number. Local or private variables are similar to this.\nSyntax for variable:\nAccess-Modifiers Data-Type Variable-Name\nBy default the Access-Modifiers are by private, we can also use public to variable.\nExample of Variable:\n{% highlight csharp %} namespace ConsoleApplication1 { // Example Program class ShanuHouseClass { public int noOfTV = 0; private Boolean doYouHaveTV = true; protected String yourTVName = \u0026ldquo;SAMSUNG\u0026rdquo;; static void Main(string[] args) { ShanuHouseClass objHouseOwner = new ShanuHouseClass(); Console.WriteLine(\u0026ldquo;You Have total \u0026quot; + objHouseOwner.noOfTV + \u0026quot; no of TV :\u0026quot;); Console.WriteLine(\u0026ldquo;Your TV Name is :\u0026rdquo; + objHouseOwner.yourTVName); Console.ReadLine(); } } } {% endhighlight %}\nIn Above example program I have declared two variables inside a class. In main method I have created object for class. Here we can see using the object we can access the variable of a class and display the output.\nMain Method is the default Method of C#, where every console and windows application will start the program execution, In the Main Method, we can declare the Object for the class and use the object, and we can access all variables and Methods of a Class. For example, we can say there will be entrance gate for every house. Using the entrance gate we can enter inside our house. Similarly, to run our program there should be some default program execution starting Method. Main method will be useful in this program execution starting point. Whenever we run our C# Console or windows application, first the Main method will be executed .From the main method we can create an object for our other classes and call their Methods.\n4. Method or Functions Method is a group of code statement .Now here we can see the above example program with method.\n{% highlight csharp %} class ShanuHouseClass { public int noOfTV = 2; private Boolean doYouHaveTV = true; protected String yourTVName = \u0026ldquo;SAMSUNG\u0026rdquo;;\npublic void myFirstMethod()\r{\rConsole.WriteLine(\u0026quot;You Have total \u0026quot; + noOfTV + \u0026quot;no of TV :\u0026quot;);\rConsole.WriteLine(\u0026quot;Your TV Name is :\u0026quot; + yourTVName);\rConsole.ReadLine();\r}\r } class a { static void Main(string[] args) { ShanuHouseClass a = new ShanuHouseClass(); a.myFirstMethod(); } } {% endhighlight %}\n Most of developers were wondering about what is the difference between the Method and Function, both Methods and Functions are the same. Here in my article, I will use Method instead of functions. However, there is one difference in Methods and Functions, In OOP Languages like C#, Java etc. We use the term Method while the non-OOP programming like C etc. we use the term Function.\n What is the use of Methods? Another real-time example let’s take our mobile phone, we can say as we have Mobile phone and store many Songs in it. However, we always like to listen to the selected songs. It will be boring and hard for us to select our favorite song every time and play it.\nInstead of doing the same work repeatedly, we use the playlist. In playlist, we can add all-favorite songs of ours. Just click on the playlist of our collections and listen to the music. This will make our work easier and we don’t need to do the same thing again and again. Methods are used like a playlist where we can write all our repeated code in one Method and just call the method wherever we needed.\nIn a house, there might be one big room or multiple rooms but each room will have different facilities, similarly in a class we can write one or multiple Methods. In a house, there might be two or three Bedrooms. Here the room name is Bedroom, but each bedroom can be different by size, color etc. This means same Rooms with different type. Similarly, in a class we can create more than one method with the same name but different parameter. In OOP it’s called as Polymorphism we can see details about Polymorphism later on in this article.\nSyntax for the Functions Access-Modifiers Return-Type Method-Name (Parameter-List)\n Access-Modifiers: We will see more details about this Topic later on. Return-Type: If our Method returns any value then we use the return Type with any Data Type as String, int etc., if our Method does not return any value then we use the type “Void”. Method-Name: Here we give our Name for every Method, which we create Parameter-List: Parameter-List or Arguments, which we pass to the function.  Here is an example of Method:\n Method with Void Type: Void is a keyword which will not return any data from the Method, for example we can see the below Method with void Type, here in this method we display all our output using the Console.WriteLine and have used the Console.ReadLine()); to get the Input. This Method has all Input and Output Operation but this method don’t return any value.  {% highlight csharp %} // Function with void and no parameter \u0026ndash; here void means no return type public void veranda() { Console.WriteLine(\u0026ldquo;Welcome to Veranda\u0026rdquo;); Console.WriteLine(\u0026ldquo;How Many Chairs Do you have in your Veranda\u0026rdquo;); NoofChair = Convert.ToInt32(Console.ReadLine()); Console.WriteLine(\u0026ldquo;I have total \u0026quot; + NoofChair + \u0026quot; Chairs in my Veranda\u0026rdquo;); } {% endhighlight %}\n Method with Return Type: Method with return type will return some result, which can be used in our program, for example, here we have Method TVNAME with return Type as String. We can say in our home we might have a TV in our LivingROOM and in the parent’s bedroom and also in kids bedroom .We might have different TV brand in each room, suppose if we want to know each room TV Brand Name then we need to enter the same code 3 times. Instead of writing the same code again, we can use a method with Return Type.  {% highlight csharp %} // Function with Return type as String public string TVNAME() { String YOURTVName; Console.WriteLine(\u0026ldquo;Enter Your TV Brand NAME\u0026rdquo;); YOURTVName = Console.ReadLine(); return YOURTVName; } {% endhighlight %}\n Method with Parameter-List: So far, we have seen methods without arguments. Arguments are used to pass some data to the Method to do our process in a better way. For example, we can say we want to do a painting, to our bedrooms. We need to get the opinions of all the member of the house in order to know there choices of color for each bedroom, we can pass the member Name and their favorite color as parameter to a Method.  {% highlight csharp %} //Function with parameter public void BedRoom(String nameandColor) { Console.WriteLine(nameandColor); } {% endhighlight %}\nSame Method name with different arguments are called as Method over loading, here we can see below .Both Method has the same name but it has different arguments.\n{% highlight csharp %} // Same Function Name with Different Paramenter public void BedRoom(String MemberName, String Color) { Console.WriteLine(MemberName + \u0026quot; Like \u0026quot; + Color + \u0026ldquo;Color\u0026rdquo;); } {% endhighlight %}\nThe Complete Class with Main Method Example:\n{% highlight csharp %} using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; // Example Program class ShanuHouseClass { public int noOfTV = 2; private Boolean doYouHaveTV = true; protected String yourTVName = \u0026ldquo;SAMSUNG\u0026rdquo;; public int NoofChair; public void myFirstMethod() { Console.WriteLine(\u0026ldquo;You Have total \u0026quot; + noOfTV + \u0026ldquo;no of TV :\u0026quot;); Console.WriteLine(\u0026ldquo;Your TV Name is :\u0026rdquo; + yourTVName); Console.ReadLine(); } // Function with void and no parameter \u0026ndash; here void means no return type public void veranda() { Console.WriteLine(\u0026ldquo;Welcome to Veranda\u0026rdquo;); Console.WriteLine(\u0026ldquo;How Many Chairs Do you have in your Veranda\u0026rdquo;); NoofChair = Convert.ToInt32(Console.ReadLine()); Console.WriteLine(\u0026ldquo;I have total \u0026quot; + NoofChair + \u0026quot; Chairs in my Veranda\u0026rdquo;); } // Function with Return type as String public string TVNAME() { String YOURTVName; Console.WriteLine(\u0026ldquo;Enter Your TV Brand NAME\u0026rdquo;); YOURTVName = Console.ReadLine(); return YOURTVName; } //Function with parameter public void BedRoom(String nameandColor) { Console.WriteLine(nameandColor); } // Same Function Name with Different Paramenter public void BedRoom(String MemberName, String Color) { Console.WriteLine(MemberName + \u0026quot; Like \u0026quot; + Color + \u0026ldquo;Color\u0026rdquo;); } static void Main(string[] args) { ShanuHouseClass objHouseOwner = new ShanuHouseClass(); objHouseOwner.veranda(); String returnvalue = objHouseOwner.TVNAME(); Console.WriteLine(\u0026ldquo;Your TV BRAND NAME IS: \u0026quot; + returnvalue); objHouseOwner.BedRoom(\u0026ldquo;My Name is Shanu I like Lavender color\u0026rdquo;); objHouseOwner.BedRoom(\u0026ldquo;My Name is Afraz I like Light Blue color\u0026rdquo;); objHouseOwner.BedRoom(\u0026ldquo;SHANU\u0026rdquo;, \u0026ldquo;Lavender\u0026rdquo;); Console.ReadLine(); } } {% endhighlight %} The Output of above class is here:\n5. Access Modifiers Access Modifiers are nothing but the Usage and Limitation of our type like variable, Methods and Class. For Example we can say it as a security limit. This six are the basic Access modifiers which we used in our C# Class/method and in Variables.\n Private  Let’s take our House Example .We will have a Security Guard in House, His duty is till the Entrance of the house. He cannot go inside the house and access all the things. In this case we can create a SecurityGuardClass and declare the variable and method for Security as private .\n Public  House Owners are public to the class where they can access all classes related to the House. They have no restrictions to access their house.\n Internal  Access limit of variable or method is within a project. For example let’s consider in our project we have more than one class and we have declared a variable as internal in one class. Let’s see an example program for internal variable.\n{% highlight csharp %} class Modifiers { public class sampleInternalClass { internal String myInternal = \u0026ldquo;Iam Internal Variable\u0026rdquo;; } class ShanuHouseClass { int noOfTV = 2; public String yourTVName = \u0026ldquo;SAMSUNG\u0026rdquo;; public void myFirstMethod() { Console.WriteLine(\u0026ldquo;You Have total \u0026quot; + noOfTV + \u0026ldquo;no of TV :\u0026quot;); Console.WriteLine(\u0026ldquo;Your TV Name is :\u0026rdquo; + yourTVName); } static void Main(string[] args) { ShanuHouseClass objHouseOwner = new ShanuHouseClass(); objHouseOwner.myFirstMethod(); sampleInternalClass intObj = new sampleInternalClass(); Console.WriteLine(\u0026ldquo;Internal Variable Example :\u0026rdquo; + intObj.myInternal); Console.ReadLine(); } } } {% endhighlight %}\n Protected  Only the main class and derived class can have access of protected variable or method. For example servants and Guests are example for the Protected. For Example Servants can go to all room and do cleaning and other activates. However, they have limitations of access in the house, as they cannot take rest in a bed of house owner.\n Protected Internal  Protected Internal variable or Method has limitation with in a project of class or Derived class. Here is a sample program for Protect Internal Variable .In this example I have used the Inheritance .we will see in detail about Inheritance more detail in this article.\n{% highlight csharp %} public class sampleProtectedInternalClass { protected internal String myprotectedInternal = \u0026ldquo;Iam Protected Internal Variable\u0026rdquo;; public void protectedInternalMethod() { Console.WriteLine(\u0026ldquo;Protected Internal Variable Example :\u0026rdquo; + myprotectedInternal); } }\npublic class derivedClass : sampleProtectedInternalClass { public void derivedprotectedInternal() { Console.WriteLine(\u0026ldquo;Derived Protected Internal Variable Example :\u0026rdquo; + myprotectedInternal); } }\nclass ShanuHouseClass { int noOfTV = 2; public String yourTVName = \u0026ldquo;SAMSUNG\u0026rdquo;; public void myFirstMethod() { Console.WriteLine(\u0026ldquo;You Have total \u0026quot; + noOfTV + \u0026ldquo;no of TV :\u0026quot;); Console.WriteLine(\u0026ldquo;Your TV Name is :\u0026rdquo; + yourTVName); } static void Main(string[] args) { ShanuHouseClass objHouseOwner = new ShanuHouseClass(); objHouseOwner.myFirstMethod(); sampleProtectedInternalClass intObj = new sampleProtectedInternalClass(); intObj.protectedInternalMethod(); derivedClass proIntObj = new derivedClass(); proIntObj.derivedprotectedInternal(); Console.ReadLine(); } } {% endhighlight %}\n The main and major things we need to know in OOP concept are Encapsulation, Abstraction,Polymorphism and Inheritance. We will discuss them in detail in this article.\n 6. Encapsulation Encapsulation is to hide the member or variable to outside class. In our house example, I have already told that House security guard limitation is till the entrance of the house. The security guard doesn’t need to know about what is happening inside the house. Therefore, the House Owner will hide all the happenings to Security guard for more safety. The hiding and limitation are called as Encapsulation.\nFor example, we have two Classes the first one is Houseclass and the other class as houseSecurityClass.\nHere we can see all the variables are wrap into a class where houseSecurityClass is set as public, so the Houseclass can access that, but houseClass has both Public and private variable where the private variable of a class cannot be accessed outside of the class.\n{% highlight csharp %} //Encapsulation in OOP public class houseSecurityClass { public int noofSecurity; public String SecurityName = String.Empty; } public class Houseclass { private int noofLockerinHosue = 2; public string OwnerName = String.Empty; } {% endhighlight %}\n7. Abstraction Abstraction is to show and share some common information to the user. Let’s take our House example, in our house we will have servant, servants can go to all rooms and do cleaning and other works. The house owner can give full rights or some partial rights to the servant for accessing his house. Here we can see an example program as private declared variables and Methods are not shared with servant but the public variable and Methods are shared with servant.\n{% highlight csharp %} public class HouseServerntClass { private int SaftyLoackerKeyNo = 10001; public String roomCleanInstructions = \u0026ldquo;Clean All rooms\u0026rdquo;;\nprivate void saftyNos()\r{\rConsole.WriteLine(\u0026quot;My SaftyLoackerKeyNo is\u0026quot; + SaftyLoackerKeyNo);\r}\rpublic void roomCleanInstruction()\r{\rConsole.WriteLine(roomCleanInstructions);\r}\rstatic void Main(string[] agrs)\r{\rHouseServerntClass a = new HouseServerntClass();\ra.saftyNos();\ra.roomCleanInstruction();\rConsole.ReadLine();\r}\r } {% endhighlight %}\n8. Inheritance Inheritance is used to reuse the code. In Protected Internal Access modifier section we have already seen an example program for Inheritance. Inheritance is nothing but accessing and using all base class variable and methods in the Derived Class. Inheritance can be\n Single level Inheritance  With one Base class and one derived Class for example.\n{% highlight csharp %} public class baseClass { private void privateMethod() { Console.WriteLine(\u0026ldquo;private Method\u0026rdquo;); } public void publicMethod() { Console.WriteLine(\u0026ldquo;This Method Shared\u0026rdquo;); } } public class DerivedClass : baseClass { static void Main(string[] args) { DerivedClass obj = new DerivedClass(); obj.publicMethod(); //This will be error as private method can not be accessed in Derived Class obj.privateMethod(); } } {% endhighlight %}\nThe Error is:\n Some time users might be not clear of what Base class is and what Derived class is. Base class is the super class and derived class is the classes which inherit the base class.\n Here we can see a simple Inheritance where the base class is the GuestVist and derived class is the HouseOwnerClass. Here HouseOwnerClass class inherits the base class of GuestVist\n Multi level Inheritance  With more than one Derived Class for example. Here we can see an example first base class is derived in DerivedClass1 and then the DerivedClass1 is derived in DerivedClass2 .Now from DerivedClass2 we can access both baseClass and DerivedClass1 variable and methods.\n{% highlight csharp %} public class baseClass { private void privateMethod() { Console.WriteLine(\u0026ldquo;private Method\u0026rdquo;); } public void publicMethod() { Console.WriteLine(\u0026ldquo;This Method Shared\u0026rdquo;); } } public class DerivedClass1 : baseClass { public void DerivedClass1() { Console.WriteLine(\u0026ldquo;Derived Class 1\u0026rdquo;); } } public class DerivedClass2 : DerivedClass1 { static void Main(string[] args) { DerivedClass2 obj = new DerivedClass2(); obj.publicMethod(); obj.DerivedClass1(); //This will be error as private method can not be accessed in Derived Class //obj.privateMethod(); } } {% endhighlight %}\n  Multiple Inheritance\n Will the .Net Support Multiple Inheritance? The Answer to this Question is No. In C #, it’s not possible to write a Multiple Inheritance using Class. What is Multiple Inheritance? Multiple Inheritance is nothing but we can have more than one class and we can inherit both Classes in our derived class. What will happen if I write a Multiple Class Inheritance Using C#? Let’s take our example House .Here we have the derived Class as HouseOwnerClass and have Two More classes as GuestVist and FriendsandRelationsClass. Now suppose for our house both Guest and Friend have visited. For this, we write above three classes and inherit the two classes in our derived class. When I write the Multiple Inheritance in C #, it will display the warning message during our code and execute our program. See the below image which shows the Warning error message while I write Multiple Inheritance.    * Then how can we use the Multiple Inheritance\r Interface will be used for Multiple Inheritance.\n9. Polymorphism Ploy means more than one form. In the beginning of the Article at Method Section, we have already seen an example of Polymorphism. Same method name with different parameter is an example for the polymorphism. Method Overloading and Method Overriding will be used in polymorphism. Polymorphism have two types of execution one is Compile Time Polymorphism and the other one is called the Run time Polymorphism.\n Method Overloading  Method overloading are nothing but same Method name will be used for more than one method with different Argument. Here is an example program for the Method Overloading. As we can see here Method name BedRoom has been used for two Method but the parameter for both methods are different.\n{% highlight csharp %} class HouseOwnerClass { //Function with parameter public void BedRoom(String nameandColor) { Console.WriteLine(nameandColor); } // Same Function Name with Different Paramenter public void BedRoom(String MemberName, String Color) { Console.WriteLine(MemberName + \u0026quot; Like \u0026quot; + Color + \u0026quot; Color\u0026rdquo;); } static void Main(string[] args) { HouseOwnerClass objHouseOwner = new HouseOwnerClass(); objHouseOwner.BedRoom(\u0026ldquo;My Name is Shanu I like Lavender color\u0026rdquo;); objHouseOwner.BedRoom(\u0026ldquo;My Name is Afraz I like Light Blue color\u0026rdquo;); objHouseOwner.BedRoom(\u0026ldquo;SHANU\u0026rdquo;, \u0026ldquo;Lavender\u0026rdquo;); Console.ReadLine(); } } {% endhighlight %}\n Method Overriding  The difference between the Method Overloading and Overriding are.In Method Overloading we will have same method name with different argument. In Method Overriding we will have same Method Name same Argument and same type but method overriding can only be used in the derived class, Method Overriding cannot be done in the same class. We will see how Method Overriding can be used in Abstract Method, Virtual Method and in Sealed Method kindly refer to that section in this article.\n10. Abstract Class/Method  Abstract Class: Abstract class will have a keyword abstract.  The Abstract class will be as a super class for all our class. Abstract class cannot be accessed by an object, which means we cannot create an object for an abstract class.\n What will happen when we create an object for an Abstract Class?  Here we can see an error warning message as “An instance of an abstract class cannot be created” when I try to create an object for my abstract class.\nAbstract class can have both Abstract Method and normal Method. In Abstract Class at least one Abstract Method should be declared. In addition, derived class should override the abstract method. To access the abstract method we should use the “override” keyword in our derived class.\n What will happen if we create an abstract method but which is not override in derived class?  Here we can see an abstract class has an abstract method, But the abstract method is not override in the derived class. See the below image for the warning message displaying as class must implement the abstract member.\nHere we can see an example of Abstract Class and for Abstract Method in detail. In this example, we can see an abstract class, which has normal Method and Abstract Method. Abstract Methods; do not have body part in Abstract Class, which means we can only declare an Abstract Method at Abstract Class, There should be minimum one Abstract Method in an Abstract Class.\n{% highlight csharp %} public abstract class GuestVist { public void Guestwelcomemessage() { Console.WriteLine(\u0026ldquo;Welcome to our AbstractHome\u0026rdquo;); } public void GuestName() { Console.WriteLine(\u0026ldquo;Guest name is: Abstract\u0026rdquo;); } public abstract void purposeofVisit(); } // derived class to inherit the abstract class public class A : GuestVist { public override void purposeofVisit() { Console.WriteLine(\u0026ldquo;Abstract just came for a Meetup and spend some time \u0026ldquo;); } static void Main(string[] args) { A objHouse = new A(); objHouse.purposeofVisit(); objHouse.Guestwelcomemessage(); objHouse.GuestName(); Console.ReadLine(); } } {% endhighlight %}\n11. Virtual Class/Method Virtual method is very useful in our day-to-day programming.\n What is virtual Method and what is the use of Virtual Method?  Take our House example one guest confirms, as today total five persons will visit your home. For this, we write a function as message display as five Guest visiting our home. Once Guest visits, we see their total 20 persons have visited. In Some cases it might be increase or decrease we will come to know when they reach us.\nIn that case, the guest will be as a Separate Class and House will be as separate class. Without changing the message in Guest class how can we change the data in our Derived class?\n What is the Difference between Abstract Method and Virtual Method?  Both similarities use the override keyword. Abstract Method can only be declared in Abstract Class, which means no body part for abstract method in Abstract class. However, for virtual it can have body part.\nSee the example program below. Here we have both Abstract Method and Virtual Method.\nIn Abstract class, the virtual method says as total five guests but in the derived Class program, the message was override as 20 guests. See the final output in below. Guess what will be displayed for Virtual Method? Will the result be 5 Guests or 20 Guests check for the output below the program.\n{% highlight csharp %} public abstract class GuestVist { public abstract void purposeofVisit(); // Abstract Method public virtual void NoofGuestwillvisit() // Virtual Method { Console.WriteLine(\u0026ldquo;Total 5 Guest will Visit your Home\u0026rdquo;); } } class AbstractHouseClass : GuestVist { public override void purposeofVisit() // Abstract method Override { Console.WriteLine(\u0026ldquo;Abstract just for a Meetup and spend some time \u0026ldquo;); } public override void NoofGuestwillvisit() // Virtual method override { Console.WriteLine(\u0026ldquo;Total 20 Guest Visited our Home\u0026rdquo;); } static void Main(string[] args) { AbstractHouseClass objHouse = new AbstractHouseClass(); objHouse.purposeofVisit(); objHouse.NoofGuestwillvisit(); Console.ReadLine(); } } {% endhighlight %}\n12. Sealed Class/Method Sealed Class: As name says this class cannot be inherited by other classes.\nTake our House Example. In a house, the Houseowner can have a secret room, as might be official or financial rooms. The owner don’t want others to access his official room. The sealed class will be useful in those cases. Sealed class can be declared using the keyword Sealed. If one class is declared as Sealed, it cannot be inherited in other derived classes.\n What will happen when we inherit sealed class in our derived class?  Let’s see an example when I try to inherit my sealed class from my derived class. It shows me the below warning message.\nHere we can see an example program of Sealed Class.\n{% highlight csharp %} public sealed class OwnerofficialRoom { public void AllMyPersonalItems() { Console.WriteLine(\u0026ldquo;All Items in this rooms are personal to me no one else can access or inherit me\u0026rdquo;); } } class HouseSealedClass { static void Main(string[] args) { OwnerofficialRoom obj = new OwnerofficialRoom(); obj.AllMyPersonalItems(); Console.ReadLine(); } } {% endhighlight %}\nSealed Method: If we declared a method as sealed that specific method cannot be override in the derived class.\nLet’s see our house class here I have base class with Virtual Method and virtual Sealed method.\nThe Virtual method can be override in the derived class .But the Virtual Sealed Method cannot be override in sealed class.\n13. Static Class/Method We have already learned about what is Sealed Class in this Article; now let’s see what are Static Class and Static Method. Both Static and Sealed Class cannot be inherited.\n What is the Difference between Static Class and Sealed Class?  We can create an Object (instance) for the Sealed Class, we can see in my sealed class section I have created a sample Sealed class and in Main Method I have created an object to access the sealed Class. And in a Sealed Class both Static and non-Static methods can be written.\nBut for a Static Class it’s not possible to create an Object. In Static Class only Static members are allowed which means in a static Class it’s not possible to write non-static method.\nWe can say our main method as an example for the Static method. When we create a console application in c# we can see each class will have a default main method. In my article I have explained that when an Console or Windows application start execute first the main method will be executed .There is no need to create an object for the main method since it was been declared as a static method.\nstatic void Main(string[] args)\nAnother interesting one in Static class is that memory will be allocated for all static variable and methods during execution but for the non static variable and methods memory will be allocated only when the object for the class are created. Let’s take our same sealed class Example for our static Class and method.\n What will happen when we inherit Static class in our derived class?  Let’s see an example when I try to inherit my static class from my derived class. It shows me the below warning message.\n What will happen when we declare non-Static method in a Static class?  Let’s see an example when I try to create a non-Static method at my Static Class.\n What will happen when we create an object for the Static class?  Let’s see an example when I try to create an object for my Static Class.\nWhen we run the program we get the error message as Can not create an instance of a static class\n How to call the Static Class Method and variable without creating the Object?  It’s simple just we can use the ClassName.Variable or Method Name for example OwnerofficialRoom.AllMyPersonalItems(); See the below example with Static class\n{% highlight csharp %} public static class OwnerofficialRoom { public static void AllMyPersonalItems() { Console.WriteLine(\u0026ldquo;All Items in this rooms are personal to me no one else can access or inherit me\u0026rdquo;); } } class HouseStaticClass { static void Main(string[] args) { OwnerofficialRoom.AllMyPersonalItems(); Console.ReadLine(); } } {% endhighlight %}\n Is that possible to create a Static Method in non-Static Class?  The answer is yes. We can create a Static Method to the non Static class. No need to create an object to access the static method in non-static class. We can directly use the class name to access the Static method. See the below example with Static method in non-static Class.\n{% highlight csharp %} public class OwnerofficialRoom { public static void AllMyPersonalItems() { Console.WriteLine(\u0026ldquo;No need to create object for me just use my class name to access me :)\u0026quot;); } public void non_staticMethod() { Console.WriteLine(\u0026ldquo;You need to create an Object to Access Me :(\u0026quot;); } } class StaticmethodClass { static void Main(string[] args) { // for statci method no need to create object just call directly using the classname. OwnerofficialRoom.AllMyPersonalItems(); // for non-static method need to create object to access the method. OwnerofficialRoom obj = new OwnerofficialRoom(); obj.non_staticMethod(); Console.ReadLine(); } } {% endhighlight %}\n14. Interface Interface is same like abstract class but in Interface we will have only method declaration but in abstract class we can have both method declaration and method definition .Methods of Interface must be implemented in a implementing class.\nSee the below Example program for an Interface. All the methods of Interface have been implemented in the class. As I have already told you that c# don’t support multiple inheritance for using multiple inheritance we can use Interface.\nThis below program is an example for multiple inheritance using Interface.\n{% highlight csharp %} interface GuestInterface { void GuestWelcomeMessage(); void NoofGuestes(); } interface FriendsandRelationsInterface { void friendwelcomemessage(); void FriendName();} class HouseOwnerClass : GuestInterface, FriendsandRelationsInterface { public void GuestWelcomeMessage() { Console.WriteLine(\u0026ldquo;All guests are well come to our home\u0026rdquo;); } public void NoofGuestes() { Console.WriteLine(\u0026ldquo;Total 15 Guestes has visited\u0026rdquo;); } public void friendwelcomemessage() { Console.WriteLine(\u0026ldquo;Welcome to our Home\u0026rdquo;); } public void FriendName() { Console.WriteLine(\u0026ldquo;Friend name is: Afraz\u0026rdquo;); } static void Main(string[] args) { HouseOwnerClass obj = new HouseOwnerClass(); obj.GuestWelcomeMessage(); obj.NoofGuestes(); obj.friendwelcomemessage(); obj.FriendName(); Console.ReadLine(); } } {% endhighlight %}\nIn some cases we need to have certain methods which will be used in many derived classes. Each derived can implement different functionality for those Methods. In These cases, we can use the Interface. We can say our Guest and house example. For Guest the Welcome Message and No of Guest Function are common, but it will be different for different owners in the same house, Guest might a fathers guest, Mothers Guest, Children’s Guest or Family Guest. Each guest can have different welcome message subject, but the functions are same as Message .let’s consider now Father is a Class, Mother is a class and Children are one Class. Both guestWelcome Message and Noofguest method are same for all. In this case, we can create an Interface and declare both methods in the Interface. All father, mother and Children Classes can inherit the interface and write their own method details.\nInterface is similar to Abstract class but the major difference between the Abstract Class and the Interface are .In Abstract Class there can be both Abstract Method and Non Abstract methods .But in Interface all methods are abstract by default which means there is no non Abstract type method in the Interface. All the Methods declared in Interface should be override in the derived class.\nWhat will happen when non-abstract methods with body part are declared in an Interface? It will display the warning message as unexpected modifier in Access modifier part and Unexpected Method body error warning at message Body.\nThis article repost from here under markdown format. Thank you!\n","permalink":"https://nuxulu.com/posts/basic-oop-concept/","summary":"\u003cblockquote\u003e\n\u003cp\u003eThere are many explanations that we can find in internet about C# OOP, but here in my article I will give a very simple example. In this article, I will use a “House (like the houses we live in) “as a real-time example for easy understanding of OOP Concept in C#.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"1-class\"\u003e1. Class\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eClass\u003c/strong\u003e is a like a Blueprint.\u003c/p\u003e\n\u003cp\u003eBlueprint is outline drawing of our actual plan. For example if we plan to build our new house, the Engineer will explain our new house plan with a blue print as shown in the image below. Once we finalized the plan the engineer will start building the house with same plan.\u003c/p\u003e\n\u003cp\u003eSame like blueprint class is an outline of program. Using the class we can write our own method and declare the variables and using the objects we can access the class Method and Variables. The class will be complete with Variables, Methods and Objects.\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e","title":"Basic C# OOP Concept"},{"content":"All main entry points to the UI and reports are access though the menu structure. This is presented in three ways: the left-hand navigation pane, the content area, and the navigation bar (at the top of the client). The menu design is controlled from the Menus node in the AOT.\nEach module will have a menu, which is created by adding a menu reference (or shortcut) to the main menu:\nThis matches the list of modules shown in the client\u0026rsquo;s left-hand navigation pane.\nCreate Menu in AOT   Open your project.\n  Right-click on the Menus node and select New \u0026gt; Menu.\n  Rename the menu as YourMainMeunuName and open the property sheet.\n  For Porperties of created Menu:\n. Enter the label and create a label.\n. As this menu will be a module with company-specifc information, set the SetCompany property to Yes.\n. Choose the icons in the NormalImage property and set ImageLocation to EmbeddedResource.\n  Create Submenu, by right-clicking on the menu and navigating to New | Submenu.\n  Finally, we need to add our menu as a module to the main menu, which is done as follows:   Place the AOT window next to our project window (if required, open the AOT window by pressing Ctrl + D).\n  Scroll down to Menus and expand MainMenu.\n  Right-click on MainMenu and navigate to New \u0026gt; Menu reference.\n  This opens a new window titled Select: Menus. Locate your menu and drag it to MainMenu, as shown in the following screenshot (the title changes from Select: Menus to the path of the select node when you click on it):\n  Note: Do not drag the menu from your project!\nSave AOT and you can check new menu on AX client.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-03-29-how-to-create-main-menu-in-dynamics-ax-2012/","summary":"\u003cp\u003eAll main entry points to the UI and reports are access though the menu structure. This is presented in three ways: the left-hand navigation pane, the content area, and the navigation bar (at the top of the client). The menu design is controlled from the Menus node in the AOT.\u003c/p\u003e\n\u003cp\u003eEach module will have a menu, which is created by adding a menu reference (or shortcut) to the main menu:\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003cp\u003eThis matches the list of modules shown in the client\u0026rsquo;s left-hand navigation pane.\u003c/p\u003e","title":"How to create main menu in Dynamics AX 2012"},{"content":"For this demo, I will create New module along with NumberSequence, about creating Number Sequence without module you also use same steps just leave some steps base on Design picture above.\nETD Create ETD ContosoId extends num datatype.\nTable Create Contoso Table with ContosoId field.\nEnum Create a new enum value Contoso in BaseEnum NumberSeqModule.\nThis value will be used to link number sequence to the module and to restrict displayed number sequence by module in Form.\nNumberSeqModule Class Create NumberSeqModuleXXX class\nCreate a new NumberSeqModuleXXX class, such as NumberSeqModuleContoso, which extends the NumberSeqApplicationModule class. The sample code for creating this class is as follows:\n{% highlight csharp %} public class NumberSeqModuleContoso extends NumberSeqApplicationModule { }\nprotected void loadModule() { NumberSeqDatatype datatype = NumberSeqDatatype::construct(); ;\n/* Contoso Id */\rdatatype.parmDatatypeId(extendedtypenum(ContosoId));\rdatatype.parmReferenceHelp(\u0026quot;ContosoId\u0026quot;);\rdatatype.parmWizardIsContinuous(true);\rdatatype.parmWizardIsManual(NoYes::No);\rdatatype.parmWizardIsChangeDownAllowed(NoYes::No);\rdatatype.parmWizardIsChangeUpAllowed(NoYes::No);\rdatatype.parmWizardHighest(999999);\rdatatype.addParameterType(NumberSeqParameterType::DataArea, true, false);\rthis.create(datatype);\r }\npublic NumberSeqModule numberSeqModule() { ; return NumberSeqModule::Contoso; } {% endhighlight %}\nUse of the DataArea segment in Step 4 to describe the default segment for the extended data types used for ContosoId.\nNote In Microsoft Dynamics AX 2009, number sequence references could be initialized by restarting the Application Object Server (AOS). In Microsoft Dynamics AX 2012, the initialization of references to populate the NumberSequenceDatatype and NumberSequenceParameterType tables has moved to the initialization checklist. To initialize the newly created references, run a job that executes the LoadModule method likes below.\n{% highlight csharp %} static void loadNumSeq (Args _args) { //define the class variable NumberSeqModuleContoso seqMod = new NumberSeqModuleContoso (); //load the number sequences that were not generated seqMod.load(); } {% endhighlight %}\nYou can also reinitialize all references by running a job that executes the LoadAll method in the NumberSequenceModuleSetup class. However, for reinitializing all references, you must ensure that there are no existing number sequences already defined in the system.\nThen run the wizard in Organization Administration \u0026gt; CommonForms \u0026gt; Numbersequences \u0026gt; Numbersequences \u0026gt; Generate \u0026gt; run the wizard.\nParameters Table and Form Create a Number sequences page in the parameters form of the new module.\nYou need to Create ContosoParameters Table along with form, See existing forms such as CustParameters or LedgerParameters for examples of the implementation.\nThese forms are using DetailsFormMaster form parten as Best Practice for Setup form.\nCreate ContosoParameters Table   Add field key, Extends from ParametersKey\n  Properties on Field key Visible = false, AllowEdit = false, AllowEditOnCreate = false\n  Create index name Key with AllowDuplicate = No.\n  Set table properties.\n  TableContent = Default data\n  ConfigurationKey\n  CacheLookup = Found\n  TableGroup = Parameter\n  PrimaryKey = Key\n  ClusterKey = Key\n  The sample code for creating method this table as below:\n{% highlight csharp %} void delete() { throw error(\u0026quot;@SYS23721\u0026quot;); } public void initValue() { ; super(); // Key is set to mandatory so set it to 1 this.Key = 1; } static ContosoParameters find(boolean _forupdate = false) { ContosoParameters parameter;\nif (_forupdate)\r{\rparameter.selectForUpdate(_forupdate);\r}\rselect firstonly RecId from parameter\rwhere parameter.Key == 1;\rif (!parameter \u0026amp;\u0026amp; !parameter.isTmp())\r{\rCompany::createParameter(parameter);\r}\rreturn parameter;\r } client server static NumberSeqModule numberSeqModule() { ; return NumberSeqModule::Contoso; } public server static NumberSequenceReference numRefContosoId() { ; NumberSeqScopeFactory::CreateDataAreaScope(curext()); return NumberSeqReference::findReference(extendedtypenum (ContosoId));\n} {% endhighlight %}\nCreate ContosoParameters form Note This form can only be used for references that have a scope of DataArea. The administration forms described in the Setup and Administration of number sequences section can be used for references that have any scope. These forms can be found in Organization Administration \u0026gt; Common \u0026gt; Number Sequences\nThe data source of Parameters form likes picture below, you can also refer to CustParameters form for design.\nCode example for form methods:\n{% highlight csharp %} public class FormRun extends ObjectRun { NumberSeqReference numberSeqReference; boolean runExecuteDirect; TmpIdRef tmpIdRef; NumberSeqScope scope; NumberSeqApplicationModule numberSeqApplicationModule; container numberSequenceModules; }\nvoid init() { ; this.numberSeqPreInit(); super(); ContosoParameters::find(); this.numberSeqPostInit(); }\nvoid numberSeqPostInit() { numberSequenceReference_ds.object(fieldNum(NumberSequenceReference, AllowSameAs)).visible(numberSeqApplicationModule.sameAsActive()); referenceSameAsLabel.visible(numberSeqApplicationModule.sameAsActive()); }\nvoid numberSeqPreInit() { ; runExecuteDirect = false;\nnumberSequenceModules = [NumberSeqModule::Contoso];\rnumberSeqApplicationModule = new NumberSeqModuleContoso ();\rscope = NumberSeqScopeFactory::createDataAreaScope();\rNumberSeqApplicationModule::createReferencesMulti(numberSequenceModules, scope);\rtmpIdRef.setTmpData(NumberSequenceReference::configurationKeyTableMulti(numberSequenceModules));\r } {% endhighlight %}\nCode example for NumberSequenceReference data source methods\n{% highlight csharp %} void removeFilter() { ; runExecuteDirect = false; numbersequenceReference_ds.executeQuery(); }\nvoid executeQuery() { if (runExecuteDirect) { super(); } else { runExecuteDirect = true; this.queryRun(NumberSeqReference::buildQueryRunMulti(numberSequenceReference, tmpIdRef, numberSequenceTable, numberSequenceModules, scope)); numbersequenceReference_ds.research(); } } {% endhighlight %}\nHow to use on Table Set number sequence in Contoso Table\n{% highlight csharp %} private void setContosoId() { NumberSeq num; NumberSequenceReference numberSequenceReference; ; numberSequenceReference = CVRParameters::numRefContosoId();\nif (numberSequenceReference)\r{\rnum = NumberSeq::newGetNum(numberSequenceReference);\rthis.Id = num.num();\r}\r } {% endhighlight %}\nOptional method – in case you don’t want to expose Number sequence on Form Level\n{% highlight csharp %} public void initValue() { ; super();\n// Initialise the title id\rthis. setContosoId ();\r }\npublic void insert() { ; if(!this.Id) { this. setContosoId (); }\nsuper();\r } {% endhighlight %}\nFrom now on you can create new record in Contoso Table with number sequence.\nHow to use on Form How to use on form level (In case you don’t want to expose NS in Table Level)\n In the class declaration of the form that will be accessing data, add a variable declaration for the number sequence handler. The following example shows the variable definition for a number sequence handler.  {% highlight csharp %} public class FormRun extends ObjectRun { NumberSeqFormHandler numberSeqFormHandlerContosoId; } {% endhighlight %}\n Add the NumberSeqFormHandler method to the form. The code in this method will create an instance of the number sequence form handler and return it.  {% highlight csharp %} public NumberSeqFormHandler numSeqFormHandlerContosoId() { if (!numberSeqFormHandlerContosoId) { numberSeqFormHandlerContosoId = NumberSeqFormHandler::newForm(ContosoParameters:: numRefContosoId().NumberSequenceId, element, Contoso_ds, fieldNum(Contoso, ContosoId) ); } return numberSeqFormHandlerContosoId; } {% endhighlight %}\n Add create, delete, and write methods to the data source of the table that contains the field for which the number sequence is being used. The following code examples show these methods that are added to the data source for the Contoso table to support the number sequence for the ContosoId field.  {% highlight csharp %} public void create(boolean _append = false) { //before create, (ensure the number seuence has not run out of numbers) element.numSeqFormHandlerContosoId().formMethodDataSourceCreatePre();\n// start: inherited table code\r// we need to let the create happen so the user can\r// choose the type\rsuper(_append);\r//number sequence, create action, (get next number)\relement.numSeqFormHandlerContosoId().formMethodDataSourceCreate();\r } public void delete() { //release the number sequence value. element. numSeqFormHandlerContosoId().formMethodDataSourceDelete();\n//delete the record\rsuper();\r } public void write() { super(); element. numSeqFormHandlerContosoId().formMethodDataSourceWrite(); }\npublic boolean validateWrite() { boolean ret; ret = super(); ret = element.numberSeqFormHandler().formMethodDataSourceValidateWrite(ret) \u0026amp;\u0026amp; ret; if (ret) { Contoso.validateWrite(); } return ret; } Link Active() public void linkActive() { ; element.numberSeqFormHandler().formMethodDataSourceLinkActive(); super(); } {% endhighlight %}\nOptional method Continuous sequence\nTo avoid having gaps for continuous sequence you should add this code to the delete of the table.\n{% highlight csharp %} public void delete() { super(); NumberSeq::releaseNumber(ContosoParameters::numRefContosoId().NumberSequenceId, this.ContosoId); } {% endhighlight %}\nTesting Testing Number sequence by Job\n{% highlight csharp %} static void Max_numberSeqRefCustAccount(Args _args) { NumberSequenceReference numberSeqRefCustAccount = CustParameters::numRefCustAccount(); NumberSeq numSeqCustAccount; try { //Use the TTS level to decide whether sequence is consumed or not. ttsBegin; if (numberSeqRefCustAccount) { numSeqCustAccount = NumberSeq::newGetNum(numberSeqRefCustAccount); if (numSeqCustAccount) { info(numSeqCustAccount.num()); } } ttsCommit; } catch (Exception::Error) { info(\u0026ldquo;Caught \u0026lsquo;Exception::Error\u0026rsquo;.\u0026quot;); } } {% endhighlight %}\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-19-number-sequence-farmework-in-dynamics-ax-2012/","summary":"\u003c!-- raw HTML omitted --\u003e\n\u003cp\u003eFor this demo, I will create New module along with NumberSequence, about creating Number Sequence \u003cem\u003ewithout\u003c/em\u003e module you also use same steps just leave some steps base on Design picture above.\u003c/p\u003e\n\u003ch2 id=\"etd\"\u003eETD\u003c/h2\u003e\n\u003cp\u003eCreate ETD \u003ccode\u003eContosoId\u003c/code\u003e extends \u003ccode\u003enum\u003c/code\u003e datatype.\u003c/p\u003e\n\u003ch2 id=\"table\"\u003eTable\u003c/h2\u003e\n\u003cp\u003eCreate \u003ccode\u003eContoso\u003c/code\u003e Table with \u003ccode\u003eContosoId\u003c/code\u003e field.\u003c/p\u003e\n\u003ch2 id=\"enum\"\u003eEnum\u003c/h2\u003e\n\u003cp\u003eCreate a new enum value \u003ccode\u003eContoso\u003c/code\u003e in BaseEnum \u003ccode\u003eNumberSeqModule\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThis value will be used to link number sequence to the module and to restrict displayed number sequence by module in Form.\u003c/p\u003e","title":"Number sequence farmework overview in Dynamics AX 2012"},{"content":"We normally create action delete in foreign table to make sure referential integrity in Logic and validation.\nThe delete action has the following options: None, Restricted, Cascade, Cascade + Restricted\nNone: This has no effect, and effectively disables the delete action. This is useful if you want to specifically state do nothing so that someone else doesn\u0026rsquo;t try to correct what seems to be an omission.\nRestricted: This will prevent the record from being deleted if there are records in the related table that match the selected relation. This occurs within the validateDelete table event, which is called by the delete table event.\nCascade: This will delete the record in the related table, based on the relation. Ex: A sales order line is of no use without a sales order. This is an extension of the delete table event.\nCascade + Restricted This is a little special. In a two-table scenario, it is the same as Restricted. It will stop the record from being deleted if a related record exists. But if the record is being deleted as part of a cascade from a table related to it, the records will be deleted.\nfor more infomation about how to create DeleteActions at MSDN.\n","permalink":"https://nuxulu.com/posts/2017-01-19-overview-delete-action-in-dyanmics-ax-2012/","summary":"\u003cp\u003eWe normally create action delete in foreign table to make sure referential integrity in Logic and validation.\u003c/p\u003e\n\u003cp\u003eThe delete action has the following options: \u003cstrong\u003eNone, Restricted, Cascade, Cascade + Restricted\u003c/strong\u003e\u003c/p\u003e","title":"Overview delete action in Dyanmics AX 2012"},{"content":"Normally, when we consume AIF Service, we use this code like below to handle Error messages\n{% highlight csharp %} try { client.register(ctx, contract); Console.WriteLine(\u0026ldquo;items registed on Trans Id: \u0026quot; + contract.InventTransId + \u0026quot; with \u0026quot; + contract.Qty + \u0026quot; quantities.\u0026quot;); Console.ReadLine(); } catch (Exception ex) { Console.WriteLine(string.Format(\u0026ldquo;Ex: {0}\u0026rdquo;, ex.Message)); Console.ReadLine(); } {% endhighlight %}\nIf it cause error, message would return like this\nIf you want to know more details, you have to go In Dynamics ax AIF Exceptions form then check\nIt\u0026rsquo;s quite hard for 3rd party developer, especially they don\u0026rsquo;t have right to access AX server.\nAnyway, we can get meaningful error message by doing below steps\n Check that box in AIF inbound ports   Use FaultException class to get message  {% highlight csharp %} try { client.register(ctx, contract); Console.WriteLine(\u0026ldquo;items registed on Trans Id: \u0026quot; + contract.InventTransId + \u0026quot; with \u0026quot; + contract.Qty + \u0026quot; quantities.\u0026quot;); Console.ReadLine();\n} catch (System.ServiceModel.FaultException\u0026lt;ItemsRegistration.RegRef.AifFault\u0026gt; aifFault) { //FaultMessageList[] list = aifFault.Detail.FaultMessageListArray[0]; InfologMessage[] list = aifFault.Detail.InfologMessageList;\nforeach (InfologMessage message in list)\r{\rConsole.WriteLine(message.Message);\r}\rConsole.ReadLine();\r } {% endhighlight %}\nwhat we got\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2017-01-12-handle-aif-error-messages-in-dynamics-ax-2012-r3/","summary":"\u003cp\u003eNormally, when we consume AIF Service, we use this code like below to handle Error messages\u003c/p\u003e\n\u003cp\u003e{% highlight csharp %}\ntry\n{\nclient.register(ctx, contract);\nConsole.WriteLine(\u0026ldquo;items registed on Trans Id: \u0026quot; + contract.InventTransId + \u0026quot; with \u0026quot; + contract.Qty + \u0026quot; quantities.\u0026quot;);\nConsole.ReadLine();\n}\ncatch (Exception ex)\n{\nConsole.WriteLine(string.Format(\u0026ldquo;Ex: {0}\u0026rdquo;, ex.Message));\nConsole.ReadLine();\n}\n{% endhighlight %}\u003c/p\u003e\n\u003cp\u003eIf it cause error, message would return like this\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/Handle-AIF-error-messages-in-dynamics-AX-2012-R3-1.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Handle AIF error messages in dynamics AX 2012 R3"},{"content":"We normally use NETTCP adapter for .NET system, It going to very easy to consume by.\nAs my experience, if we use NETTCP adapter for consuming by Java system, we will need add some extension or 3rd party due to binding and mismatch schema and of course client doesn’t want to do that either me.\nSo, my solution is creating HTTP adapter for Java, you can also use .NET system to consume HTTP adapter without any problems\nHow to do  You need to make sure install web services component on Internet Information Services (IIS) and verify that the default website is working.   In AX, under System administrator \u0026gt; Services and AIF \u0026gt; Web sites, open from a create new record follow information  Name: DynamicsAXAif60 Virtual directory share path: \\\\YourServerName\\MicrosoftDynamicsAXAif60 URL: http://YourServerName/MicrosoftDynamicsAXAif60\n From now on, you can create AIF inbound port with HTTP adapter.  After you\u0026rsquo;ve actived the services, that Port will deployed under DynamicsAXAif60 folder in IIS.\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2017-01-12-how-to-create-http-adapter-port/","summary":"\u003cp\u003eWe normally use \u003ccode\u003eNETTCP\u003c/code\u003e adapter for .NET system, It going to very easy to consume by.\u003c/p\u003e\n\u003cp\u003eAs my experience, if we use \u003ccode\u003eNETTCP\u003c/code\u003e adapter for consuming by Java system, we will need add some extension or 3rd party due to binding and mismatch schema and of course client doesn’t want to do that either me.\u003c/p\u003e\n\u003cp\u003eSo, my solution is creating HTTP adapter for Java, you can also use .NET system to consume HTTP adapter without any problems\u003c/p\u003e","title":"How to create HTTP adapter port"},{"content":"At the previous post I already show how to customize Response Value list in AIF Document standard service, today we will talk about response in Custom AIF service class.\nWe already know for Custom AIF service we actually need 2 classes, one is contract for data input and one is service to process a logic. With Response class, it\u0026rsquo;s literally same with contract class. They both hold parm value.\n  contract class gets parametters.\n  Response class sets return values.\n  Scenario I want to get HcmPersonnelNumberId and HcmWorkerName of current userID on C#.NET application.\nSolution   Write Custom AIF service to get Worker information, and then public this service.\n  Write C#.NET console to consume that service.\n  Walkthrough Response class {% highlight csharp %} [DataContractAttribute] class MaxWorkerResponse { str gId; str gName; } {% endhighlight %}\ntwo value that I want to return is HcmPersonnelNumberId and HcmWorkerName, I will store it in 2 parms method\n{% highlight csharp %} [DataMemberAttribute(\u0026lsquo;HcmWorkerName\u0026rsquo;)] public HcmWorkerName parmName(HcmWorkerName _name = gName) { gName = _name;\nreturn gName;\r }\n[DataMemberAttribute(\u0026lsquo;HcmPersonnelNumberId\u0026rsquo;)] public HcmPersonnelNumberId parmId(HcmPersonnelNumberId _id = gId) { gId = _id;\nreturn gId;\r } {% endhighlight %}\nService class Create MaxPRService Class, This class consume through service that need to be extend SysOperationServiceBase class\n{% highlight csharp %} class MaxPRService extends SysOperationServiceBase { } {% endhighlight %}\nMain logic\n{% highlight csharp %} [SysEntryPointAttribute(true), AifCollectionTypeAttribute(\u0026lsquo;return\u0026rsquo;, Types::String)] public MaxWorkerResponse getEmployee() { HcmWorkerRecId workerRecId; HcmPersonnelNumberId personnelNumber; HcmWorkerName name; MaxWorkerResponse response;\nworkerRecId = DirPersonUser::currentWorker();\rpersonnelNumber = HcmWorker::find(workerRecId).PersonnelNumber;\rname = HcmWorker::find(workerRecId).name();\rresponse = new MaxWorkerResponse();\rresponse.parmId(personnelNumber);\rresponse.parmName(name);\rreturn response;\r } {% endhighlight %}\nCreate service In AOT create new service and add recent created class to that Service, in operations node add getEmployee method, you will get something likes\nThen right click service \u0026gt; Add-ins \u0026gt; Register Service.\ngo to AIF inbound form to create new service and add getEmployee operation to that service then Active.\nConsume service Add recent WSDL URI http://WINSERVER:8104/DynamicsAx/Services/MaxPurchReqGeneral into Service reference in C# Console project\n{% highlight csharp %} static void Main(string[] args) { CallContext context = new CallContext() { Company = \u0026ldquo;USMF\u0026rdquo;, Language = \u0026ldquo;EN-US\u0026rdquo;, };\nMaxPRServiceClient client = new MaxPRServiceClient();\rMaxWorkerResponse response = client.getEmployee(context);\rConsole.WriteLine(response.HcmWorkerName + \u0026quot;, \u0026quot; + response.HcmPersonnelNumberId);\rConsole.ReadLine();\r } {% endhighlight %}\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2017-01-12-respone-in-aif-custom-service-class/","summary":"\u003cp\u003eAt the previous \u003ca href=\"http://nuxulu.com/ax2012/trick/tools/integration/AIF-Custom-response-value-in-Dynamics-AX-2012-R3/\"\u003epost\u003c/a\u003e I already show how to customize Response Value list in AIF Document standard service, today we will talk about response in Custom AIF service class.\u003c/p\u003e\n\u003cp\u003eWe already know for Custom AIF service we actually need 2 classes, one is \u003ccode\u003econtract\u003c/code\u003e for data input and one is \u003ccode\u003eservice\u003c/code\u003e to process a logic. With \u003ccode\u003eResponse\u003c/code\u003e class, it\u0026rsquo;s literally same with \u003ccode\u003econtract\u003c/code\u003e class. They both hold parm value.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003econtract\u003c/code\u003e class\u003c/strong\u003e gets parametters.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ccode\u003eResponse\u003c/code\u003e class\u003c/strong\u003e sets return values.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"scenario\"\u003eScenario\u003c/h3\u003e\n\u003cp\u003eI want to get \u003ccode\u003eHcmPersonnelNumberId\u003c/code\u003e and \u003ccode\u003eHcmWorkerName\u003c/code\u003e of current userID on C#.NET application.\u003c/p\u003e","title":"Response in AIF custom service class"},{"content":"For this demonstration, I use AIF service to create Sales order with SalesSalesOrderService and I gonna consume AIF using C#.NET.\nNormally, in consume service application we handle return value by using EntityKeyList, EntityKey, KeyData[0].Value. And for Sales Order It will return Created SalesID.\nLet\u0026rsquo;s take a look on AxdSalesOrder class. More about Axd class please prefer this.\nThen go to createList method, this method will handle response value\n{% highlight csharp %} public AifEntityKeyList createList( AifDocumentXml _xml, AifEndpointActionPolicyInfo _actionPolicyInfo, AifConstraintListCollection _constraintListCollection) { AifEntityKeyList aifEntityKeyList;\naifEntityKeyList = super(_xml, _actionPolicyInfo, _constraintListCollection);\r// Sales orders are committed - master planning explosion can be executed and confirmed dates be set\rthis.postSalesOrderCreation(aifEntityKeyList);\rreturn aifEntityKeyList;\r } {% endhighlight %}\nSo, how about customer wants another meaning value beside SalesID likes InventTransId information in SalesLine table or another tables base on your requirement. To do that, we need to customize this method.\nHow to do I will use Map and MapEnumerator classes (Please refer MSDN to understand these class), this is steps:\n We get the SalesId from the original entityKeyList and add into Map. add Map to MapEnumerator. Use SalesId to find the sales line. (mapEnumerator.currentValue() can get current SalesID). Create a new entityKey and insert the sales line information to this entityKey. add back entityKey to entityKeyList  From here in entityKeyList will store information of SalesID and List fields of SalesLine table.\nHere is the code for createList method, beside SalesId I will try to get InventTransId in SalesLine Table\n{% highlight csharp %}\npublic AifEntityKeyList createList( AifDocumentXml _xml, AifEndpointActionPolicyInfo _actionPolicyInfo, AifConstraintListCollection _constraintListCollection) { AifEntityKeyList aifEntityKeyList;\nSalesId salesId;\rSalesLine salesLine;\rAifEntityKey entityKey;\rAifEntityKey salesEntityKey;\rMap keyDataMap;\rMap salesOrderMap;\rMapEnumerator mapEnumerator;\raifEntityKeyList = super(_xml, _actionPolicyInfo, _constraintListCollection);\r// Sales orders are committed - master planning explosion can be executed and confirmed dates be set\rthis.postSalesOrderCreation(aifEntityKeyList);\rentityKey = aifEntityKeyList.getEntityKey(1);\rkeyDataMap = entityKey.parmKeyDataMap();\rmapEnumerator = keyDataMap.getEnumerator();\rwhile (mapEnumerator.moveNext())\r{\rsalesId = mapEnumerator.currentValue();\rif (salesId)\r{\rwhile select InventTransId, RecId from salesLine\rwhere salesLine.SalesId == salesId\r{\rsalesEntityKey = new AifEntityKey();\rsalesOrderMap = new Map(Types::Integer, Types::Container);\rsalesEntityKey.parmTableId(tableNum(SalesLine));\rsalesEntityKey.parmRecId(salesLine.RecId);\rsalesOrderMap.insert(fieldNum(SalesLine, InventTransId), [salesLine.InventTransId]);\rsalesEntityKey.parmKeyDataMap(salesOrderMap);\raifEntityKeyList.addEntityKey(salesEntityKey);\r}\r}\r}\rreturn aifEntityKeyList;\r }\n{% endhighlight %}\nCode in C#.NET to get list key\n{% highlight csharp %}\nSalesOrderServiceClient client = new SalesOrderServiceClient(); try { EntityKey[] salesOrderCreatedEntity = client.create(callContext, salesOrder); //EntityKey salesOrderCreated = (EntityKey)salesOrderCreatedEntity.GetValue(0);\nSystem.Collections.IEnumerator enumerator = salesOrderCreatedEntity.GetEnumerator();\rwhile (enumerator.MoveNext())\r{\rEntityKey salesOrderCreated = (EntityKey)enumerator.Current;\rConsole.WriteLine(salesOrderCreated.KeyData[0].Field);\rConsole.WriteLine(salesOrderCreated.KeyData[0].Value);\r}\r//Console.WriteLine(\u0026quot;The sales order created has a Sales ID of \u0026quot; + salesOrderCreated.KeyData[0].Value);\rConsole.ReadLine();\r } catch (Exception e) { Console.WriteLine(e.ToString()); Console.ReadLine(); } {% endhighlight %}\nHere is what we got\nJust remember this one just for AIF Document Standard, for AIF custom service we do another way I will so in next post.\nThank you for reading.\n","permalink":"https://nuxulu.com/posts/2017-01-11-aif-custom-response-value-in-dynamics-ax-2012-r3/","summary":"\u003cp\u003eFor this demonstration, I use AIF service to create Sales order with \u003ccode\u003eSalesSalesOrderService\u003c/code\u003e and I gonna consume AIF using C#.NET.\u003c/p\u003e\n\u003cp\u003eNormally, in consume service application we handle return value by using \u003ccode\u003eEntityKeyList\u003c/code\u003e, \u003ccode\u003eEntityKey\u003c/code\u003e, \u003ccode\u003eKeyData[0].Value\u003c/code\u003e. And for Sales Order It will return Created SalesID.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s take a look on \u003ccode\u003eAxdSalesOrder\u003c/code\u003e class. More about \u003ccode\u003eAxd\u003c/code\u003e class please prefer \u003ca href=\"https://technet.microsoft.com/en-us/library/aa862063.aspx\"\u003ethis\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThen go to \u003ccode\u003ecreateList\u003c/code\u003e method, this method will handle response value\u003c/p\u003e\n\u003cp\u003e{% highlight csharp %}\npublic AifEntityKeyList createList(\nAifDocumentXml                  _xml,\nAifEndpointActionPolicyInfo     _actionPolicyInfo,\nAifConstraintListCollection     _constraintListCollection)\n{\nAifEntityKeyList aifEntityKeyList;\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaifEntityKeyList = super(_xml, _actionPolicyInfo, _constraintListCollection);\r\n\r\n// Sales orders are committed - master planning explosion can be executed and confirmed dates be set\r\nthis.postSalesOrderCreation(aifEntityKeyList);\r\n\r\nreturn aifEntityKeyList;\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e}\n{% endhighlight %}\u003c/p\u003e\n\u003cp\u003eSo, how about customer wants another meaning value beside \u003ccode\u003eSalesID\u003c/code\u003e likes \u003ccode\u003eInventTransId\u003c/code\u003e information in \u003ccode\u003eSalesLine\u003c/code\u003e table or another tables base on your requirement. To do that, we need to customize this method.\u003c/p\u003e","title":"AIF Custom response value in Dynamics AX 2012 R3"},{"content":"Main requirements is Using batch to find and block vendor base on last transaction condition and notify for them by emails.\nSet up E-mail parameters For set up email, we need Go to AX System administrator \u0026gt; Setup \u0026gt; E-mail parameters\nVendor emails Vendor emails locate on LogisticsElectronicAddress.Locator, partyTable.PrimaryContactEmail, partyLocation.Location, please take a look on this job to find how to update Vendor emails and you also could see the relations more clearly.\n{% highlight csharp %} static void UpdateVendorEmail(Args _args) { VendTable vendTable; LogisticsElectronicAddress electronicAddress; DirPartyTable partyTable; DirPartyLocation partyLocation;\nelectronicAddress.initValue();\relectronicAddress.Type = LogisticsElectronicAddressMethodType::Email;\relectronicAddress.Description = \u0026quot;Max Nguyen\u0026quot;;\relectronicAddress.Locator = \u0026quot;luan52@outlook.com\u0026quot;;\relectronicAddress.IsPrimary = NoYes::Yes;\relectronicAddress.insert();\rwhile select forUpdate partyTable\rexists join vendTable\rwhere vendTable.Party == partyTable.RecId\r{\rttsBegin;\rpartyTable.PrimaryContactEmail = electronicAddress.RecId;\rpartyTable.update();\rttsCommit;\rselect firstOnly forupdate partyLocation\rwhere partyLocation.Party == partyTable.RecId;\rif (partyLocation)\r{\rttsBegin;\rpartyLocation.Location = electronicAddress.Location;\rpartyLocation.update();\rttsCommit;\r}\relse\r{\rpartyLocation.initValue();\rpartyLocation.Location = electronicAddress.Location;\rpartyLocation.Party = partyTable.RecId;\rpartyLocation.insert();\r}\r}\r } {% endhighlight %}\nBatch class Main logic here is find Vend accounts are not exist in VendTrans table with condition endTrans.TransDate \u0026gt;= beginDate, and beginDate count from today systemDateGet().\n{% highlight csharp %} public class Max_VendorBlockedBatch extends RunBaseBatch { } {% endhighlight %}\nGet the date before 6 months from today\n{% highlight csharp %} public TransDate getBeginDate() { TransDate beginDate; TransDate currentDate; Months month; YearBase years; Day day;\ncurrentDate = systemDateGet();\rday = dayOfMth(currentDate);\rmonth = mthOfYr(currentDate);\ryears = year(currentDate);\rif (month \u0026lt; 6)\r{\rbeginDate = mkDate(day, 12 - (6 - month) + 1, years - 1);\r}\relse\r{\rbeginDate = mkDate(day, month - 6 + 1, years);\r}\rreturn beginDate;\r } {% endhighlight %}\nSend E-mail\n{% highlight csharp %}\npublic void sendEmail(AccountNum _vendor, str _recipient) { str sender = \u0026lsquo;sender@email.com\u0026rsquo;; str subject = \u0026lsquo;Account blocked\u0026rsquo;; str body = \u0026lsquo;Your account is blocked due to last transaction\u0026rsquo;; List toList; ListEnumerator le; Set permissionSet; System.Exception e; str mailServer; int mailServerPort; System.Net.Mail.SmtpClient mailClient; System.Net.Mail.MailMessage mailMessage; System.Net.Mail.MailAddress mailFrom; System.Net.Mail.MailAddress mailTo; System.Net.Mail.MailAddressCollection mailToCollection; try { toList = strSplit(_recipient, \u0026lsquo;;'); permissionSet = new Set(Types::Class); permissionSet.add(new InteropPermission(InteropKind::ClrInterop)); CodeAccessPermission::assertMultiple(permissionSet); mailServer = SysEmaiLParameters::find(false).SMTPRelayServerName; mailServerPort = SysEmaiLParameters::find(false).SMTPPortNumber; mailClient = new System.Net.Mail.SmtpClient(mailServer, mailServerPort); le = toList.getEnumerator(); le.moveNext(); mailFrom = new System.Net.Mail.MailAddress(sender); mailTo = new System.Net.Mail.MailAddress(strLTrim(strRTrim(le.current()))); mailMessage = new System.Net.Mail.MailMessage(mailFrom, mailTo);  mailToCollection = mailMessage.get_To(); while (le.moveNext()) { mailToCollection.Add(strLTrim(strRTrim(le.current()))); } mailMessage.set_Priority(System.Net.Mail.MailPriority::High); mailMessage.set_Subject(subject); mailMessage.set_Body(body); mailClient.Send(mailMessage); mailMessage.Dispose(); CodeAccessPermission::revertAssert(); info(strFmt(\u0026lsquo;Email was sent to vendor %1.\u0026rsquo;, _vendor)); } catch (Exception::CLRError) { e = ClrInterop::getLastException(); while (e) { info(e.get_Message()); e = e.get_InnerException(); }\n CodeAccessPermission::revertAssert(); } }\n{% endhighlight %}\nInitializes a new instance of the Batch class.\n{% highlight csharp %} public static MAX_VendorBlockedBatch construct() { return new MAX_VendorBlockedBatch(); } {% endhighlight %}\nGets description of the dialog.\n{% highlight csharp %} public static ClassDescription description() { return \u0026lsquo;Vendor blocked batch\u0026rsquo;; } {% endhighlight %}\nFind the vendor without transaction and disable, then send email to vendor\n{% highlight csharp %} public void run() { VendTrans vendTrans; VendTable vendTable; TransDate beginDate; Email email; int i;\ntry\r{\rbeginDate = this.getBeginDate();\rwhile select forUpdate AccountNum, Party from vendTable\rNotexists join vendTrans\rwhere vendTrans.AccountNum == vendTable.AccountNum\r\u0026amp;\u0026amp; vendTrans.TransDate \u0026gt;= beginDate\r{\r//Set the vendor blocked\rttsBegin;\rvendTable.Blocked = CustVendorBlocked::All;\rvendTable.update();\rttsCommit;\r//Send E-mail to vendor\remail = vendTable.email();\rif (email)\r{\rthis.sendEmail(vendTable.AccountNum, email);\r}\relse\r{\rwarning(strFmt('The vendor %1 did not have E-mail address.', vendTable.AccountNum));\r}\r}\r}\rcatch (Exception::Deadlock)\r{\rretry;\r}\r } {% endhighlight %}\nProvides an enter point for the Batchclass.\n{% highlight csharp %} public static void main(Args _args) { MAX_VendorBlockedBatch vendorBlockedBatch = MAX_VendorBlockedBatch::construct();\nif (vendorBlockedBatch.prompt())\r{\rvendorBlockedBatch.run();\r}\r } {% endhighlight %}\nFrom here you can run class and set up recurrence for batch job.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-10-block-vendor-and-send-email-for-notification-in-dynamics-ax/","summary":"\u003cp\u003eMain requirements is Using batch to find and block vendor base on last transaction condition and notify for them by emails.\u003c/p\u003e\n\u003ch3 id=\"set-up-e-mail-parameters\"\u003eSet up E-mail parameters\u003c/h3\u003e\n\u003cp\u003eFor set up email, we need Go to AX \u003ccode\u003eSystem administrator \u0026gt; Setup \u0026gt; E-mail parameters\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/Email-parameters.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"How to block vendor and send email for notification."},{"content":"  There is a maximum of one Primary Key per table, whereas a table can have several alternate keys. The primary key is usually the type of key that other tables, called child tables, refer to when a foreign key field in those other tables need a relational identifier.\n  For new tables the default is a primary key based on the RecId field , incremented number or a completely meaningless number that is generated by the system surrogate key.\n  Alternate key can be chosen as the Replacement Key of a table that can display on forms instead of a meaningless numeric primary key value. Each table can have a maximum of one replacement key.\n  Natural key has meaning to people. Most replacement keys are natural keys.\n  Relations represents a foreign key.\n  Thank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-03-simple-custom-serivce-in-ax-r3/","summary":"There is a maximum of one Primary Key per table, whereas a table can have several alternate keys. The primary key is usually the type of key that other tables, called child tables, refer to when a foreign key field in those other tables need a relational identifier.\n  For new tables the default is a primary key based on the RecId field , incremented number or a completely meaningless number that is generated by the system surrogate key.","title":"Simple summary Keys are in Dynamics AX"},{"content":"I got this error at version Dynamics 2012 R3 CU9, SQL Server 2014, Windows Server 2012 R2.\nThose kind of errors will come when you move report from Dev Server to Live Server, even compilation in DP Class, Query, Contract, table … without any error\n{:.rounded}\nAX1004: Reference System.Core, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089 was not a valid model assembly.MSB3644: The reference assemblies for framework “.NETFramework,Version=v4.0” were not found. To resolve this, install the SDK or Targeting Pack for this framework version or retarget your application to a version of the framework for which you have the SDK Cause There is no reference Assemblies Version 4.0 under C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework on Windows Server 2012 R2 (only Version 4.5 it have)\nSolution Copy those Assemblies from your any where (Windows 7,8,10) to Server\nRestore and compile again.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-02-ax1004-error/","summary":"I got this error at version Dynamics 2012 R3 CU9, SQL Server 2014, Windows Server 2012 R2.\nThose kind of errors will come when you move report from Dev Server to Live Server, even compilation in DP Class, Query, Contract, table … without any error\n{:.rounded}\nAX1004: Reference System.Core, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089 was not a valid model assembly.MSB3644: The reference assemblies for framework “.NETFramework,Version=v4.0” were not found. To resolve this, install the SDK or Targeting Pack for this framework version or retarget your application to a version of the framework for which you have the SDK Cause There is no reference Assemblies Version 4.","title":"AX1004 error in Dynamics AX"},{"content":"InventSum is needed to recalculate sometimes. We should use InventSumRecalcItem class in Dynamics AX.\n{% highlight csharp %}\nInventSumRecalcItem InventSumRecalcItem; ; InventSumRecalcItem = new InventSumRecalcItem(\u0026ldquo;ITEM001\u0026rdquo;, true, checkfix::fix); InventSumRecalcItem.updatenow(); {% endhighlight %}\nFirst parameter : ItemId\nSecond parameter : Show errors\nThird parameter : Fix or only check\nWhat if you want to calculate for all items:\n{% highlight csharp %} InventTable InventTable; InventSumRecalcItem InventSumRecalcItem;\nWHILE SELECT InventTable WHERE (InventTable.ItemType == ItemType::Item) || (InventTable.ItemType == ItemType::BOM) { InventSumRecalcItem = new InventSumRecalcItem(InventTable.ItemId, true, checkfix::fix); InventSumRecalcItem.updatenow(); } {% endhighlight %}\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-02-recalculate-inventsum/","summary":"InventSum is needed to recalculate sometimes. We should use InventSumRecalcItem class in Dynamics AX.\n{% highlight csharp %}\nInventSumRecalcItem InventSumRecalcItem; ; InventSumRecalcItem = new InventSumRecalcItem(\u0026ldquo;ITEM001\u0026rdquo;, true, checkfix::fix); InventSumRecalcItem.updatenow(); {% endhighlight %}\nFirst parameter : ItemId\nSecond parameter : Show errors\nThird parameter : Fix or only check\nWhat if you want to calculate for all items:\n{% highlight csharp %} InventTable InventTable; InventSumRecalcItem InventSumRecalcItem;\nWHILE SELECT InventTable WHERE (InventTable.ItemType == ItemType::Item) || (InventTable.","title":"Recalculate InventSum in Dynamics AX"},{"content":"  There is a maximum of one Primary Key per table, whereas a table can have several alternate keys. The primary key is usually the type of key that other tables, called child tables, refer to when a foreign key field in those other tables need a relational identifier.\n  For new tables the default is a primary key based on the RecId field , incremented number or a completely meaningless number that is generated by the system surrogate key.\n  As The RecId data type, surrogate keys exist on a primary key table. As The RefRecId ETD, surrogate foreign keys exist on a foreign key table (Ex: Party field is on CustTable Table).\n  Alternate key can be chosen as the Replacement Key of a table that can display on forms instead of a meaningless numeric primary key value. Each table can have a maximum of one replacement key.\n  Natural key has meaning to people. A set of fields that uniquely identify a record and would have formed the primary key of the table, if not for the existence of a surrogate key.\n  Relations represents a foreign key.\n  Thank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-02-simple-summary-keys-are-in-dynamics-ax/","summary":"There is a maximum of one Primary Key per table, whereas a table can have several alternate keys. The primary key is usually the type of key that other tables, called child tables, refer to when a foreign key field in those other tables need a relational identifier.\n  For new tables the default is a primary key based on the RecId field , incremented number or a completely meaningless number that is generated by the system surrogate key.","title":"Simple summary Keys are in Dynamics AX"},{"content":"In Dynamics AX, there is a class called SysQueryRangeUtil that can be utilized in both query ranges and table filters. Using methods from this class allow you to be very precise about what dates you want to use in reports or for filtering your data.\nLet’s say you have a report that you always want to run to see orders with shipping dates of the next day. It is possible to do so by using one of the methods from the SysQueryRangeUtil. The use of the letter \u0026rsquo;t' will work for today’s date, but when you try to add days to it, it doesn’t work in reports. Instead, I will use the currentdate() method and add 1 to it.\n All methods \u0026amp; expressions must be surrounded by parentheses as shown below.\n Filtering the requested ship dates in an AX query for tomorrow (current day() + 1)\nOn any form with a grid, you filter your data by pressing Ctrl+G.\nIf I were to want to see open customer invoices from the last 90 days, I would filter my open customer invoices form and use the method (dayRange(-90,0)). The first number represents how many months backward from this month, and the second represents how many months forward. The same sorts of things can be done for the monthRange(), yearRange(), and dateRange() methods. The best part about this is that you can of course save these filters to create views that you might use on a daily basis.\nIf you are creating your query ranges in code, these methods can also be utilized whenever you are setting them. Definitely be sure to check out the SysQueryRangeUtil class as there are many more methods to use.\nHere is some methods you can use:\n{% highlight html %} currentCustomerAccount()\ncurrentVendorAccount()\ncurrentUserId()\ncurrentDate()\ndateRange()\nday()\ndayRange()\ngreaterThanDate()\ngreaterThanUtcDate()\ngreaterThanUtcNow()\nlessthanDate()\nlessthanUtcDate()\nlessthanUtcNow()\nmonthRange()\nyearRange() {% endhighlight %}\n for example:\n {% highlight html %} (dayRange(-30,0)) – Results in a date range for the last 30 days: \u0026ldquo;26-01-2017\u0026rdquo;..\u0026ldquo;25-02-2017\u0026rdquo;\n(day(-1)) – Results in yesterday\u0026rsquo;s date: 24-02-2017\n(day(0)) – Results in today\u0026rsquo;s date: 25-02-2017\n(day(1)) – Results in tomorrow\u0026rsquo;s date: 26-02-2017\n(greaterThanDate(2)) – Results in every date after today plus 2: \u0026gt; 27-02-2017\n(lessThanDate(-1)) – Results in every date of today minus 1: \u0026lt; 24-02-2017\n(monthRange(0,2)) – Results in first day till the last day of the month\u0026rsquo;s choosen (0 = current month): \u0026ldquo;01-02-2017\u0026rdquo;..\u0026ldquo;30-04-2017\u0026rdquo;\n(yearRange(-1,-1)) – Results in first day till the last day of the chosen year: \u0026ldquo;01-01-2017\u0026rdquo;..\u0026ldquo;31-12-2017\u0026rdquo; {% endhighlight %}\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-02-using-methods-in-table-filters-query-ranges-in-dynamics-ax-2012/","summary":"\u003cp\u003eIn Dynamics AX, there is a class called \u003ccode\u003eSysQueryRangeUtil\u003c/code\u003e that can be utilized in both query ranges and table filters. Using methods from this class allow you to be very precise about what dates you want to use in reports or for filtering your data.\u003c/p\u003e\n\u003cp\u003eLet’s say you have a report that you always want to run to see orders with shipping dates of the next day. It is possible to do so by using one of the methods from the \u003ccode\u003eSysQueryRangeUtil\u003c/code\u003e. The use of the letter \u003cstrong\u003e\u0026rsquo;t'\u003c/strong\u003e will work for today’s date, but when you try to add days to it, it doesn’t work in reports. Instead, I will use the \u003ccode\u003ecurrentdate()\u003c/code\u003e method and add \u003ccode\u003e1\u003c/code\u003e to it.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAll methods \u0026amp; expressions must be surrounded by parentheses as shown below.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/Using-Methods-in-Table-Filters-Query-Ranges-in-Dynamics-AX-2012.jpg\" alt=\"Figure 1 – Filtering the requested ship dates in an AX query for tomorrow\"  /\u003e\n\u003c/p\u003e","title":"Using Methods in Table Filters \u0026 Query Ranges in Dynamics AX 2012"},{"content":"In Dynamics AX, we have two way to filter the result set of records in joined query is using QueryFilter class and QueryBuildRange class.\nSo what\u0026rsquo;s difference between them? when do we use QueryFilter class? When do we use QueryBuildRange class?\nToday, i will make a simple sample to show what is difference between them. You can download example project from here.\nI have two table :\nTable DuyDang_ParentTable which have 1 columns : ID.\nTable DuyDang_ChildTable which have 3 columns : ID, ParentID, Qty.\nand they have relation like this : DuyDang_ParentTable.ID = DuyDang_ChildTable.ParentID.\nI have the Outer Join query:\nUse QueryFilter class: {% highlight csharp %} static void DuyDang_QueryFilter(Args _args) { Query query; QueryBuildDataSource qbds, qbds1; QueryRun queryRun; DuyDang_ParentTable parentTable; DuyDang_ChildTable childTable; QueryFilter qFilter; QueryBuildRange qRange; struct structSet;\nstructSet = new struct\r(\u0026quot;str ParentID;\u0026quot;\r+ \u0026quot;str ChildID;\u0026quot;\r+ \u0026quot;real Quantity\u0026quot;\r);\rquery = new Query();\rqbds = query.addDataSource(tableNum(DuyDang_ParentTable)); qbds1 = qbds.addDataSource(tableNum(DuyDang_ChildTable)); qbds1.joinMode(JoinMode::OuterJoin); // Set join type. qbds1.addLink(fieldNum(DuyDang_ParentTable, ID), fieldNum(DuyDang_ChildTable, ParentID));\rqFilter = query.addQueryFilter(qbds1, 'Qty');\rqFilter.value(queryValue(15));\r//qRange = qbds1.addRange(fieldNum(DuyDang_ChildTable, Qty));\r//qRange.value(queryValue(15));\rqueryRun = new QueryRun(query); while (queryRun.next())\r{\rparentTable = queryRun.get(tableNum(DuyDang_ParentTable));\rchildTable = queryRun.get(tableNum(DuyDang_ChildTable));\rstructSet.value(\u0026quot;ParentID\u0026quot;, parentTable.ID);\rstructSet.value(\u0026quot;ChildID\u0026quot;, childTable.ID);\rstructSet.value(\u0026quot;Quantity\u0026quot;, childTable.Qty);\rinfo(structSet.toString());\r}\r } {% endhighlight %}\nand result info here:\nUse QueryBuildRange class: {% highlight csharp %} static void DuyDang_QueryFilter(Args _args) { Query query; QueryBuildDataSource qbds, qbds1; QueryRun queryRun; DuyDang_ParentTable parentTable; DuyDang_ChildTable childTable; QueryFilter qFilter; QueryBuildRange qRange; struct structSet;\nstructSet = new struct\r(\u0026quot;str ParentID;\u0026quot;\r+ \u0026quot;str ChildID;\u0026quot;\r+ \u0026quot;real Quantity\u0026quot;\r);\rquery = new Query();\rqbds = query.addDataSource(tableNum(DuyDang_ParentTable)); qbds1 = qbds.addDataSource(tableNum(DuyDang_ChildTable)); qbds1.joinMode(JoinMode::OuterJoin); // Set join type. qbds1.addLink(fieldNum(DuyDang_ParentTable, ID), fieldNum(DuyDang_ChildTable, ParentID));\r//qFilter = query.addQueryFilter(qbds1, 'Qty');\r//qFilter.value(queryValue(15));\rqRange = qbds1.addRange(fieldNum(DuyDang_ChildTable, Qty));\rqRange.value(queryValue(15));\rqueryRun = new QueryRun(query); while (queryRun.next())\r{\rparentTable = queryRun.get(tableNum(DuyDang_ParentTable));\rchildTable = queryRun.get(tableNum(DuyDang_ChildTable));\rstructSet.value(\u0026quot;ParentID\u0026quot;, parentTable.ID);\rstructSet.value(\u0026quot;ChildID\u0026quot;, childTable.ID);\rstructSet.value(\u0026quot;Quantity\u0026quot;, childTable.Qty);\rinfo(structSet.toString());\r}\r }\n{% endhighlight %}\nand result info here:\nnow you can see the difference between them\n When you use QueryBuidRange class, the restriction is in the ON clause of the OUTER JOIN in the ANSI SQL select statement that is generated by the AOS for the underlying database system.  {% highlight sql %} SELECT * FROM DuyDang_ParentTable(DuyDang_ParentTable_1) OUTER JOIN * FROM DuyDang_ChildTable(DuyDang_ChildTable_1) ON DuyDang_ParentTable.ID = DuyDang_ChildTable.ParentID AND ((Qty = 15)) {% endhighlight %}\n When you use QueryFilter class, the restriction is in the WHERE clause of the OUTER JOIN in the ANSI SQL select statement that is generated by the AOS for the underlying database system.  {% highlight sql %} SELECT * FROM DuyDang_ParentTable(DuyDang_ParentTable_1) OUTER JOIN * FROM DuyDang_ChildTable(DuyDang_ChildTable_1) ON DuyDang_ParentTable.ID = DuyDang_ChildTable.ParentID WHERE ((DuyDang_ChildTable(DuyDang_ChildTable_1).Qty = 15)) {% endhighlight %}\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2017-01-01-difference-between-queryfilter-and-querybuildrange/","summary":"\u003cp\u003eIn Dynamics AX, we have two way to filter the result set of records in joined query is using \u003ccode\u003eQueryFilter\u003c/code\u003e class and \u003ccode\u003eQueryBuildRange\u003c/code\u003e class.\u003c/p\u003e\n\u003cp\u003eSo what\u0026rsquo;s difference between them? when do we use \u003ccode\u003eQueryFilter\u003c/code\u003e class? When do we use \u003ccode\u003eQueryBuildRange\u003c/code\u003e class?\u003c/p\u003e\n\u003cp\u003eToday, i will make a simple sample to show what is difference between them. You can download example project from \u003cem\u003e\u003ca href=\"https://1drv.ms/u/s!AuMuU-6qq5hxiYQLElp6RoLgmB_uIA\"\u003ehere\u003c/a\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eI have two table :\u003c/p\u003e","title":"Difference between QUERYFILTER and QUERYBUILDRANGE"},{"content":"The workflow process moves purchase requisitions through the review process, from an initial status of Draft to a final status of Approved. When a purchase requisition is submitted for review, the workflow process is started. After a purchase requisition is approved, a purchase order can be generated for the purchase requisition lines and submitted to the vendor for order fulfillment.\nWe will use AIF standard service for import PR from outside, service name PurchReqImportService.\nin AxPurchReqTable class and setPurchReqId() method\n{% highlight csharp %} protected void setPurchReqId() { NumberSequenceReference numberSequenceReference;\nif (this.isMethodExecuted(funcName()))\r{\rreturn;\r}\rif (this.isFieldSetExternally(fieldNum(PurchReqTable, PurchReqId)))\r{\rif (this.isSetMethodsCalledFromSave())\r{\rif (!this.purchReqTable())\r{\rnumberSequenceReference = PurchReqTable::numRefPurchReqId();\rthis.checkNumber(numberSequenceReference.numberSequenceTable(),fieldNum(PurchReqTable,PurchReqId),this.parmPurchReqId());\rif (numberSequenceReference.NumberSequenceId \u0026amp;\u0026amp; numberSequenceReference.numberSequenceTable().Continuous)\r{\rNumberSeq::newReserveNum(numberSequenceReference).reserve(this.parmPurchReqId());\r}\r}\r}\r}\relse\r{\rif (this.isFieldSet(fieldNum(PurchReqTable, PurchReqId)))\r{\rreturn;\r}\rif (!this.parmPurchReqId())\r{\rif (this.isSetMethodsCalledFromSave())\r{\r//this.parmPurchReqId(NumberSeq::newGetNum(PurchParameters::numRefPurchReqId()).num());\rnumberSequenceReference = PurchReqTable::numRefPurchReqId();\rif(numberSequenceReference)\r{\rthis.setField(fieldNum(PurchReqTable, PurchReqId), NumberSeq::newGetNum(PurchParameters::numRefPurchReqId()).num());\r}\relse\r{\rthis.setField(fieldNum(PurchReqTable, PurchReqId), this.parmExternalSourceID());\r}\r}\r}\r}\r }\n{% endhighlight %}\nBase on this method, you could know how System get PurchReqId.\nHow to do Go to Inbound ports form to create new service with NETTCP adapter, choose service operations likes below:\nThen active AIF inbound service\nConsume Pruchase requisition service Open visual studio and create new console project.\nAdd service reference\nhttp://DEV-ERP:8101/DynamicsAx/Services/MavPurchaseRequisition\nThis one just for demo, so I just create code base on required fields of AIF.\nHere is the code in main method\n{% highlight csharp %}\nPurchReqImportServiceClient client = new PurchReqImportServiceClient(); CallContext context = new CallContext() { Company = \u0026ldquo;BGR\u0026rdquo;, Language = \u0026ldquo;En-us\u0026rdquo; };\nAxdEntity_PurchReqLine purchReqLine = new AxdEntity_PurchReqLine() { Requisitioner = \u0026ldquo;000007\u0026rdquo;, BuyingLegalEntity = \u0026ldquo;BGR\u0026rdquo;, ItemId = \u0026ldquo;110329\u0026rdquo;, PurchUnitOfMeasure = \u0026ldquo;Box\u0026rdquo;, CurrencyCode = \u0026ldquo;KRW\u0026rdquo;, PurchQty = 100, PurchQtySpecified = true, PriceUnit = 1, PriceUnitSpecified = true, };\n// Create an instance of the document class. AxdEntity_PurchReqTable purchReqTable = new AxdEntity_PurchReqTable() { PurchReqId = \u0026ldquo;\u0026rdquo;, PurchReqName = \u0026ldquo;Purch Req by Max\u0026rdquo;, ExternalSourceID = \u0026ldquo;PR002\u0026rdquo;, ExternalSourceName = \u0026ldquo;PR002\u0026rdquo;, AutoSubmitToWorkflowRequired = AxdEnum_NoYes.No, StatusToBeSaved = AxdEnum_PurchReqCreationStatus.Draft, RequisitionStatus = AxdEnum_PurchReqRequisitionStatus.Draft, RequisitionStatusSpecified = true, RequiredDate = new DateTime(2016, 12, 30), RequiredDateSpecified = true, TransDate = new DateTime(2016, 12, 30), TransDateSpecified = true, PurchReqLine = new AxdEntity_PurchReqLine[1] { purchReqLine } };\n// Create instances of the entities that are used in the service and // set the needed fields on those entities. AxdPurchReqImport purchReq = new AxdPurchReqImport() { PurchReqTable = new AxdEntity_PurchReqTable[1] { purchReqTable } };\ntry { client.create(context, purchReq); } catch (Exception e) { Console.WriteLine(e.ToString()); Console.ReadLine(); }\n{% endhighlight %}\nRun it and check result in AX\nPlease prefer previous post for another operations\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2016-12-29-crud-purchase-requisition-using-aif-in-dynamics-ax-2012-r3/","summary":"\u003cp\u003eThe workflow process moves purchase requisitions through the review process, from an initial status of \u003cstrong\u003eDraft\u003c/strong\u003e to a final status of \u003cstrong\u003eApproved\u003c/strong\u003e. When a purchase requisition is submitted for review, the workflow process is started. After a purchase requisition is approved, a purchase order can be generated for the purchase requisition lines and submitted to the vendor for order fulfillment.\u003c/p\u003e\n\u003cp\u003eWe will use AIF standard service for import PR from outside, service name \u003ccode\u003ePurchReqImportService\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003ein \u003ccode\u003eAxPurchReqTable\u003c/code\u003e class and \u003ccode\u003esetPurchReqId()\u003c/code\u003e method\u003c/p\u003e\n\u003cp\u003e{% highlight csharp %}\nprotected void setPurchReqId()\n{\nNumberSequenceReference numberSequenceReference;\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eif (this.isMethodExecuted(funcName()))\r\n{\r\n    return;\r\n}\r\n\r\nif (this.isFieldSetExternally(fieldNum(PurchReqTable, PurchReqId)))\r\n{\r\n    if (this.isSetMethodsCalledFromSave())\r\n    {\r\n        if (!this.purchReqTable())\r\n        {\r\n            numberSequenceReference = PurchReqTable::numRefPurchReqId();\r\n            this.checkNumber(numberSequenceReference.numberSequenceTable(),fieldNum(PurchReqTable,PurchReqId),this.parmPurchReqId());\r\n            if (numberSequenceReference.NumberSequenceId \u0026amp;\u0026amp; numberSequenceReference.numberSequenceTable().Continuous)\r\n            {\r\n                NumberSeq::newReserveNum(numberSequenceReference).reserve(this.parmPurchReqId());\r\n            }\r\n        }\r\n    }\r\n}\r\nelse\r\n{\r\n    if (this.isFieldSet(fieldNum(PurchReqTable, PurchReqId)))\r\n    {\r\n        return;\r\n    }\r\n\r\n    if (!this.parmPurchReqId())\r\n    {\r\n        if (this.isSetMethodsCalledFromSave())\r\n        {\r\n            //this.parmPurchReqId(NumberSeq::newGetNum(PurchParameters::numRefPurchReqId()).num());\r\n            numberSequenceReference = PurchReqTable::numRefPurchReqId();\r\n            if(numberSequenceReference)\r\n            {\r\n                this.setField(fieldNum(PurchReqTable, PurchReqId), NumberSeq::newGetNum(PurchParameters::numRefPurchReqId()).num());\r\n            }\r\n            else\r\n            {\r\n                this.setField(fieldNum(PurchReqTable, PurchReqId), this.parmExternalSourceID());\r\n            }\r\n        }\r\n    }\r\n}\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e}\u003c/p\u003e\n\u003cp\u003e{% endhighlight %}\u003c/p\u003e\n\u003cp\u003eBase on this method, you could know how System get PurchReqId.\u003c/p\u003e","title":"CRUD Purchase Requisition using AIF in Dynamics AX 2012 R3"},{"content":"Due to Purchase order doesn’t have Standard document service so we have to create new Document service for that using AIF wizards.\nI’m using AIF document service with NETTCP or HTTP Adapter to Create Purchase order service, here is steps\nCreate Query with three datasouce (PurchTable, PurchLine, InventDim) likes below\n As best practice for Document service, name of query should be start with Axd* prefix.\n Using AIF Wizards In AX development environment, go to Tools \u0026gt; Wizards \u0026gt; AIF document service wizards\nChoose recent created query and click next.\nCreate Service operation and AxBC class Click next and then Generate.\nYou will get service project in Private project\nDeploy and create service •\tRight click on PurchOrderService \u0026gt; Add-Ins \u0026gt; Register service\n•\tSystem administration \u0026gt; Setup \u0026gt; Services and AIF \u0026gt; Inbound ports\n•\tClick New on Inbound ports form and name the Service.\n•\tAdapter: NETTCP (it also works with HTTP adapter)\n•\tIn Service contract customizations fast tab click Service operations\nActive recent created Service\nConsume service using C#.NET After service is activated, you can get WSDL URI likes\n http://servername:port/DynamicsAx/Services/PurchaseOrder\nCreate console project and Add Service References, the code below In Class, I will write method to return list of EnityKey PurchId\n{% highlight csharp %} private static EntityKey[] EntityForPurchId(string purchId) { KeyField field = new KeyField() { Field = \u0026ldquo;PurchId\u0026rdquo;, Value = purchId };\nEntityKey key = new EntityKey()\r{\rKeyData = new[] { field }\r};\rreturn new[] { key };\r } {% endhighlight %}\nCode for create purchase order \n{% highlight csharp %} var dim = new AxdEntity_InventDim() { InventSiteId = \u0026ldquo;DN\u0026rdquo;, InventLocationId = \u0026ldquo;F10-S120\u0026rdquo;, InventBatchId = \u0026ldquo;BATCH001\u0026rdquo; };\nvar purchLine = new AxdEntity_PurchLine() { ItemId = \u0026ldquo;220067\u0026rdquo;, PurchQty = 15, PurchUnit= \u0026ldquo;ea\u0026rdquo;,\nInventDim = new AxdEntity_InventDim[] { dim }\r };\nvar purchTable = new AxdEntity_PurchTable() { OrderAccount = \u0026ldquo;101-01-75441\u0026rdquo;, LanguageId = \u0026ldquo;en-us\u0026rdquo;, CurrencyCode = \u0026ldquo;KRW\u0026rdquo;, PurchName = \u0026ldquo;AIF PO Test\u0026rdquo;, PurchLine = new AxdEntity_PurchLine[] { purchLine }\n};\nAxdMav_PurchOrderService purchOrder = new AxdMav_PurchOrderService();\npurchOrder.PurchTable = new AxdEntity_PurchTable[] { purchTable };\nCallContext callContext = new CallContext { Company = \u0026ldquo;bgr\u0026rdquo;, Language = \u0026ldquo;en-us\u0026rdquo; };\nMav_PurchOrderServiceServiceClient client = new Mav_PurchOrderServiceServiceClient();\ntry { EntityKey[] purchOrderCreatedEntity = client.create(callContext, purchOrder); EntityKey purchOrderCreated = (EntityKey)purchOrderCreatedEntity.GetValue(0);\nConsole.WriteLine(\u0026quot;The purch order created has a Purch ID of \u0026quot; + purchOrderCreated.KeyData[0].Value);\rConsole.ReadLine();\r } catch (Exception e) { Console.WriteLine(e.ToString()); Console.ReadLine(); } {% endhighlight %}\nCode for read purchase order \n{% highlight csharp %} EntityKey[] entityKeyList = EntityForPurchId(\u0026ldquo;BGR-000054\u0026rdquo;);\nCallContext callContext = new CallContext(); callContext.Company = \u0026ldquo;bgr\u0026rdquo;; Mav_PurchOrderServiceServiceClient client = new Mav_PurchOrderServiceServiceClient();\nAxdMav_PurchOrderService purchOrders = client.read(callContext, entityKeyList); AxdEntity_PurchTable[] purchTables = purchOrders.PurchTable; AxdEntity_PurchTable purchTable = purchTables[0]; AxdEntity_PurchLine purchLine = purchTable.PurchLine[0];\nConsole.WriteLine(\u0026ldquo;Purch Name: \u0026quot; + purchTable.PurchName); Console.WriteLine(\u0026ldquo;Order Account: \u0026quot; + purchTable.OrderAccount); Console.WriteLine(\u0026ldquo;Language Id: \u0026quot; + purchTable.LanguageId); Console.WriteLine(\u0026ldquo;Qty: \u0026quot; + purchLine.PurchQty); Console.WriteLine(\u0026ldquo;Item Id: \u0026quot; + purchLine.ItemId);\nclient.Close(); Console.ReadLine();\n{% endhighlight %}\nCode for update purchase order \n{% highlight csharp %} Mav_PurchOrderServiceServiceClient client = new Mav_PurchOrderServiceServiceClient(); CallContext callContext = new CallContext(); callContext.Company = \u0026ldquo;bgr\u0026rdquo;;\nEntityKey[] entityKeyList = EntityForPurchId(\u0026ldquo;BGR-000078\u0026rdquo;); AxdMav_PurchOrderService purchOrders = client.read(callContext, entityKeyList);\n//salesOrders.GetHashCode(); AxdEntity_PurchTable[] purchTables = purchOrders.PurchTable; AxdEntity_PurchTable purchTable = new AxdEntity_PurchTable(); purchTable = purchTables.First();\n//salesTable.GetHashCode(); AxdEntity_PurchLine purchLine = new AxdEntity_PurchLine(); purchLine = purchTable.PurchLine.First();\ndecimal purchQty = 20; purchLine.PurchQty = purchQty;\ntry { client.update(callContext, entityKeyList, purchOrders); EntityKey purchOrdersUpdated = (EntityKey)entityKeyList.GetValue(0); Console.WriteLine(\u0026ldquo;The purchase order has been updated has a Purch ID of \u0026quot; + purchOrdersUpdated.KeyData[0].Value + \u0026quot; with Qty \u0026quot; + purchQty.ToString() + \u0026ldquo;\u0026quot;); Console.ReadLine(); } catch (Exception ex) { Console.WriteLine(ex.ToString()); Console.ReadLine(); } {% endhighlight %}\nCode for delete purchase order \n{% highlight csharp %} Mav_PurchOrderServiceServiceClient client = new Mav_PurchOrderServiceServiceClient(); CallContext callContext = new CallContext(); callContext.Company = \u0026ldquo;bgr\u0026rdquo;;\nEntityKey[] entityKeyList = EntityForPurchId(\u0026ldquo;BGR-000054\u0026rdquo;); try { client.delete(callContext, entityKeyList); EntityKey purchOrdersDeleted = (EntityKey)entityKeyList.GetValue(0); Console.WriteLine(\u0026ldquo;The purch order has been deleted has a purch ID of \u0026quot; + purchOrdersDeleted.KeyData[0].Value); Console.ReadLine(); } catch (Exception ex) { Console.WriteLine(ex.ToString()); Console.ReadLine(); } {% endhighlight %}\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2016-12-28-crud-purchase-order-using-aif-in-dynamics-ax-2012-r3/","summary":"\u003cp\u003eDue to Purchase order doesn’t have Standard document service so we have to create new Document service for that using AIF wizards.\u003c/p\u003e\n\u003cp\u003eI’m using AIF document service with \u003ccode\u003eNETTCP\u003c/code\u003e or \u003ccode\u003eHTTP\u003c/code\u003e Adapter to Create Purchase order service, here is steps\u003c/p\u003e\n\u003ch2 id=\"create-query\"\u003eCreate Query\u003c/h2\u003e\n\u003cp\u003ewith three \u003cstrong\u003edatasouce\u003c/strong\u003e (\u003ccode\u003ePurchTable, PurchLine, InventDim\u003c/code\u003e) likes below\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/CRUD-Purchase-order-using-AIF-in-Dynamics-AX-2012-R3-01.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAs best practice for Document service, name of query should be start with Axd* prefix.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"using-aif-wizards\"\u003eUsing AIF Wizards\u003c/h2\u003e\n\u003cp\u003eIn AX development environment, go to \u003cem\u003eTools \u0026gt; Wizards \u0026gt; AIF document service wizards\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/CRUD-Purchase-order-using-AIF-in-Dynamics-AX-2012-R3-02.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eChoose recent created query and click \u003cstrong\u003enext\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"create-service-operation-and-axbc-class\"\u003eCreate Service operation and \u003ccode\u003eAxBC class\u003c/code\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/CRUD-Purchase-order-using-AIF-in-Dynamics-AX-2012-R3-03.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eClick \u003ccode\u003enext\u003c/code\u003e and then \u003ccode\u003eGenerate\u003c/code\u003e.\u003c/p\u003e","title":"CRUD Purchase order using AIF in Dynamics AX 2012 R3"},{"content":"In previous post, I already show how to create purchase order through AIF with NETTCP or HTTP adapter. In this post, we will get little deep more about action on line of order.\nThe following code sample shows how to insert, update, delete a line of an existing purchase order through AIF, currently I’m using C#.NET console project for demo.\nAs Partial update, we must include just the fields to change and any fields required by the document (you can check Data policies in AIF Service ports form for that).\nAlso, notice how action properties are specified – no matter we do with the line, which means updating the order.\nI’m giving you an idea how it looks like, here is the code\nThe first method will handle the key of AIF Service\n{% highlight csharp %} private static EntityKey[] EntityForPurchId(string purchId) { KeyField field = new KeyField() { Field = \u0026ldquo;PurchId\u0026rdquo;, Value = purchId };\nEntityKey key = new EntityKey()\r{\rKeyData = new[] { field }\r};\rreturn new[] { key };\r }\n{% endhighlight %}\ncreate Line, delete Line, update line in Purchase order\n{% highlight csharp %} EntityKey[] entityKeyList = EntityForPurchId(\u0026ldquo;BGR-000054\u0026rdquo;);\nCallContext callContext = new CallContext(); callContext.Company = \u0026ldquo;bgr\u0026rdquo;;\nMav_PurchOrderServiceServiceClient client = new Mav_PurchOrderServiceServiceClient(); AxdMav_PurchOrderService purchOrders = client.read(callContext, entityKeyList);\n//Define which line need to be update or delete var lastLine = purchOrders.PurchTable[0].PurchLine.Last();\nvar purchLine = new AxdEntity_PurchLine() { ItemId = \u0026ldquo;110329\u0026rdquo;, PurchQty = 1, CurrencyCode = \u0026ldquo;KRW\u0026rdquo;, RecIdSpecified = true, LineNumberSpecified = true, action = AxdEnum_AxdEntityAction.create, actionSpecified = true\n//for delete\r//RecId = lastLine.RecId,\r//RecIdSpecified = true,\r//action = AxdEnum_AxdEntityAction.delete,\r//actionSpecified = true\r//for Update\r//RecId = lastLine.RecId,\r//RecIdSpecified = true,\r//action = AxdEnum_AxdEntityAction.update,\r//actionSpecified = true\r };\nvar purchTable = new AxdEntity_PurchTable() { _DocumentHash = purchOrders.PurchTable[0]._DocumentHash, OrderAccount = \u0026ldquo;101-01-75441\u0026rdquo;, LanguageId = \u0026ldquo;en-us\u0026rdquo;, CurrencyCode = \u0026ldquo;KRW\u0026rdquo;, PurchName = \u0026ldquo;AIF PO Test\u0026rdquo;, action = AxdEnum_AxdEntityAction.update, actionSpecified = true, PurchLine = new[] { purchLine } };\nAxdMav_PurchOrderService purchOrder = new AxdMav_PurchOrderService() { PurchTable = new AxdEntity_PurchTable[] {purchTable} };\nclient.update(callContext, entityKeyList, purchOrder);\n{% endhighlight %}\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2016-12-28-insert-update-delete-order-line-through-aif-in-ax-2012-r3/","summary":"\u003cp\u003eIn previous \u003ca href=\"https://dynamics365.github.io/ax2012/integration/CRUD-Purchase-order-using-AIF-in-Dynamics-AX-2012-R3/\"\u003epost\u003c/a\u003e, I already show how to create purchase order through AIF with \u003ccode\u003eNETTCP\u003c/code\u003e or \u003ccode\u003eHTTP adapter\u003c/code\u003e. In this post, we will get little deep more about action on line of order.\u003c/p\u003e\n\u003cp\u003eThe following code sample shows how to \u003ccode\u003einsert\u003c/code\u003e, \u003ccode\u003eupdate\u003c/code\u003e, \u003ccode\u003edelete\u003c/code\u003e a line of an existing purchase order through AIF, currently I’m using C#.NET console project for demo.\u003c/p\u003e\n\u003cp\u003eAs Partial update, we must include just the fields to change and any fields required by the document (you can check \u003ccode\u003eData policies\u003c/code\u003e in AIF Service ports form for that).\u003c/p\u003e\n\u003cp\u003eAlso, notice how action properties are specified – no matter we do with the line, which means updating the order.\u003c/p\u003e\n\u003cp\u003eI’m giving you an idea how it looks like, here is the code\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eThe first method will handle the key of AIF Service\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e{% highlight csharp %}\nprivate static EntityKey[] EntityForPurchId(string purchId)\n{\nKeyField field = new KeyField()\n{\nField = \u0026ldquo;PurchId\u0026rdquo;,\nValue = purchId\n};\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eEntityKey key = new EntityKey()\r\n{\r\n\tKeyData = new[] { field }\r\n};\r\n\r\nreturn new[] { key };\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e}\u003c/p\u003e\n\u003cp\u003e{% endhighlight %}\u003c/p\u003e","title":"Insert, update, Delete order line through AIF In AX 2012 R3"},{"content":"Scenarios: I’m trying to create product/master product in Dynamics AX using AIF inbound port, the AIF services consume by C#.NET.\nFrom AX 2012 R2, Item is replaced with Product. Item master was in Inventory Management Module, now there is a separate module for item/product creation Product information Management.\nSome definitions you should know There are two types of Products in 2012 they are:\n  Product\nProduct information management/Common/Products/Products\n  Product Master\nProduct information management/Common/Products/Products master\na. Variants:\n  To create a product variant, you must define at least one product dimension for a product master. You can also rename dimensions. To create product variants, you must complete the following tasks:\n  Set up dimensions, such as size, color, and style.\n  Set up variant groups.\n  Assign variant groups to a retail hierarchy.\n  Create a product master and variants.\n  b. Product dimensions\nProduct dimensions are characteristics that serve to identify a product variant. You can use combinations of product dimensions to define product variants. You must define at least one product dimension for a product master to create a product variant.\nProcess: Normally in AX, we create items master follow process:\n  Create product/product master.\n  Assigning Dimensions Groups to a Product Master.\n  Create Product dimension combinations (Product Variants)\n  Release product to legal entities\n  Assigning Item Model Group \u0026amp; Item Groups to a Product Master\n  How to do: Ax provides us standard services for this purpose, so we don’t need to create any custom services for this. I will use 4 services for this purpose, descriptions below\n   Service Purpose     EcoResProductService Create products (all types). The service can also be used to retrieve data that has already been created (Create Product details in The EcoRes tables).   \u0026mdash;-    EcoResProductMasterDimValueService Specify values of product dimensions for a product master. These values become available for the creation of product variants. The service can also be used to retrieve data that has already been created.   \u0026mdash;-    ItemService Release distinct products and product masters. The service can also be used to retrieve data that has already been created.   \u0026mdash;-    InventDimCombinationService Release product variants. The service can also be used to retrieve data that has already been created.   {: rules=\u0026ldquo;groups\u0026rdquo;}     we have 4 steps\n  Create 4 AIF inbound services against Services operation above and active it http://DEV-ERP:8103/DynamicsAx/Services/BCEcoResProduct http://DEV-ERP:8103/DynamicsAx/Services/BCEcoResProductMasterDimValue http://DEV-ERP:8103/DynamicsAx/Services/BCItemsMaster http://DEV-ERP:8103/DynamicsAx/Services/BCInventDimCombination\n  After services creation, open visual studio then creates new Console project and add service References for that, you will get somethings like pic below:\n  Using C#.Net to consume service\n  {% highlight csharp %}\nusing ItemsMaster.ItemsRef; using ItemsMaster.EcoResProductRef; using ItemsMaster.EcoResProductMasterRef; using ItemsMaster.InventDimRef; static void Main(string[] args) { Program master = new Program(); master.createDistinctProduct(); Program.releaseProduct(); }\n{% endhighlight %}\nEcoResProductServiceClient\n{% highlight csharp %}\npublic void createDistinctProduct() { AxdEntity_Product_EcoResDistinctProduct distinctProduct = new AxdEntity_Product_EcoResDistinctProduct() { DisplayProductNumber = \u0026ldquo;MAX00002\u0026rdquo;, ProductType = AxdEnum_EcoResProductType.Item, SearchName = \u0026ldquo;Max\u0026rdquo;,\n};\rdistinctProduct.Translation = new AxdEntity_Translation[1];\rdistinctProduct.Translation[0] = new AxdEntity_Translation()\r{\rLanguageId = \u0026quot;en-us\u0026quot;,\rName = \u0026quot;Max Nguyen\u0026quot;\r};\rdistinctProduct.Identifier = new AxdEntity_Identifier[1];\rdistinctProduct.Identifier[0] = new AxdEntity_Identifier()\r{\rProductNumber = \u0026quot;MAX00002\u0026quot;\r};\rdistinctProduct.StorageDimGroup = new AxdEntity_StorageDimGroup[1];\rdistinctProduct.StorageDimGroup[0] = new AxdEntity_StorageDimGroup()\r{\rProduct = \u0026quot;MAX00002\u0026quot;,\rStorageDimensionGroup = \u0026quot;SW_P\u0026quot;\r};\rdistinctProduct.TrackingDimGroup = new AxdEntity_TrackingDimGroup[1];\rdistinctProduct.TrackingDimGroup[0] = new AxdEntity_TrackingDimGroup()\r{\rProduct = \u0026quot;MAX00002\u0026quot;,\rTrackingDimensionGroup = \u0026quot;Batch Only\u0026quot;\r};\rAxdEcoResProduct product = new AxdEcoResProduct()\r{\rProduct = new AxdEntity_Product_EcoResProduct[1] { distinctProduct }\r};\rEcoResProductRef.CallContext EcoResProductSctx = new EcoResProductRef.CallContext()\r{\rCompany = \u0026quot;bgr\u0026quot;,\rLanguage = \u0026quot;en-us\u0026quot;,\r};\rEcoResProductRef.EcoResProductServiceClient ecoResProductSClient = new EcoResProductRef.EcoResProductServiceClient();\recoResProductSClient.create(EcoResProductSctx, product);\r }\n{% endhighlight %}\nEcoResProductServiceClient\n{% highlight csharp %}\npublic void createMaster() { AxdEntity_Product_EcoResProductMaster productMaster = new AxdEntity_Product_EcoResProductMaster() { DisplayProductNumber = \u0026ldquo;MAX00002\u0026rdquo;, ProductType = AxdEnum_EcoResProductType.Item, SearchName = \u0026ldquo;Max Nguyen\u0026rdquo; };\nproductMaster.Translation = new AxdEntity_Translation[1];\rproductMaster.Translation[0] = new AxdEntity_Translation()\r{\rLanguageId = \u0026quot;en-us\u0026quot;,\rName = \u0026quot;Max Nguyen\u0026quot;\r};\rproductMaster.Identifier = new AxdEntity_Identifier[1];\rproductMaster.Identifier[0] = new AxdEntity_Identifier()\r{\rProductNumber = \u0026quot;MAX00002\u0026quot;\r};\rproductMaster.ProductDimGroup = new AxdEntity_ProductDimGroup[1];\rproductMaster.ProductDimGroup[0] = new AxdEntity_ProductDimGroup()\r{\rProduct = \u0026quot;MAX00002\u0026quot;,\rProductDimensionGroup = \u0026quot;MAX10\u0026quot;\r};\rproductMaster.VariantConfigurationTechnology = AxdEnum_EcoResVariantConfigurationTechnologyType.PredefinedVariants;\rAxdEcoResProduct axdProduct = new AxdEcoResProduct()\r{\rProduct = new AxdEntity_Product_EcoResProduct[1] { productMaster }\r};\rEcoResProductRef.CallContext EcoResProductSctx = new EcoResProductRef.CallContext()\r{\rCompany = \u0026quot;bgr\u0026quot;,\rLanguage = \u0026quot;en-us\u0026quot;,\r};\rEcoResProductRef.EcoResProductServiceClient ecoResProductSClient = new EcoResProductRef.EcoResProductServiceClient();\recoResProductSClient.create(EcoResProductSctx, axdProduct);\r }\n{% endhighlight %}\nEcoResProductMasterDimValueServiceClient\n{% highlight csharp %}\nstatic void createMasterDimensions() {\nEcoResProductMasterRef.AxdEntity_EcoResSize ecoResSizeL = new EcoResProductMasterRef.AxdEntity_EcoResSize()\r{\rName = \u0026quot;L\u0026quot;\r};\rEcoResProductMasterRef.AxdEntity_EcoResSize ecoResSizeM = new EcoResProductMasterRef.AxdEntity_EcoResSize()\r{\rName = \u0026quot;M\u0026quot;\r};\r//master dimensions definition (two sizes, L and M)\rAxdEntity_MasterDim_EcoResProductMasterSize sizeDimensionL = new AxdEntity_MasterDim_EcoResProductMasterSize()\r{\rSizeProductMaster = \u0026quot;MAX00002\u0026quot;,\rSize = \u0026quot;L\u0026quot;,\rEcoResSize = new EcoResProductMasterRef.AxdEntity_EcoResSize[1] { ecoResSizeL }\r};\rAxdEntity_MasterDim_EcoResProductMasterSize sizeDimensionM = new AxdEntity_MasterDim_EcoResProductMasterSize()\r{\rSizeProductMaster = \u0026quot;MAX00002\u0026quot;,\rSize = \u0026quot;M\u0026quot;,\rEcoResSize = new EcoResProductMasterRef.AxdEntity_EcoResSize[1] { ecoResSizeM }\r};\rAxdEcoResProductMasterDimValue axdDimValue = new AxdEcoResProductMasterDimValue()\r{\rMasterDim = new AxdEntity_MasterDim_EcoResProductMasterDimensionValue[2] { sizeDimensionL, sizeDimensionM }\r};\rEcoResProductMasterRef.CallContext masterDimctx = new EcoResProductMasterRef.CallContext();\rEcoResProductMasterDimValueServiceClient masterDimensionService = new EcoResProductMasterDimValueServiceClient();\rtry\r{\rmasterDimensionService.create(masterDimctx, axdDimValue);\r}\rcatch (Exception e)\r{\rSystem.Console.WriteLine(e.Message);\rSystem.Console.ReadKey();\r}\r }\n{% endhighlight %}\nEcoResProductServiceClient\n{% highlight csharp %}\nstatic void createVariant() { //product variant definition AxdEntity_Product_EcoResDistinctProductVariant productVariant = new AxdEntity_Product_EcoResDistinctProductVariant() { DisplayProductNumber = \u0026ldquo;MAXL\u0026rdquo;, ProductType = AxdEnum_EcoResProductType.Item, SearchName = \u0026ldquo;MAXL\u0026rdquo;, ProductMaster = \u0026ldquo;MAX00002\u0026rdquo; }; productVariant.Translation = new AxdEntity_Translation[1]; productVariant.Translation[0] = new AxdEntity_Translation() { LanguageId = \u0026ldquo;en-us\u0026rdquo;, Name = \u0026ldquo;Max L size\u0026rdquo; }; productVariant.VariantDimValue = new AxdEntity_VariantDimValue_EcoResProductVariantDimensionValue[1]; productVariant.VariantDimValue[0] = new AxdEntity_VariantDimValue_EcoResProductVariantSize() { DistinctProductVariant = \u0026ldquo;MAXL\u0026rdquo;, ProductDimensionAttribute = 3173,//The ID of the EcoResSize table Size = \u0026ldquo;L\u0026rdquo;, EcoResSize = new EcoResProductRef.AxdEntity_EcoResSize[1] { new EcoResProductRef.AxdEntity_EcoResSize() { Name = \u0026ldquo;L\u0026rdquo; } } };\nAxdEcoResProduct axdProduct = new AxdEcoResProduct()\r{\rProduct = new AxdEntity_Product_EcoResProduct[1] { productVariant }\r};\rEcoResProductRef.CallContext inventDimctx = new EcoResProductRef.CallContext();\rEcoResProductServiceClient productService = new EcoResProductServiceClient();\rtry\r{\rproductService.create(inventDimctx, axdProduct);\r}\rcatch (Exception e)\r{\rSystem.Console.WriteLine(e.Message);\rSystem.Console.ReadKey();\r}\r }\n{% endhighlight %}\nthis method can use to release a distinct product or a product master\n{% highlight csharp %}\npublic static void releaseProduct() { var invent = new AxdEntity_Invent() { ItemId = \u0026ldquo;MAX00002\u0026rdquo;, UnitId = \u0026ldquo;box\u0026rdquo; };\nvar purch = new AxdEntity_Purch()\r{\rItemId = \u0026quot;MAX00002\u0026quot;,\rUnitId = \u0026quot;box\u0026quot;\r};\rvar sales = new AxdEntity_Sales()\r{\rItemId = \u0026quot;MAX00002\u0026quot;,\rUnitId = \u0026quot;box\u0026quot;\r};\rvar inventTable = new AxdEntity_InventTable()\r{\rItemId = \u0026quot;MAX00002\u0026quot;,\rPmfProductType = AxdEnum_PmfProductType.MainItem,\rProduct = \u0026quot;MAX00002\u0026quot;,\rInvent = new AxdEntity_Invent[] { invent },\rPurch = new AxdEntity_Purch[] { purch },\rSales = new AxdEntity_Sales[] { sales },\r};\rvar item = new AxdItem()\r{\rInventTable = new AxdEntity_InventTable[1] { inventTable }\r};\rItemsRef.CallContext callContext = new ItemsRef.CallContext();\rItemsRef.ItemServiceClient client = new ItemsRef.ItemServiceClient();\rcallContext.Company = \u0026quot;bgr\u0026quot;;\rcallContext.Language = \u0026quot;en-us\u0026quot;;\rtry\r{\rItemsRef.EntityKey[] itemCreatedEntity = client.create(callContext, item);\rItemsRef.EntityKey itemCreated = (ItemsRef.EntityKey)itemCreatedEntity.GetValue(0);\rConsole.WriteLine(\u0026quot;AxdEntity_Invent \u0026quot; + itemCreated.KeyData[0].Value);\rConsole.ReadLine();\r}\rcatch (Exception e)\r{\rConsole.WriteLine(e.ToString());\rConsole.ReadLine();\r}\r }\n{% endhighlight %}\nRelease product Variants\n{% highlight csharp %}\nstatic void releaseProductVariants() { AxdEntity_InventDimCombination releasedVariant = new AxdEntity_InventDimCombination() { DistinctProductVariant = \u0026ldquo;MAX00002\u0026rdquo;, ItemId = \u0026quot;\u0026quot; };\nAxdInventDimCombination inventDimCombination = new AxdInventDimCombination()\r{\rInventDimCombination = new AxdEntity_InventDimCombination[1] { releasedVariant }\r};\rInventDimRef.CallContext itemsctx = new InventDimRef.CallContext()\r{\rCompany = \u0026quot;bgr\u0026quot;,\rLanguage = \u0026quot;en-us\u0026quot;\r};\rInventDimCombinationServiceClient inventDimCombinationService = new InventDimCombinationServiceClient();\rtry\r{\rinventDimCombinationService.create(itemsctx, inventDimCombination);\r}\rcatch (Exception e)\r{\rSystem.Console.WriteLine(e.Message);\rSystem.Console.ReadKey();\r}\r }\n{% endhighlight %}\nJust for example, in main I only create Distinct product and release it, but you can use another method to create master, variant, masterDim and so on.\n4. Try to run it and here is a result\r Thank you for reading and feel free to give me a question.\n","permalink":"https://nuxulu.com/posts/2016-12-27-crud-items-products-products-master-dimension-variants-using-aif-in-dynamics-ax-2012-r3/","summary":"\u003ch2 id=\"scenarios\"\u003eScenarios:\u003c/h2\u003e\n\u003cp\u003eI’m trying to create \u003cstrong\u003eproduct/master product\u003c/strong\u003e in Dynamics AX using AIF inbound port, the AIF services consume by C#.NET.\u003c/p\u003e\n\u003cp\u003eFrom AX 2012 R2, Item is replaced with Product. Item master was in Inventory Management Module, now there is a separate module for item/product creation \u003ccode\u003eProduct information Management\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSome definitions you should know\nThere are two types of Products in 2012 they are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eProduct\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eProduct information management/Common/Products/Products\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eProduct Master\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eProduct information management/Common/Products/Products master\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003ea. \u003cstrong\u003eVariants:\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTo create a product variant, you must define at least one product dimension for a product master. You can also rename dimensions.\nTo create product variants, you must complete the following tasks:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eSet up dimensions, such as size, color, and style.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSet up variant groups.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAssign variant groups to a retail hierarchy.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a product master and variants.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"CRUD Items, products, products master dimension, variants using AIF in Dynamics AX 2012 R3"},{"content":"The Microsoft Dynamics AX runtime manages the storage of value type data on the call stack and reference type objects on the memory heap.\nThe call stack is the memory structure that holds data about the active methods called during program execution. The memory heap is the memory area that allocates storage for objects that are destroyed automatically by the Microsoft Dynamics AX run-time.\nValue types Value types include the built-in primitive types, extended data types, enumeration types, and built-in collection types.\n The primitive types are boolean, int, int64, real, date, utcDateTime, timeofday, str, and guid. The extended data types are specialized primitive types and specialized base enumerations. The enumeration types are base enumerations and extended data types. The collection types are the built-in array and container types.   By default, variables declared as value types are assigned their zero value by the Microsoft Dynamics AX runtime. These variables can’t be set to null. Variable values are copied when variables are used to invoke methods and when they are used in assignment statements. Therefore, two value type variables can’t reference the same value.\n Reference types Reference types include the record types, class types, and interface types.\n The record types are table, map, and view. User-defined record types are dynamically composed from application model layers. Microsoft Dynamics AX runtime record types are exposed in the system application programming interface (API). Although the methods are not visible in the AOT, all record types implement the methods that are members of the system xRecord type, a Microsoft Dynamics AX runtime class type. User-defined class types are dynamically composed from application model layers and Microsoft Dynamics AX runtime class types exposed in the system API. Interface types are type specifications and can’t be instantiated in the Microsoft Dynamics AX runtime. Class types can, however, implement interfaces. Variables declared as reference types contain references to objects that the Microsoft Dynamics AX runtime instantiates from dynamically composed types defined in the application model layering system and from types exposed in the system API. The Microsoft Dynamics AX runtime also performs memory deallocation (garbage collection) for these objects when they are no longer referenced.   Reference variables declared as record types reference objects that the Microsoft Dynamics AX runtime instantiates automatically. Class type objects are programmatically instantiated using the new operator. Copies of object references are passed as reference parameters in method calls and are assigned to reference variables, so two variables can reference the same object.\n Thank you for reading!\n","permalink":"https://nuxulu.com/posts/2016-11-25-the-type-system-of-dynamics-ax-2012/","summary":"\u003cp\u003eThe Microsoft Dynamics AX runtime manages the storage of \u003cem\u003evalue type\u003c/em\u003e data on the \u003cstrong\u003ecall stack\u003c/strong\u003e and \u003cem\u003ereference type objects\u003c/em\u003e on the \u003cstrong\u003ememory heap\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe call stack\u003c/strong\u003e is the memory structure that holds data about the active methods called during program execution. \u003cstrong\u003eThe memory heap\u003c/strong\u003e is the memory area that allocates storage for objects that are destroyed automatically by the Microsoft Dynamics AX run-time.\u003c/p\u003e\n\u003ch2 id=\"value-types\"\u003eValue types\u003c/h2\u003e\n\u003cp\u003eValue types include the built-in \u003ccode\u003eprimitive types, extended data types, enumeration types, and built-in collection types\u003c/code\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eThe primitive types\u003c/strong\u003e are boolean, int, int64, real, date, utcDateTime, timeofday, str, and guid.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe extended data types\u003c/strong\u003e are specialized primitive types and specialized base enumerations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe enumeration types\u003c/strong\u003e are base enumerations and extended data types.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe collection types\u003c/strong\u003e are the built-in array and container types.\u003c/li\u003e\n\u003c/ul\u003e","title":"The Type system of Dynamics AX 2012"},{"content":"{%- include extensions/youtube.html id=\u0026lsquo;tL1ZMGNbYIo\u0026rsquo; -%}\n","permalink":"https://nuxulu.com/posts/2016-11-23-copy-data-between-microsoft-dynamics-ax-companies-using-dixf-in-ax-2012-r3/","summary":"{%- include extensions/youtube.html id=\u0026lsquo;tL1ZMGNbYIo\u0026rsquo; -%}","title":"Copy data between Microsoft Dynamics AX companies using DIXF In AX 2012 R3"},{"content":"{%- include extensions/youtube.html id=\u0026lsquo;6B5cqeeNvH4\u0026rsquo; -%}\n","permalink":"https://nuxulu.com/posts/2016-11-23-step-by-step-how-to-build-ssrs-report-advanced-rdp-class/","summary":"{%- include extensions/youtube.html id=\u0026lsquo;6B5cqeeNvH4\u0026rsquo; -%}","title":"Step by Step How to build SSRS Report - Advanced RDP Class"},{"content":"Document services Document services use documents to represent business objects such as purchase and sales orders, customers, vendors, and so on.\nA document service is composed of the following components:\n Document query : This is a query that is created in the Application Object Tree (AOT) and contains all the tables that are related to the business object that you want to expose. Based on this query, the Document Service Generation Wizard can be used to generate the other artifacts that make up the document service. Table AxBC classes : An AxBC class is a wrapper for a table and contains business logic that is needed for Create, Read, Update, Delete (CRUD) operations. Document class : The purpose of the document class is to contain business logic that is associated with the creation and modification of the business entity itself. For example, the AxdCustomer class could contain logic to handle party information of a customer. Document service class : This is the actual service implementation class and extends the AifDocumentService class. This class implements the service operations that are published through the service contract.  When creating document services, developers need to make sure that the business object is mapped correctly to the document query. The document services framework will handle all other things such as the serialization and deserialization of XML, date effectiveness, and so on.\nDocument services can be deployed using the integration ports and all available adapters can be used.\nCustom services Custom services were already available in Microsoft Dynamics AX 2009, but support for Extended Data Types(EDTs) was limited, which resulted in developers having to provide custom serialization and deserialization logic.\nMicrosoft Dynamics AX 2012 introduces the concept of attributes. Attributes provide a way to specify metadata about classes and methods. Two of these attributes are used when creating data contracts: the DataContractAttribute and DataMemberAttribute attributes.\nThe \u0026lsquo;DataContractAttribute\u0026rsquo; attribute is used to define that a class is a data contract. The\u0026rsquo;DataMemberAttribute' attribute is added to methods of data contracts that represent data members that have to be exposed. This way of defining data contracts is very similar to other programming languages such as C#.\nSupport for more complex data types such as collections and tables has been added so that these types can be serialized and deserialized without developers having to provide the logic themselves.\nIn a typical custom service you will find the following components:\n Service contract : A service contract is an X++ class that contains methods with the SysEntryPointAttribute attribute. This identifies methods that will result in a service operation contract when the service is exposed. Data contracts : A data contract is an X++ class that is attributed with the DataContractAttribute attribute. It contains parameter methods that will be attributed as data members for each member variable that needs to be part of the data contract.  Custom services can be deployed using the integration ports and any available adapter can be used.\nSystem services These services are new since the release of Microsoft Dynamics AX 2012. The main difference between these services and the previous two types is that they are not customizable and are not mapped to a query or X++ code. They are not customizable because they are written by Microsoft in managed code. One exception is the user session service, which is written in X++ code but is generally considered as a system service.\nThere are three system services available for use in Microsoft Dynamics AX 2012: the query service, the metadata service, and the user session service.\nQuery service The query service provides the means to run queries of the following three types:\n Static queries defined in the AOT. User-defined queries by using the QueryMetaData class in the service. Dynamic queries that are written in X++ classes. These classes need to extend the \u0026lsquo;AIFQueryBuilder\u0026rsquo; class.  When queries are called by a service, the AOS authorization ensures that the caller has the correct permissions to retrieve the information. This means that unpermitted fields will be omitted from the query result. Furthermore, when joined data sources are not allowed to be used, the query call will result in an error that can be caught by the calling application.\nThe resulting rows will be returned as an ADO.NET DataSet object. This can be very useful when you make use of controls in your application that can be bound to a DataSet object.\nThe query service can be found at the following address:\nnet.tcp://hostname:port/DynamicsAX/Services/QueryService\nMetadata service This system service can be used to retrieve metadata information about the AOT. Consumers of this service can get information such as which tables, classes, forms, and menu items are available in the system. An example usage of this service could be retrieving information about the AOT and using it in a dashboard application running on the Microsoft .NET Framework.\nThe metadata service can be found at the following address:\nnet.tcp://hostname:port/DynamicsAX/Services/MetaDataService\nUser session service The third system service is the user session service. With this service you can retrieve information about the caller\u0026rsquo;s user session. This information includes the user\u0026rsquo;s default company, language, preferred calendar, time zone, and currency.\nThe user session service can be found at the following address:\nnet.tcp://hostname:port/DynamicsAX/Services/UserSessionService\nThe right service for the right job Now that it is clear what types of services Microsoft Dynamics AX 2012 has to offer, the question arises as to when each type of service should be used. There is no simple answer for this due to the fact that every type has its strengths and weaknesses. Let us take a look at two factors that may help you make the right decision.\nComplexity Both document services and custom services can handle any business entity complexity. The document services framework parses the incoming XML and validates it against an XML Schema Definition(XSD) document. After validation, the framework calls the appropriate service action. Custom services on the other hand use the .NET XML Serializer and no validation of data is done. This means that any validations of the data in the data contract need to be written in code. Another advantage of document services over custom services is that the AxBC classes already contain a lot of the logic that is needed for CRUD operations.\nFlexibility Document services have service contracts that are tightly coupled with the AOT Query object. This means that when the query changes, the schema also changes. Data policies allow you to control which fields are exposed. When using custom services, this cannot be done by setup, but has to be done by attributing at design time.\nCustom services have the flexibility towards the service contract that the document services are lacking. Here the developer is in full control about what is in the contract and what is not. The operations, input parameters, and return types are all the responsibility of the developer.\nAnother benefit in using custom services is the ability to use shared data contracts as parameters for your operations. Think of a company-wide software solution that involves the use of Microsoft Dynamics AX 2012 together with SharePoint and .NET applications that are all linked through BizTalk. You could opt to share data contracts to make sure that entities are the same for all of the components in the architecture.\nIn that scenario, you\u0026rsquo;re able to create a data contract in managed code and reference it in Microsoft Dynamics AX 2012. Then you can use that .NET data contract in your service operations as a parameter.\nThere will probably be more factors that you will take into consideration to choose between the service types. But we can come to the following conclusion about when to use what type of service:\n Custom services : Custom services should be used when exposing entities that have a low complexity or data contracts that need to be shared between other applications.  They are also ideal when custom logic needs to be exposed that may have nothing to do with data structures within Microsoft Dynamics AX.\n Document services : Document services should be used when exposing entities that have a high complexity and when validation of the data and structure would require a lot of work for developers to implement on their own. Query service : The query service should be used when only read operations are needed and there is no need for updates, inserts, or delete actions.  It can be used when writing .NET Framework applications that leverage the data from Microsoft Dynamics AX returned as an ADO.NET DataSet.\n Metadata service : Use the metadata service when metadata information about objects in the AOT is required. User session service : The user session service should be used when user session-related information is required. ","permalink":"https://nuxulu.com/posts/2016-11-10-types-services-microsoft-dynamics-ax-2012/","summary":"\u003ch1 id=\"document-services\"\u003eDocument services\u003c/h1\u003e\n\u003cp\u003eDocument services use documents to represent business objects such as purchase and sales orders, customers, vendors, and so on.\u003c/p\u003e\n\u003cp\u003eA document service is composed of the following components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDocument query\u003c/strong\u003e : This is a query that is created in the \u003cstrong\u003eApplication Object Tree (AOT)\u003c/strong\u003e and contains all the tables that are related to the business object that you want to expose. Based on this query, the Document Service Generation Wizard can be used to generate the other artifacts that make up the document service.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTable AxBC classes\u003c/strong\u003e : An \u003ccode\u003eAxBC\u003c/code\u003e class is a wrapper for a table and contains business logic that is needed for \u003cstrong\u003eCreate, Read, Update, Delete (CRUD)\u003c/strong\u003e operations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDocument class\u003c/strong\u003e : The purpose of the document class is to contain business logic that is associated with the creation and modification of the business entity itself. For example, the \u003ccode\u003eAxdCustomer\u003c/code\u003e class could contain logic to handle party information of a customer.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDocument service class\u003c/strong\u003e : This is the actual service implementation class and extends the \u003ccode\u003eAifDocumentService\u003c/code\u003e class. This class implements the service operations that are published through the service contract.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhen creating document services, developers need to make sure \u003cstrong\u003ethat the business object is mapped correctly to the document query\u003c/strong\u003e. The document services framework will handle all other things such as the serialization and deserialization of XML, date effectiveness, and so on.\u003c/p\u003e\n\u003cp\u003eDocument services can be deployed using the integration ports and all available adapters can be used.\u003c/p\u003e","title":"Types of services in Microsoft Dynamics AX 2012"},{"content":"SUMMARY Tool can be used for two different purposes. One is to install demo data and other one is elaborated as below. Customers often need to have a production dataset to use when building and validating customizations in non-production environments.\nThe Microsoft Dynamics AX 2012 Test Data Transfer Tool (beta) is a tool that helps move data between from production to non-production environments or from non-production environments to production environments to make a new production environment. But you must be careful becasue the tool imports data table by table and deletes the data in the table before importing. Hence it is highly advised against running the tool for import in production environments.\nBENEFITS   Export and import data outside AX, without running an AOS instance.\n  Export and import processing are faster compared to other tools because this tool is based on SQL Server bcp.\n  The tool can work around the table/field metadata changes between builds and environments and hence can be used to move data from build to build, and environment to environment even when there are customizations, and metadata changes.\n  The tool minimally changes data during the import process. The only data the tool changes are the entity IDs (IDs related to table, field, etc.) that are stored as data and that could change with each deployment. The tool recognizes the changes and patches the data with the AXIDs of the system that the data is being imported into.\n  The data file format is the standard format produced by SQL Server bcp. The output is text based and can be stored and compared against other versions in a version control system.\n  How to  Download AX2012TestDataTransferTool.zip file from LCS  PROD Environment\n Run the setup file in SQL SERVER environment and complete the installation.\n  \u0026ldquo;C:\\Program Files (x86)\\Microsoft Dynamics AX 2012 Test Data Transfer Tool (Beta)\u0026quot; file appears automatically. Find the MetadataXMLGenerator.xpo file and import it into AX.\n  A job named MetadataXMLGenrator appears among AOT/jobs. Find and run the job.\n  Job generates a file named MetaData.Xml and gives you a file path via infolog.\n  Copy MetaData.Xml file and paste in \u0026ldquo;C:\\Program Files (x86)\\Microsoft Dynamics AX 2012 Test Data Transfer Tool (Beta)[Lists]\u0026quot; file in Prod(Golden) SQL SERVER. Overwrite the existing MetaData.Xml file.\n  DEV or TEST Environment\n Repeat the steps 2-6 for DEV environment.\n  The windows user who is going to execute the process should have access MicrosoftDynamicsAx ve Model database in DEV and Prod SQL servers. \u0026ldquo;Read\u0026rdquo; is enough for (Prod) exporting. \u0026ldquo;Full\u0026rdquo; right is enough for importing (DEV).\n  The windows user who is going to execute the process should have \u0026ldquo;full\u0026rdquo; access \u0026ldquo;C:\\Program Files (x86)\\Microsoft Dynamics AX 2012 Test Data Transfer Tool (Beta)\u0026quot; in both DEV and Prod SQL servers. System generates logs here.\n  Now, Live (Golden) environment\u0026rsquo;s data will be exported. Prepare a file to export live(Golden) data on Live SQL Server. For instance C:\\DC_EXPORT\n  Type the following command in command line and initiate the exportation process:\nDP.exe EXPORT Directory\u0026gt;\u0026gt; Database name\u0026gt;\u0026gt; Server\u0026gt;\u0026gt;\nDP.exe EXPORT C:\\DC_EXPORT \u0026quot;MicrosoftDynamicsAX\u0026quot; \u0026quot;ServerName\u0026quot;\n  Copy the exported file (C:\\DC_EXPORT) to DEV environment\n  Stop DEV AOS service\n  Type the following command in command line and initiate the importation process:\nDP.exe IMPORT Directory\u0026gt;\u0026gt; Database name\u0026gt;\u0026gt; Server\u0026gt;\u0026gt;\n  Start DEV AOS service.\n  Note: DP.exe EXPORT/IMPORT commands must be run from the related directories\n","permalink":"https://nuxulu.com/posts/2016-11-08-microsoft_dyanmics_ax_2012_test_data_transfer_tool/","summary":"\u003ch2 id=\"summary\"\u003eSUMMARY\u003c/h2\u003e\n\u003cp\u003eTool can be used for two different purposes. One is to install \u003cstrong\u003edemo data\u003c/strong\u003e and other one is elaborated as below. Customers often need to have a production dataset to use when building and validating customizations in \u003cstrong\u003enon-production\u003c/strong\u003e environments.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eThe Microsoft Dynamics AX 2012 Test Data Transfer Tool (beta)\u003c/code\u003e is a tool that helps move data between from production to non-production environments or from non-production environments to production environments to make a new production environment. But you must be careful becasue the tool imports data table by table and deletes the data in the table before importing. Hence it is highly advised against running the tool for import in production environments.\u003c/p\u003e","title":"Microsoft AX 2012 Test Data Transfer Tool"},{"content":"Dear all,\nWe use modifiedField() method to perform any actions after the field is modifed\nEx:\nCreate new table with 2 fields ItemId and ItemNameDisplay from Extended Data Types node in AOT then Override modifiedField() in Table\u0026rsquo;s method node, something likes:\npublic void modifiedField(FieldId _fieldId) { InventTable inventTable; super(_fieldId); switch (_fieldId) { case fieldNum(IBD_Invent,itemid): this.ItemNameDisplay = inventTable::find(this.itemid).NameAlias; break; } } the modifiedField() method is located on tables and it is called by a form (or a dataset) when any field is changed. It has field ID as a parameter and therefore it is very easy to write code reacting to a change of a particular field. It is so easy that the modifiedField() method is often used even in such cases, when it really shouldn’t be used.\nIf you want to get previous field value in the modifiedField() for comparative purpose, you can use this.orig() method:\npublic void modifiedField(fieldId _fieldId) { super(_fieldId); info(strfmt(\u0026#34;Field number %1 changed from %2 to %3\u0026#34;,_fieldId,this.orig().(_fieldId),this.(_fieldId))); } ","permalink":"https://nuxulu.com/posts/2016-11-08-understanding_modifedfield_table_method_in_dax_2012/","summary":"\u003cp\u003eDear all,\u003c/p\u003e\n\u003cp\u003eWe use \u003ccode\u003emodifiedField()\u003c/code\u003e method to perform any actions  after the field is modifed\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eEx:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eCreate new table with 2 fields \u003cstrong\u003eItemId\u003c/strong\u003e and \u003cstrong\u003eItemNameDisplay\u003c/strong\u003e  from Extended Data Types node in AOT then Override \u003ccode\u003emodifiedField()\u003c/code\u003e in Table\u0026rsquo;s method node, something likes:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003epublic \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emodifiedField\u003c/span\u003e(FieldId _fieldId)\n{\n    InventTable inventTable;\n    super(_fieldId);\n    \u003cspan style=\"color:#66d9ef\"\u003eswitch\u003c/span\u003e (_fieldId)\n    {\n        \u003cspan style=\"color:#66d9ef\"\u003ecase\u003c/span\u003e fieldNum(IBD_Invent,itemid)\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n            this.ItemNameDisplay \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e inventTable\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003efind(this.itemid).NameAlias;\n            \u003cspan style=\"color:#66d9ef\"\u003ebreak\u003c/span\u003e;\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Understanding modifedField() Table method in DAX 2012"},{"content":"Expressions are usually used for appearance of the data in a report, change properties of the fields, calculate some values and display them in a proper way, compares values between data of fields and then display them.\nTypes of Expressions We have 3 types:\n Globals Operators - Arithmetic, Comparison,Concatenation, Logical Common Functions - Text, Date \u0026amp; Time, Math, Inspection, Program Flow, Aggregate, Financial, Conversion, Miscellaneous  We can see each and every one very deataily\n1. Globals Global expressions executes/works in Page Header and Footer parts only.\nExecutionTime Shows date and time at when report executes\nPageNumber shows page number of each and every page but allowed only in page header and footer\nReportName displays name of the active report what name we have assigned to the active report\nUserId shows current user name like company/userID\nLanguage displays language like US-English…\n2. Operators Arithmetic ^ power of\n*multiplication\n/ divides two numbers and returns a floating point result\n: divides two numbers and returns a integer result\nMod divides two numbers and returns remainder only\nadds two numbers and concatenation for two strings\n- subtraction and indicates negative value for numeric values\nComparison Known operators : \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= \u0026lt;\u0026gt;\nLike compares two strings and return true if matched or else returns False.\nEx: =Fields!Title.Value Like Fields!LoginID.Value\nIs compare two object reference variables\nEx: =Fields!Title.Value Is Null\nConcatenation + and \u0026amp; symbols uses for concatenation\nLogical Known: And, Not, Or, Xor\nSELECT * FROM users where firstname = \u0026#39;Larry\u0026#39; XOR lastname = \u0026#39;Smith\u0026#39;` AndAlso First condition will check first and if it is true only, goes to next or else it won\u0026rsquo;t need to check. Because our execution time is saving in a logical operation in which more conditions is combined using AndAlso function.\nOrElse same like above\n3. Common Functions Text Asc, AscW returns an integer value represents character code corresponding to a character.\nChr, chrw returns the character associated with the specified character code\nFilter =Filter(Fields!Title.Value,\u0026ldquo;Pr\u0026rdquo;,true,0)\nFormat=Format(Fields!Price.Value, \u0026ldquo;#,##0.00\u0026rdquo;), Format(Fields!Date.Value, \u0026ldquo;yyyy-MM-dd\u0026rdquo;)\nFormatCurrency =formatcurrency(Fields!SickLeaveHours.Value,3)\nFormatDateTime =FormatDateTime(Fields!BirthDate.Value,Integer)\nEx:\n0 returns 6/3/1977\n1 returns Friday, June 03, 1977\n2 returns 6/3/1977\n3 returns 12:00:00AM\n4 returns 00:00\nFormatNumber =FormatNumber(Fields!EmployeeID.Value,2), then result is 2.00\nFormatPercent =\u0026ldquo;Percentage : \u0026quot; \u0026amp; formatpercent(Fields!SickLeaveHours.Value)\nGetChar =GetChar(Fields!Title.Value,5)\nInStr =InStr(Fields!Title.Value,\u0026ldquo;a\u0026rdquo;)\nInStrRev =Instrrev(Fields!Title.Value,\u0026ldquo;a\u0026rdquo;)\nLCase =Lcase(Fields!Title.Value), Change strings into lower case\nLeft =Left(Fields!Title.Value,4), Returns left side characters from a string\nLen =Len(Fields!Title.Value), Finds length of a string\nLSet =Lset(Fields!Title.Value,5), Returns some length of a string from left\nLTrim =Ltrim(\u0026rdquo; \u0026ldquo;\u0026amp;Fields!Title.Value), Trim left side of a string\nMid =Mid(Fields!Title.Value,InSTrRev(Fields!Title.Value,\u0026ldquo;T\u0026rdquo;)), Returns characters from the mentioned starting position\nReplace =Replace(Fields!Title.Value,\u0026ldquo;a\u0026rdquo;,\u0026ldquo;A\u0026rdquo;), Replaces one string with another\nRight =Right(Fields!Title.Value,10), Returns right side characters from a string\nRSet =Rset(Fields!Title.Value,5),Returns some length of a string from left\nRTrim =Rtrim(Fields!Title.Value \u0026amp; \u0026quot; \u0026ldquo;), Trim left side of a string\nSpace =Fields!Title.Value \u0026amp; Space(5) \u0026amp; Fields!Title.Value, Specifies some spaces within strings\nStrComp Returns a value indicating the result of a string comparison\nvbBinaryCompare 0 Perform a binary comparison.\nvbTextCompare 1 Perform a textual comparison.\nstring1 is less than string2 -1\nstring1 is equal to string2 0\nstring1 is greater than string2 1\nstring1 or string2 is Null Null\nStrConv\n=Strconv(Fields!Title.Value,vbProperCase)\n=Strconv(Fields!Title.Value,vbLowerCase)\n=Strconv(Fields!Title.Value,vbUpperCase)\nStrDup =StrDup(3,\u0026ldquo;M\u0026rdquo;), Returns a string or object consisting of the specified character repeated the specified number of times.\nStrReverse =StrReverse(Fields!Title.Value)\nTrim =Trim(\u0026rdquo; \u0026ldquo;\u0026amp; Fields!Title.Value \u0026amp; \u0026quot; \u0026ldquo;)\nUCase =Ucase(Fields!Title.Value)\nDate \u0026amp; Time CDate Converts a object into date format\n=Format(CDate(Fields!BirthDate.Value),\u0026ldquo;MMMM yyyy\u0026rdquo;)\nDateAdd Returns a datetime that is the result of adding the specified number of time interval units to the original datetime.\n=dateadd(\u0026ldquo;m\u0026rdquo;,12,Fields!BirthDate.Value)\nDateDiff Find number of days, months and years between two dates\n=datediff(\u0026ldquo;d\u0026rdquo;,Fields!BirthDate.Value,Now)\nDatePart DatePart(DateInterval.Weekday, CDate(\u0026ldquo;2009/11/13\u0026rdquo;), FirstDayOfWeek.Monday) returns 5 (Friday)\nDateSerial for first day of the month\n=DateSerial(Year(Now), Month(Now), 1)\nfor the last day of the month\n=DateSerial(Year(Now), Month(Now)+1, 0) DateString Returns string value of system date\n=datestring()\nDateValue Returns current date\nDay Returns day value from date\n=day(Fields!BirthDate.Value)\nFormatDateTime =FormatDateTime(Fields!BirthDate.Value,Integer) Examples: 0 returns 6/3/1977\n1 returns Friday, June 03, 1977\n2 returns 6/3/1977\n3 returns 12:00:00AM\n4 returns 00:00\nHour =Hour(Fields!BirthDate.Value)\nMinute =Minute(Fields!BirthDate.Value)\nMonth =Month(Fields!BirthDate.Value)\nMonthName =MonthName(Month(Fields!BirthDate.Value))\nNow Indicates current month\n=Now() or =Now\nSecond =Second(Fields!BirthDate.Value)\nTimeOfDay =TimeOfDay()\nReturns a date value containing the current time of day according to your system\nTimer =Timer() Returns number of seconds elapsed since midnight\nTimeSerial =TimeSerial(24,60,60)\nReturns a date value representing a specified hour, minute and second\nTimeString =TimeString()\nReturns string value representing the current time of day according to your system\nTimeValue Returns a date value set to jan 1 of year 1\n=TimeValue(Fields!BirthDate.Value)\nToday Returns Current date\nWeekday Returns an integer value representing day of week\n=WeekDay(Fields!BirthDate.Value)\nWeekdayName =WeekdayName(Weekday(Fields!BirthDate.Value))\nReturns name of the day of week\nYear =year(Fields!BirthDate.Value)\nReturns year of specified date\nMath Abs=Abs(-2.36)\nReturns the absolute value\nBigMul =BigMul(2,3)\nReturns multiplication value of two specified numbers\nCeiling =Ceiling(2.67)\nReturns next highest value\nCos=Cos(2.33)\nReturns cos value for specified number\nCosh=Cosh(2.33) Returns hyperbolic cos value\nDivRem=DivRem(23,2,5)\nFix=Fix(23.89)\nReturns integer portion\nFloor=Floor(24.54)\nReturns largest integer\nInt=Int(24.78)\nReturns integer portion of a number\nLog=Log(24.78)\nReturns logarithm value\nLog10=Log10(24.78)\nReturns the base 10 logaritm value\nMax=Max(Fields!EmployeeID.Value)\nReturns larger value in the specified values\nMin=Min(Fields!EmployeeID.Value)\nReturns smaller value in the specified values\nPow=Pow(Fields!EmployeeID.Value,2)\nReturns power of value for specified number\nRnd=Rnd()\nReturns a random number\nRound=Round(43.16)\nReturns rounded value to the nearest integer\nSign=Sign(-34534543)\nSin=Sin(Fields!EmployeeID.Value) Returns the sin value\nSinh=Sinh(Fields!EmployeeID.Value) Returns the hyperbolic sin value\nInspection IsArray\r=IsArray(Fields!EmployeeID.Value)\nReturns a boolean value indicating whether the specified object is array or not\nIsDate\r=IsDate(Fields!BirthDate.Value)\nReturns a boolean value indicating whether the specified object is Date or not\nIsNothing\r=IsNothing(Fields!EmployeeID.Value)\nReturns a boolean value depends on specified object is Nothing or not\nIsNumeric\r=IsNumeric(Fields!EmployeeID.Value)\nReturns a boolean value depends on specified object is Numeric value or not\nProgram Flow Choose\r=CHOOSE(3, \u0026quot;Red\u0026quot;, \u0026quot;Yellow\u0026quot;, \u0026quot;Green\u0026quot;, \u0026quot;White\u0026quot;)\nReturns a specific value using index in a list of arguments\nIIf\r=IIF(Fields!EmployeeID.Value\u0026gt;10,\u0026quot;Yes\u0026quot;,\u0026quot;No\u0026quot;)\nReturns any one value depends on condition\nSwitch\r=Switch(Fields!EmployeeID.Value\u0026lt;10,\u0026quot;Red\u0026quot;,Fields!EmployeeID.Value\u0026gt;10,\u0026quot;Green\u0026quot;)\nEvaluates list of expressions\nAggregate Avg\r=Avg(Fields!EmployeeID.Value)\nReturns average value for all specified values\nCount\r=Count(Fields!EmployeeID.Value)\nReturns count of all specified values\nCountDistinct\r=CountDistinct(Fields!EmployeeID.Value)\nReturns count of all distinct values\nCountRows\r=CountRows()\nReturns count of rows\nFirst\r=First(Fields!EmployeeID.Value)\nReturns first for all specified values\nLast\r=Last(Fields!EmployeeID.Value)\nReturns last for all specified values\nMax\r=Max(Fields!EmployeeID.Value)\nReturns max for all specified values\nMin\r=Min(Fields!EmployeeID.Value)\nReturns min for all specified values\nStDev\r=StDev(Fields!EmployeeID.Value)\nReturns standard deviation value\nStDevP\r=StDevP(Fields!EmployeeID.Value)\nReturns Population standard deviation value\nSum\r=Sum(Fields!EmployeeID.Value)\nReturns sum of all values\nVar\r=Var(Fields!EmployeeID.Value)\nReturns variance of all values\nVarP\r=Var(Fields!EmployeeID.Value)\nReturns population variance of all values\nRunningValue\r=RunningValue(Fields!EmployeeID.Value,sum,nothing)\nReturns running aggregate of the specified expression\nFinancial DDB DDB (Double Declining Balance) method computes depreciation of an asset for a specified period.\nSyntax: DDB (Cost, Salvage, life, period, factor)\nFV FV (Future Value) of an investment based on periodic, constant payments and a constant interest rate.\nSyntax: FV (rate, nper, pmt, pv, type)\nIPmt IPmt (Interest Payment) for a given period for an investment based on periodic, constant payment and a constant interest rate\nIPMT (rate, per, nper, pv, fv, type)\nIRR IRR (Interest Rate of Return) for a series of cash flows represented by the numbers in values.\nIRR(values,guess)\nMIRR MIRR ( Modified internal rate of return ) for a series of periodic cash flows\nMIRR(values,finance_rate,reinvest_rate)\nNPer Returns the number of periods for an investment based on periodic, constant payments and a constant interest rate.\nNPER (rate, pmt, pv, fv, type)\nNPV Calculates the net present value of an investment by using a discount rate and a series of future payments (negative values) and income (positive values).\nSyntax: NPV(rate,value1,value2, ...)\nPmt Calculates the payment for a loan based on constant payments and a constant interest rate.\nPMT(rate,nper,pv,fv,type)\nPPmt Returns the payment on the principal for a given period for an investment based on periodic, constant payments and a constant interest rate.\nPPMT(rate,per,nper,pv,fv,type)\nPV Returns the present value of an investment. The present value is the total amount that a series of future payments is worth now. For example, when you borrow money, the loan amount is the present value to the lender.\nPV(rate,nper,pmt,fv,type)\nRate Returns the interest rate per period of an annuity. RATE is calculated by iteration and can have zero or more solutions.\nRATE(nper,pmt,pv,fv,type,guess)\nSLN Returns the straight-line depreciation of an asset for one period.\nSLN(cost,salvage,life)\nSYD Returns the sum-of-years' digits depreciation of an asset for a specified period. SYD(cost,salvage,life,per)\nConversion CBool Convert to boolean\nCByte Convert to byte\nCChar Convert to char\nCDate Convert to date\nCDbl Convert to double\nCDec Convert to decimal\nCInt Convert to integer\nCLng Convert to long\nCObj Convert to object\nCShort Convert to short\nCSng Convert to single\nCStr Convert to string\nHex =Hex(Fields!EmployeeID.Value)\nReturns a hexadecimal value of a number\nInt =Int(43.44)\nReturns integer portion of a number\nOct =Oct(Fields!EmployeeID.Value)\nReturns a octal value of a number\nStr =Str(Fields!EmployeeID.Value)\nReturns string value of a number\nVal =Val(\u0026quot;32.43\u0026quot;)\nReturns numeric value in string format\nMiscellaneous Previous =Previous(Fields!EmployeeID.Value)\nReturns the previous value\n","permalink":"https://nuxulu.com/posts/2016-10-07-ssrs-sql-server-reporting-services-expressions-or-functions-used-in-ssrs/","summary":"\u003cp\u003eExpressions are usually used for appearance of the data in a report, change properties of the fields, calculate some values and display them in a proper way, compares values between data of fields and then display them.\u003c/p\u003e\n\u003ch1 id=\"types-of-expressions\"\u003eTypes of Expressions\u003c/h1\u003e\n\u003cp\u003eWe have 3 types:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGlobals\u003c/li\u003e\n\u003cli\u003eOperators - Arithmetic, Comparison,Concatenation, Logical\u003c/li\u003e\n\u003cli\u003eCommon Functions - Text, Date \u0026amp; Time, Math, Inspection, Program Flow, Aggregate, Financial, Conversion, Miscellaneous\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe can see each and every one very deataily\u003c/p\u003e","title":"Expressions or Functions used in SSRS"},{"content":"","permalink":"https://nuxulu.com/posts/2016-09-06-evolution-of-programming-languages/","summary":"","title":"Evolution of programming languages"},{"content":"Sometime we need this RecId value in DimansionAttributeValueCombination Table for some reasons likes putting new dimension in LedgerDimension field in LedgerJournalTrans Table, or somewhere else.\nSuppose my Account structure likes: MainAcct - Dept - Woker - SubAcct - Item, you can custom base on your struture.\nThis code is for creating a record into DimansionAttributeValueCombination in code and then get RecID of this.\nstatic void createDimensionAttributeValueCombination(Args _args) { DimensionAttributeValueContract ValueContract; DimensionAttributeValueCombination davc; MainAccount valueMainAccount; DimensionStorage dimStorage; DimensionServiceProvider DimensionServiceProvider = new DimensionServiceProvider(); LedgerAccountContract LedgerAccountContract = new LedgerAccountContract(); List ListValueContract = new List(Types::Class); // Get main account with structure : Mainacct - DE - woker - subacct - item  valueMainAccount = MainAccount::findByMainAccountId(\u0026#39;910001\u0026#39;); //Get main account  ValueContract = new DimensionAttributeValueContract(); ValueContract.parmName(\u0026#39;Department\u0026#39;) ; ValueContract.parmValue(\u0026#39;6020\u0026#39;); //Value for dimension Department  ListValueContract.addEnd(ValueContract); ValueContract = new DimensionAttributeValueContract(); ValueContract.parmName(\u0026#39;Worker\u0026#39;) ; ValueContract.parmValue(\u0026#39;000001\u0026#39;); //Value for dimension ExpenseCode  ListValueContract.addEnd(ValueContract); ValueContract = new DimensionAttributeValueContract(); ValueContract.parmName(\u0026#39;SubAcct\u0026#39;) ; ValueContract.parmValue(\u0026#39;교보생명\u0026#39;); //Value for dimension Project  ListValueContract.addEnd(ValueContract); ValueContract = new DimensionAttributeValueContract(); ValueContract.parmName(\u0026#39;Item\u0026#39;) ; ValueContract.parmValue(\u0026#39;110329\u0026#39;); //Value for dimension Project  ListValueContract.addEnd(ValueContract); LedgerAccountContract.parmMainAccount(valueMainAccount.MainAccountId); LedgerAccountContract.parmValues(ListValueContract); //if combination is not exist then create new one  dimStorage = DimensionServiceProvider::buildDimensionStorageForLedgerAccount(LedgerAccountContract); davc = DimensionAttributeValueCombination::find(dimStorage.save()); info(strFmt(\u0026#34;RecId : %1 - DisplayValue : %2\u0026#34;, davc.RecId, davc.DisplayValue)); } or you can use this way\nstatic void createDimension(Args _args) { Struct struct = new Struct(); container financialDimension; LedgerDimensionAccount legder; //// Get main account with structure : Mainacct - DE - woker - subacct - item  struct.add(\u0026#39;Department\u0026#39;, \u0026#39;6020\u0026#39;); struct.add(\u0026#39;Item\u0026#39;, \u0026#39;220006\u0026#39;); struct.add(\u0026#39;SubAcct\u0026#39;, \u0026#39;Max test account\u0026#39;); struct.add(\u0026#39;Worker\u0026#39;, \u0026#39;000002\u0026#39;); financialDimension += struct.fields(); financialDimension += struct.fieldName(1); financialDimension += struct.valueIndex(1); financialDimension += struct.fieldName(2); financialDimension += struct.valueIndex(2); financialDimension += struct.fieldName(3); financialDimension += struct.valueIndex(3); financialDimension += struct.fieldName(4); financialDimension += struct.valueIndex(4); legder = DimensionDefaultingService::serviceCreateLedgerDimension( DimensionStorage::getDefaultAccountForMainAccountNum(\u0026#34;910001\u0026#34;), AxdDimensionUtil::getDimensionAttributeValueSetId(financialDimension)); info(strFmt(\u0026#34;%1\u0026#34;,legder)); } ","permalink":"https://nuxulu.com/posts/2016-07-05-create-record-in-dimensionattributevaluecombination-manually/","summary":"\u003cp\u003eSometime we need this \u003ccode\u003eRecId\u003c/code\u003e value in \u003ccode\u003eDimansionAttributeValueCombination\u003c/code\u003e Table for some reasons likes putting new dimension in \u003ccode\u003eLedgerDimension\u003c/code\u003e field in \u003ccode\u003eLedgerJournalTrans\u003c/code\u003e Table, or somewhere else.\u003c/p\u003e\n\u003cp\u003eSuppose my Account structure likes: \u003cem\u003e\u003cstrong\u003eMainAcct - Dept - Woker - SubAcct - Item\u003c/strong\u003e\u003c/em\u003e,\nyou can custom base on your struture.\u003c/p\u003e\n\u003cp\u003eThis code is for creating a record into \u003ccode\u003eDimansionAttributeValueCombination\u003c/code\u003e in code and then get \u003ccode\u003eRecID\u003c/code\u003e of this.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-c++\" data-lang=\"c++\"\u003e\u003cspan style=\"color:#66d9ef\"\u003estatic\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecreateDimensionAttributeValueCombination\u003c/span\u003e(Args _args)\n{\n    DimensionAttributeValueContract     ValueContract;\n    DimensionAttributeValueCombination  davc;\n    MainAccount                         valueMainAccount;\n    DimensionStorage                    dimStorage;\n\n    DimensionServiceProvider DimensionServiceProvider   \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e DimensionServiceProvider();\n    LedgerAccountContract LedgerAccountContract         \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e LedgerAccountContract();\n    List ListValueContract                              \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e List(Types\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eClass);\n    \n    \u003cspan style=\"color:#75715e\"\u003e// Get main account with structure : Mainacct - DE - woker - subacct - item\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    valueMainAccount \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e MainAccount\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003efindByMainAccountId(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e910001\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e); \u003cspan style=\"color:#75715e\"\u003e//Get main account\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\n    ValueContract \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e DimensionAttributeValueContract();\n    ValueContract.parmName(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003eDepartment\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e) ;\n    ValueContract.parmValue(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e6020\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e); \u003cspan style=\"color:#75715e\"\u003e//Value for dimension Department\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    ListValueContract.addEnd(ValueContract);\n\n    ValueContract \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e DimensionAttributeValueContract();\n    ValueContract.parmName(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003eWorker\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e) ;\n    ValueContract.parmValue(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e000001\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e); \u003cspan style=\"color:#75715e\"\u003e//Value for dimension ExpenseCode\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    ListValueContract.addEnd(ValueContract);\n\n    ValueContract \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e DimensionAttributeValueContract();\n    ValueContract.parmName(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003eSubAcct\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e) ;\n    ValueContract.parmValue(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;교보생명\u0026#39;\u003c/span\u003e); \u003cspan style=\"color:#75715e\"\u003e//Value for dimension Project\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    ListValueContract.addEnd(ValueContract);\n\n    ValueContract \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e DimensionAttributeValueContract();\n    ValueContract.parmName(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003eItem\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e) ;\n    ValueContract.parmValue(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e110329\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e\u0026#39;\u003c/span\u003e); \u003cspan style=\"color:#75715e\"\u003e//Value for dimension Project\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    ListValueContract.addEnd(ValueContract);\n\n    LedgerAccountContract.parmMainAccount(valueMainAccount.MainAccountId);\n    LedgerAccountContract.parmValues(ListValueContract);\n\n    \u003cspan style=\"color:#75715e\"\u003e//if combination is not exist then create new one\n\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    dimStorage \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e DimensionServiceProvider\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ebuildDimensionStorageForLedgerAccount(LedgerAccountContract);\n    davc \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e DimensionAttributeValueCombination\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003efind(dimStorage.save());\n    info(strFmt(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;RecId : %1 - DisplayValue : %2\u0026#34;\u003c/span\u003e, davc.RecId, davc.DisplayValue));\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Create record in DimensionAttributeValueCombination table manually."},{"content":"If you want to list items come along with their Actvie Dimension, you can do it easily with this simple job\nInventTable inventTable; InventDimParm inventDimParm; ; inventTable = InventTable::find(\u0026#39;110924\u0026#39;); inventDimParm = InventDimParm::activeDimFlag(InventDimGroupSetup::newInventTable(inventTable)); if(inventDimParm.InventSiteIdFlag) { info(\u0026#34;Site Actived\u0026#34;); } ","permalink":"https://nuxulu.com/posts/2016-01-17-how-to-verify-active-inventory-dimensions-on-an-item/","summary":"If you want to list items come along with their Actvie Dimension, you can do it easily with this simple job\nInventTable inventTable; InventDimParm inventDimParm; ; inventTable = InventTable::find(\u0026#39;110924\u0026#39;); inventDimParm = InventDimParm::activeDimFlag(InventDimGroupSetup::newInventTable(inventTable)); if(inventDimParm.InventSiteIdFlag) { info(\u0026#34;Site Actived\u0026#34;); } ","title":"How to verify active inventory dimensions on an item"},{"content":"I updated my dynamics Ax development Environment by restoring Database from Production Database . After the restoring the database, when I run any report form Dynamics Ax, all gave error “Only integrated security is supported for AX queries.”\nThe reporting Services are still working fine.\nFor this, best way to redeploy them, create a new report folder, open Microsoft Dynamics Ax 2012 Management Shell (make sure you run Powershell as Administrator).\nPublish-AXReport -ReportName *\n","permalink":"https://nuxulu.com/posts/2016-01-15-only-integrated-security-is-supported-for-ax-queries/","summary":"I updated my dynamics Ax development Environment by restoring Database from Production Database . After the restoring the database, when I run any report form Dynamics Ax, all gave error “Only integrated security is supported for AX queries.”\nThe reporting Services are still working fine.\nFor this, best way to redeploy them, create a new report folder, open Microsoft Dynamics Ax 2012 Management Shell (make sure you run Powershell as Administrator).","title":"Only integrated security is supported for AX queries"},{"content":"Recently I updated my dynamics Ax development Environment by restoring Database from Production Dynamics AX DB and I got this problem\nMake sure that SQL Server Reporting Services is configured correctly. Verify the Web Service URL and Report Manager URL configuration in the SQL Reporting Services Configuration Manager.\nAnyway, The reporting Services are still working fine and i have already granted the AX Admin as System Administrator under site settings, Home folder settings and DynamicsAX folder with “Browser, Content Manager, DynamicsAXBrowser, My Reports, Publisher, Report Builder” roles.\nSolution is we need disable UAC (I\u0026rsquo;m using Windows Server 2012 R2) by Regedit\ngo to Regedit: “HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsCurrentVersionpoliciessystem” and changing the DWORD “EnableLUA” from 1 to 0.\nAfter the reboot, UAC is disabled.\nHappy AXsing\n","permalink":"https://nuxulu.com/posts/2016-01-15-ssrs-report-server-settings-validation-error-ax-2012-r3-cu9/","summary":"Recently I updated my dynamics Ax development Environment by restoring Database from Production Dynamics AX DB and I got this problem\nMake sure that SQL Server Reporting Services is configured correctly. Verify the Web Service URL and Report Manager URL configuration in the SQL Reporting Services Configuration Manager.\nAnyway, The reporting Services are still working fine and i have already granted the AX Admin as System Administrator under site settings, Home folder settings and DynamicsAX folder with “Browser, Content Manager, DynamicsAXBrowser, My Reports, Publisher, Report Builder” roles.","title":"SSRS Report Server Settings Validation Error- AX 2012 R3 CU9"},{"content":"Link Type: Active: Parent and child data source is updated immediately when a new record in the parent data source is selected. Continuous updates consume lots of resources consuming.\nDelayed: Parent and child A pause is inserted before linked child data sources are updated. This enables faster navigation in the parent data source because the records from child data sources are not updated immediately.\nFor example, you can scroll a list of orders where you do not want to review the lines associated with the order until you stop scrolling.\nPassive: Parent and child Linked child data sources are not updated automatically. Updates of the child data source must be programmed on the active() method of the master data source.\nJoin Types: InnerJoin Combined data source select the record from the main table that matches records in the joined table and vice versa.\n//X++ select AccountNum from custTable join TaxGroupId from custGroup where custGroup.CustGroup == custTable.CustGroup; //CROSS JOIN in T-SQL: SELECT T1.ACCOUNTNUM, T1.RECID, T2.TAXGROUPID, T2.RECID FROM CUSTTABLE T1 CROSS JOIN CUSTGROUP T2 WHERE ((T1.PARTITION=?) AND (T1.DATAAREAID=?)) AND (((T2.PARTITION=?) AND (T2.DATAAREAID=?)) AND (T2.CUSTGROUP=T1.CUSTGROUP)) There is one record for each match. Records without related records in the other data source are eliminated from the result.\nOuter Join: Combined data source select the records from the main table. The records are retrieved whether they have matching records in the joined table\n//X++: select AccountNum from custTable outer join AccountID from custBankAccount where custBankAccount.CustAccount == custTable.AccountNum; //LEFT OUTER JOIN in T-SQL: SELECT T1.ACCOUNTNUM, T1.RECID, T2.ACCOUNTID, T2.RECID FROM CUSTTABLE T1 LEFT OUTER JOIN CUSTBANKACCOUNT T2 ON (((T2.PARTITION=?) AND (T2.DATAAREAID=?)) AND (T1.ACCOUNTNUM=T2.CUSTACCOUNT)) WHERE ((T1.PARTITION=?) AND (T1.DATAAREAID=?)) Exist Join: Combined data sourceThe data source retrieves a record from the main table for each matching record in the joined table.\n//X++: select AccountNum from custBankAccount exists join custTable where custBankAccount.CustAccount == custTable.AccountNum; //EXISTS (SELECT \u0026#39;x\u0026#39;...) in T-SQL: SELECT T1.ACCOUNTNUM, T1.RECID FROM CUSTBANKACCOUNT T1 WHERE ((T1.PARTITION=?) AND (T1.DATAAREAID=?)) AND EXISTS (SELECT \u0026#39;x\u0026#39; FROM CUSTTABLE T2 WHERE (((T2.PARTITION=?) AND (T2.DATAAREAID=?)) AND (T1.CUSTACCOUNT=T2.ACCOUNTNUM))) The differences between InnerJoin and ExistJoin are as follows:\n  When the join type is ExistJoin, the search ends after the first match has been found.\n  When the join type is InnerJoin, all matching records are searched for.\n  NotExistJoin: Combined data source Select records from the main table that do not have a match in the joined table.\n ","permalink":"https://nuxulu.com/posts/2016-01-07-link-type-and-join-types-in-ax-2012/","summary":"\u003ch1 id=\"link-type\"\u003eLink Type:\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eActive\u003c/strong\u003e: \n\u003ccode\u003eParent and child\u003c/code\u003e data source is updated immediately when a new record in the parent data source is selected. Continuous updates consume lots of resources consuming.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDelayed\u003c/strong\u003e:\n\u003ccode\u003eParent and child\u003c/code\u003e A pause is inserted before linked child data sources are updated. This enables faster navigation in the parent data source because the records from child data sources are not updated immediately.\u003c/p\u003e\n\u003cp\u003eFor example, you can scroll a list of orders where you do not want to review the lines associated with the order until you stop scrolling.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePassive\u003c/strong\u003e: \u003ccode\u003eParent and child\u003c/code\u003e Linked child data sources are not updated automatically. Updates of the child data source must be programmed on the \u003ccode\u003eactive()\u003c/code\u003e method of the master data source.\u003c/p\u003e\n\u003ch1 id=\"join-types\"\u003eJoin Types:\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eInnerJoin\u003c/strong\u003e \u003ccode\u003eCombined data source\u003c/code\u003e select the record from the main table that matches records in the joined table and vice versa.\u003c/p\u003e","title":"Link Type and Join Types in ax 2012"},{"content":"This blog post is show how to apply OR conditions in query build ranges in a simple way on same field in a table. Let\u0026rsquo;s see the simple query :\nselect * from CustTable where AccountNum == \u0026#39;2001\u0026#39; || AccountNum == \u0026#39;2002\u0026#39; We can find out solutions on MSDN by using expression in query ranges, but as it has lot of specifications which needs to be followed. However there is a simple way to do it :\nstatic void Job12(Args _args) { CustTable cust; Query query = new Query(); QueryBuildDataSource qbds; QueryBuildRange queryRange1, queryRange2; ; qbds = query.addDataSource(tableNum(CustTable)); queryRange1 = qbds.addRange(fieldNum(CustTable, AccountNum)); queryRange1.value(\u0026#39;1168201\u0026#39;); queryRange2 = qbds.addRange(fieldNum(CustTable, AccountNum)); queryRange2.value(\u0026#39;9034518\u0026#39;); info(qbds.toString()); } Result in string format as below image :\nSELECT * FROM CustTable(CustTable_1) WHERE ((AccountNum = N\u0026#39;1168201\u0026#39;) OR (AccountNum = N\u0026#39;9034518\u0026#39;)) Thanks for reading :). Happy New Year.\n","permalink":"https://nuxulu.com/posts/2015-12-30-how-to-use-operator-in-querybuildrange/","summary":"This blog post is show how to apply OR conditions in query build ranges in a simple way on same field in a table. Let\u0026rsquo;s see the simple query :\nselect * from CustTable where AccountNum == \u0026#39;2001\u0026#39; || AccountNum == \u0026#39;2002\u0026#39; We can find out solutions on MSDN by using expression in query ranges, but as it has lot of specifications which needs to be followed. However there is a simple way to do it :","title":"How to use operator \"or\" in QueryBuildRange"},{"content":"Scenario: I have 2 AOS AX (maybe same in one server or different servers), but only one server for reporting server (I will install and configure multiple SRS instances in this server). Thing is how can we install and configure 2 SSRS instances on same server and running for 2 AOS.\neasierly, please take a look the picture below\nFigure 1: Two SSRS instance are running same server.\nProblems: First you need to install 2 SSRS instance in same server, and install reporting service component for first AOS, this step is very simple. The problems come when we install second reporting service component for second AOS.\nReporting service component uses business connector to connect to AX, and it\u0026rsquo;s saved on configuration in Registry. When we install second reporting service component it will through message that component already installed.\nHow to do:  After installed second instance into C:\\Program Files\\Microsoft SQL Server\\MSRS12.InstanceName\\Reporting Services\\ReportServer\\bin, we need to create one Dynamics.AX.ReportConfiguration.axc file by AX configuration client with second AOS information.  open MS Dyanmics AX Management Shell, run command below:  Install-AXReportInstanceExtensions –ReportServerInstanceName [SSRSInstanceName] -Credential [DomainNameUserName]\ngoto C:\\Program Files\\Microsoft SQL Server\\MSRS12.SecondInstance\\Reporting Services\\ReportServer fix fileconfi with value from Execution to FullTrust  After this, remember restart reporting services.\nFrom now on, you can run 2 AX reporting instance in same server.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2015-12-16-how-to-install-2-instances-ssrs-on-1-server/","summary":"\u003ch2 id=\"scenario\"\u003eScenario:\u003c/h2\u003e\n\u003cp\u003eI have 2 AOS AX (maybe same in one server or different servers), but only one server for reporting server (I will install and configure multiple SRS instances in this server). Thing is how can we install and configure 2 SSRS instances on same server and running for 2 AOS.\u003c/p\u003e\n\u003cp\u003eeasierly, please take a look the picture below\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/two_instances_SSRS_on_one_server_architechture.jpg\" alt=\"two_instances_SSRS_on_one_server_architechture\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eFigure 1\u003c/strong\u003e: Two SSRS instance are running same server.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"problems\"\u003eProblems:\u003c/h2\u003e\n\u003cp\u003eFirst you need to install 2 SSRS instance in same server, and install \u003ccode\u003ereporting service component\u003c/code\u003e for first AOS, this step is very simple. The problems come when we install second \u003ccode\u003ereporting service component\u003c/code\u003e for second AOS.\u003c/p\u003e\n\u003cp\u003eReporting service component uses \u003cstrong\u003ebusiness connector\u003c/strong\u003e to connect to AX, and it\u0026rsquo;s saved on configuration in Registry. When we install second \u003ccode\u003ereporting service component\u003c/code\u003e it will through message that component already installed.\u003c/p\u003e","title":"How to install two instances SSRS on one server"},{"content":"When you try connecting between TFS and Dynamics AX CU8, CU9 and you got error like this:\nError 1 Team Foundation services are not available from server ****.visualstudio.com/defaultcollection.\nTechnical information (for administrator):\nTF400813: Resource not available for anonymous access. Client authentication required.\nError 2\nCould not load file or assembly \u0026lsquo;Microsoft.TeamFoundation.Client, Version=10.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a\u0026rsquo; or one of its dependencies. The system cannot find the file specified.\nClient computers that are not running Visual Studio 2010 must have the Team Foundation Server 2010 SP1 Object Model installed to use TFS source control with Microsoft Dynamics AX.\nTFS support TFS 2010, TFS 2012, TFS 2013 and TFS online, but to connect to these from AX you will need “TFS 2010 object model Sp1” which is client to connect to server.\nAfter install Team Foundation Server 2010 SP1 Object Model above. Client computers that are not running Visual Studio 2010 must have hotfix KB 2662296 installed to use TFS source control with Microsoft Dynamics AX. Once you have installed the above , restart your AOS and open AX client again, then try to check-in some Jobs.\n","permalink":"https://nuxulu.com/posts/2015-11-20-ax-2012-r3-cu8-cu9-and-tfs-online-2013-error/","summary":"When you try connecting between TFS and Dynamics AX CU8, CU9 and you got error like this:\nError 1 Team Foundation services are not available from server ****.visualstudio.com/defaultcollection.\nTechnical information (for administrator):\nTF400813: Resource not available for anonymous access. Client authentication required.\nError 2\nCould not load file or assembly \u0026lsquo;Microsoft.TeamFoundation.Client, Version=10.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a\u0026rsquo; or one of its dependencies. The system cannot find the file specified.\nClient computers that are not running Visual Studio 2010 must have the Team Foundation Server 2010 SP1 Object Model installed to use TFS source control with Microsoft Dynamics AX.","title":"AX 2012 R3 CU8 CU9 and TFS Online 2013 Error"},{"content":"Scenario:\nNormally when we open form for the first time in Dynamics AX, it will take sometimes to compilte and cache into AOS. That\u0026rsquo;s why at second time always faster. If we often restart AOS, how can we keep performance ?\nSolution:\nAfter restart AOS we can let some scripts run to open up some often used forms then cache to AOS, then close those forms.\nHere is the code\nstatic void WarmupRF(Args _args) { UtilElements e; TreeNode treeNode; FormRun formRun; Args args = new Args(); while select e where e.utilLevel == UtilEntryLevel::var //\u0026lt;-- specify layer here  \u0026amp;\u0026amp; e.recordType == UtilElementType::Form //\u0026lt;-- and only forms  \u0026amp;\u0026amp; e.name like \u0026#34;nameofformPrefix\u0026#34; { try { treeNode = xUtilElements::getNodeInTree(xUtilElements::parentElement(e)); args.name(treeNode.AOTname()); formRun = ClassFactory.formRunClass(args); formRun.init(); //formRun.run(); //\u0026lt;-- No need to run the form, but sometimes it can load the data  formRun.close(); } catch { Infolog.clear(); continue; } } } ","permalink":"https://nuxulu.com/posts/2015-11-17-caching-form-into-aos-to-increase-performance-for-dynamics-ax/","summary":"Scenario:\nNormally when we open form for the first time in Dynamics AX, it will take sometimes to compilte and cache into AOS. That\u0026rsquo;s why at second time always faster. If we often restart AOS, how can we keep performance ?\nSolution:\nAfter restart AOS we can let some scripts run to open up some often used forms then cache to AOS, then close those forms.\nHere is the code","title":"Caching form into AOS to increase performance for Dynamics AX"},{"content":"Beside modify metadata on form properties, as best practice we can use code like below to assign default value for combobox.\n You can use this code in the form\u0026rsquo;s init method after super():  {% highlight csharp %} ComboBoxName.selection(ComboBoxName::DefaultValue); {% endhighlight %}\n If this is a table field we should you best practice overriding the initValue method in the table:  {% highlight csharp %} this.ComboBoxName = ComboBoxName::DefaultValue; {% endhighlight %}\n Override initValue in the form\u0026rsquo;s datasource only if it should be a specific behaviour in this form only.  ","permalink":"https://nuxulu.com/posts/2015-11-03-defaultvale-combobox-in-x/","summary":"Beside modify metadata on form properties, as best practice we can use code like below to assign default value for combobox.\n You can use this code in the form\u0026rsquo;s init method after super():  {% highlight csharp %} ComboBoxName.selection(ComboBoxName::DefaultValue); {% endhighlight %}\n If this is a table field we should you best practice overriding the initValue method in the table:  {% highlight csharp %} this.ComboBoxName = ComboBoxName::DefaultValue; {% endhighlight %}","title":"DefaultValue ComboBox in Dynamics AX with X++"},{"content":"To make something as the LIKE operator in a query, just assign a value to the QueryRange including a wildcard.\nstatic void QueryBuildRange_Sample(Args _args) { Query query = new Query(); QueryRun queryRun; QueryBuildDataSource qbds; QueryBuildRange queryRange; CustTable custTable; qbds.addDataSource(tableNum(CustTable)); queryRange = qbds.addRange(fieldNum(CustTable, AccountNum)); queryRange.value(\u0026#34;axd*\u0026#34;); queryRun = new QueryRun(query); while(queryRun.next()) { custTable = queryRun.get(tableNum(CustTable)); print custTable.AccountNum; } pause; } ","permalink":"https://nuxulu.com/posts/2015-11-13-how-to-use-like-operator-in-querybuildrange/","summary":"To make something as the LIKE operator in a query, just assign a value to the QueryRange including a wildcard.\nstatic void QueryBuildRange_Sample(Args _args) { Query query = new Query(); QueryRun queryRun; QueryBuildDataSource qbds; QueryBuildRange queryRange; CustTable custTable; qbds.addDataSource(tableNum(CustTable)); queryRange = qbds.addRange(fieldNum(CustTable, AccountNum)); queryRange.value(\u0026#34;axd*\u0026#34;); queryRun = new QueryRun(query); while(queryRun.next()) { custTable = queryRun.get(tableNum(CustTable)); print custTable.AccountNum; } pause; } ","title":"HOW TO USE \"LIKE\" OPERATOR IN QUERYBUILDRANGE"},{"content":"  Open Microsoft SQL Server Management Studio.\n  Connect to the server where in the DB you want to rename is located.\n  Modify the following script and run it\n  -- Replace all MyDBs with the name of the DB you want to change its name USE [MyDB]; -- Changing Physical names and paths -- Replace all NewMyDB with the new name you want to set for the DB -- Replace \u0026#39;C:...NewMyDB.mdf\u0026#39; with full path of new DB file to be used ALTER DATABASE MyDB MODIFY FILE (NAME = \u0026#39; MyDB \u0026#39;, FILENAME = \u0026#39;C:...NewMyDB.mdf\u0026#39;); -- Replace \u0026#39;C:...NewMyDB_log.ldf\u0026#39; with full path of new DB log file to be used ALTER DATABASE MyDB MODIFY FILE (NAME = \u0026#39; MyDB _log\u0026#39;, FILENAME = \u0026#39;C:...NewMyDB_log.ldf\u0026#39;); -- Changing logical names ALTER DATABASE MyDB MODIFY FILE (NAME = MyDB, NEWNAME = NewMyDB); ALTER DATABASE MyDB MODIFY FILE (NAME = MyDB _log, NEWNAME = NewMyDB_log);  Right click on the DB and select Tasks\u0026gt;Take Offline\n  Go to the location that MDF and LDF files are located and rename them exactly as you specified in first two alter commands. If you changed the folder path, then you need to move them there.\n  Go back to Microsoft SQL Server Management Studio and right click on the DB and select Tasks\u0026gt;Bring Online.\n  ","permalink":"https://nuxulu.com/posts/2015-11-03-rename-a-database-in-sql-server/","summary":"Open Microsoft SQL Server Management Studio.\n  Connect to the server where in the DB you want to rename is located.\n  Modify the following script and run it\n  -- Replace all MyDBs with the name of the DB you want to change its name USE [MyDB]; -- Changing Physical names and paths -- Replace all NewMyDB with the new name you want to set for the DB -- Replace \u0026#39;C:.","title":"Rename a Database in SQL Server"},{"content":"When I try to install Web Services on IIS for Retails POS Component, and I got the problem.\nThis scenario shouldn\u0026rsquo;t be common in a production environment, but, it is indeed quite common in a VM machine (I\u0026rsquo;m using virtual machine Hyper-V on Windows 8.1)\nError:Exception has been thrown by the target of an invocation\nSo, the problem is relate to thee AOS Service account, which by default is usually NT AUTHORITYNETWORK SERVICE account.\nbut because of we are running on a Domain Controller Server, we should use any specific Domain account created just for running AOS services, then you’ll success installing Web Services on IIS.\nNow please restart services and try again.\nThank you for reading\n","permalink":"https://nuxulu.com/posts/2015-08-04-web-services-on-iis-exception-has-been-thrown-by-the-target-of-an-invocation-ax-installation/","summary":"\u003cp\u003eWhen I try to install Web Services on IIS for Retails POS Component, and I got the problem.\u003c/p\u003e\n\u003cp\u003eThis scenario shouldn\u0026rsquo;t be common in a production environment, but, it is indeed quite common in a VM machine (I\u0026rsquo;m using virtual machine Hyper-V on Windows 8.1)\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/web-services-on-iis-exception_1.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Web Services on IIS - Exception has been thrown by the target of an invocation"},{"content":"reportManagerWebConfig.ps1\n{% highlight powershell %} #Modify the Report Server Web.config file. ie replace MSRS11.VAS with your folder name Set-ExecutionPolicy Unrestricted $webConfig = \u0026ldquo;C:Program FilesMicrosoft SQL ServerMSRS11.VASReporting ServicesReportManagerWeb.config\u0026rdquo; $currentDate = (get-date).tostring(\u0026ldquo;mm_dd_yyyy-hh_mm_s\u0026rdquo;) # month_day_year - hours_mins_seconds $backup = $webConfig + \u0026ldquo;_$currentDate\u0026rdquo; $doc = new-object System.Xml.XmlDocument $doc.Load($webConfig) #save a backup copy $doc.Save($backup) Write-Host \u0026ldquo;Backup saved as \u0026quot; + $backup $node = $doc.get_DocumentElement().\u0026ldquo;system.web\u0026rdquo;.httpRuntime $attribute = $doc.CreateAttribute(\u0026ldquo;maxRequestLength\u0026rdquo;) $attribute.set_Value(\u0026ldquo;100000\u0026rdquo;) $node.SetAttributeNode($attribute ) $doc.Save($webConfig) Write-Host \u0026ldquo;1) Modified the Report Manager Web.config file\u0026rdquo; {% endhighlight %}\n RsReportServer.ps1\n{% highlight powershell %} #Modify the Report Server Web.config file. ie replace MSRS11.VAS with your folder name Set-ExecutionPolicy Unrestricted\n$version = \u0026ldquo;6.3.0.0\u0026rdquo; $webConfig = \u0026ldquo;C:Program FilesMicrosoft SQL ServerMSRS11.VASReporting ServicesReportServerRsReportServer.config\u0026rdquo; $currentDate = (get-date).tostring(\u0026ldquo;mm_dd_yyyy-hh_mm_s\u0026rdquo;) # month_day_year - hours_mins_seconds $backup = $webConfig + \u0026ldquo;.xml\u0026rdquo; #\u0026quot;_$currentDate\u0026rdquo; $doc = new-object System.Xml.XmlDocument\n$doc.Load($webConfig)\n#save a backup copy $doc.Save($backup) Write Write-Host \u0026ldquo;Backup saved as \u0026quot; + $backup #remove node \u0026ldquo;RSWindowsNegotiate\u0026rdquo; $node = $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Authentication/AuthenticationTypes/RSWindowsNegotiate\u0026rdquo;) if ($node) {\n{ $node.ParentNode.RemoveChild($node) Write Write-Host \u0026ldquo;RSWindowsNegotiate Removed\u0026rdquo; } #Add node \u0026ldquo;IsRdceEnabled\u0026rdquo; $nodeService = $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Service\u0026quot;) $nodeIsRdceEnabled = $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Service/IsRdceEnabled\u0026quot;)\n#Set to true if already exists, otherwise create the node if ($nodeIsRdceEnabled) {\n{ $nodeIsRdceEnabled.InnerXml = \u0026ldquo;True\u0026rdquo; } else { if ($nodeService) {\n{\r$subnodeService = $doc.createElement(\u0026quot;IsRdceEnabled\u0026quot;)\r$subnodeService.InnerXml = \u0026quot;True\u0026quot;\r$nodeService.appendChild($subnodeService)\rWrite\rWrite-Host \u0026quot;IsRdceEnabled added\u0026quot;\r}\r } #Find Data $nodeCodeAxQuery = $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Extensions/Data/Extension[@Name=\u0026lsquo;AXQUERY\u0026rsquo;]\u0026quot;)\n#Add Data Extension if (-not($nodeCodeAxQuery)) {\n{ $nodeData = $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Extensions/Data\u0026quot;)\nif ($nodeData) {\r{\r$nodeData.InnerXml = $nodeData.InnerXml + \u0026quot;\u0026lt;Extension Name='AXQUERY' Type='Microsoft.Dynamics.Framework.Reports.AxQueryConnection,Microsoft.Dynamics.Framework.ReportsExtensions, Version=\u0026quot; +\r$version +\r\u0026quot;, Culture=neutral, PublicKeyToken=31bf3856ad364e35'/\u0026gt;\r\u0026lt;Extension Name='AXDATAMETHOD' Type='Microsoft.Dynamics.Framework.Reports.AxDataMethodConnection,Microsoft.Dynamics.Framework.ReportsExtensions, Version=\u0026quot; +\r$version +\r\u0026quot;, Culture=neutral, PublicKeyToken=31bf3856ad364e35'/\u0026gt;\r\u0026lt;Extension Name='AXREPORTDATAPROVIDER' Type='Microsoft.Dynamics.Framework.Reports.AxReportProviderConnection,Microsoft.Dynamics.Framework.ReportsExtensions, Version=\u0026quot; +\r$version +\r\u0026quot;, Culture=neutral, PublicKeyToken=31bf3856ad364e35'/\u0026gt;\r\u0026lt;Extension Name='AXADOMD' Type='Microsoft.Dynamics.Framework.Reports.AxAdomdConnection,Microsoft.Dynamics.Framework.ReportsExtensions, Version=\u0026quot; +\r$version +\r\u0026quot;, Culture=neutral, PublicKeyToken=31bf3856ad364e35'/\u0026gt;\r\u0026lt;Extension Name='AXENUMDATAPROVIDER' Type='Microsoft.Dynamics.Framework.Reports.EnumProviderConnection,Microsoft.Dynamics.Framework.ReportsExtensions, Version=\u0026quot; +\r$version +\r\u0026quot;, Culture=neutral, PublicKeyToken=31bf3856ad364e35'/\u0026gt;\u0026quot;\rWrite-Host \u0026quot;Data Extension added\u0026quot;\r}\r } #Find Extensions $nodeExtensions = $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Extensions\u0026quot;) $nodeReportDefinitionCustomization = $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Extensions/ReportDefinitionCustomization\u0026quot;)\nif (-not($nodeReportDefinitionCustomization)) {\n{ #Add ReportDefinitionCustomization if ($nodeExtensions) {\n{\r$subnodeExt = $doc.createElement(\u0026quot;ReportDefinitionCustomization\u0026quot;)\r$subnodeExt.InnerXml = \u0026quot;\u0026lt;Extension Name='AXRDCE' Type='Microsoft.Dynamics.Framework.Reports.AxRdce.CustomizationExtension,Microsoft.Dynamics.Framework.ReportsExtensions, Version=\u0026quot; +\r$version +\r\u0026quot;, Culture=neutral, PublicKeyToken=31bf3856ad364e35'/\u0026gt;\u0026quot;\r$nodeExtensions.appendChild($subnodeExt)\rWrite\rWrite-Host \u0026quot;ReportDefinitionCustomization added\u0026quot;\r}\r } else { $nodeAXRDCE= $doc.documentElement.SelectSingleNode(\u0026quot;//Configuration/Extensions/ReportDefinitionCustomization/Extension [@Name=\u0026lsquo;AXRDCE\u0026rsquo;]\u0026quot;)\nif (-not($nodeAXRDCE))\r{\r{\r$nodeReportDefinitionCustomization.InnerXml = $nodeReportDefinitionCustomization.InnerXml +\r\u0026quot;\u0026lt;Extension Name='AXRDCE' Type='Microsoft.Dynamics.Framework.Reports.AxRdce.CustomizationExtension,Microsoft.Dynamics.Framework.ReportsExtensions, Version=\u0026quot; +\r$version +\r\u0026quot;, Culture=neutral, PublicKeyToken=31bf3856ad364e35'/\u0026gt;\u0026quot;\r}\r } $doc.Save($webConfig)\nWrite Write-Host \u0026ldquo;2) Modified the Report Server Web.config file\u0026rdquo; text some thing {% endhighlight %}\nReportServerRsSrvPolicyConfig.ps1\n{% highlight powershell %} #Instructions: Modify the Report Server Web.config file. ie replace MSRS11.VAS with your folder name\nModify version 6.3.0.0 for Dynamics AX 2012 Set-ExecutionPolicy Unrestricted $version = \u0026ldquo;6.3.0.0\u0026rdquo; $webConfig = \u0026ldquo;C:Program FilesMicrosoft SQL ServerMSRS11.VASReporting ServicesReportServerrssrvpolicy.config\u0026rdquo; $currentDate = (get-date).tostring(\u0026ldquo;mm_dd_yyyy-hh_mm_s\u0026rdquo;) # month_day_year - hours_mins_seconds $backup = $webConfig + \u0026ldquo;.xml\u0026rdquo; #\u0026quot;_$currentDate\u0026quot; $doc = new-object System.Xml.XmlDocument $doc.Load($webConfig) #save a backup copy $doc.Save($backup) Write Write-Host \u0026ldquo;Backup saved as \u0026quot; + $backup #Add SecurityClass $nodeAxSessionPermission = $doc.documentElement.SelectSingleNode(\u0026quot;//configuration/mscorlib/security/policy/PolicyLevel/SecurityClasses/SecurityClass[@Name=\u0026lsquo;AxSessionPermission\u0026rsquo;]\u0026quot;) if (-not($nodeAxSessionPermission)) { { $nodeSecurityClasses = $doc.documentElement.SelectSingleNode(\u0026quot;//configuration/mscorlib/security/policy/PolicyLevel/SecurityClasses\u0026rdquo;) if ($nodeSecurityClasses) { { $nodeSecurityClass = $doc.createElement(\u0026ldquo;SecurityClass\u0026rdquo;) $nodeSecurityClass.InnerXml = \u0026ldquo;\u0026rdquo; $nodeSecurityClasses.appendChild($nodeSecurityClass.FirstChild) Write Write-Host \u0026ldquo;SecurityClass added\u0026rdquo; } } #Add PermissionSet $nodeAxSessionPermissionSet = $doc.documentElement.SelectSingleNode(\u0026quot;//configuration/mscorlib/security/policy/PolicyLevel/NamedPermissionSets/PermissionSet[@class=\u0026lsquo;NamedPermissionSet\u0026rsquo; and @version=\u0026lsquo;1\u0026rsquo; and @Name=\u0026lsquo;AxSessionPermissionSet\u0026rsquo;]\u0026quot;) if (-not($nodeAxSessionPermissionSet)) { { $nodeNamedPermissionSets = $doc.documentElement.SelectSingleNode(\u0026quot;//configuration/mscorlib/security/policy/PolicyLevel/NamedPermissionSets\u0026quot;) if ($nodeNamedPermissionSets) { { $nodePermissionSet = $doc.createElement(\u0026ldquo;PermissionSet\u0026rdquo;) $nodePermissionSet.InnerXml = \u0026ldquo;\u0026rdquo; $nodeNamedPermissionSets.appendChild($nodePermissionSet.FirstChild) Write Write-Host \u0026ldquo;PermissionSet added\u0026rdquo; } } #Add CodeGroup $nodeAxStrongName = $doc.documentElement.SelectSingleNode(\u0026quot;//configuration/mscorlib/security/policy/PolicyLevel/CodeGroup[@class=\u0026lsquo;FirstMatchCodeGroup\u0026rsquo; and @version=\u0026lsquo;1\u0026rsquo; and @PermissionSetName=\u0026lsquo;Nothing\u0026rsquo;]/CodeGroup[@class=\u0026lsquo;UnionCodeGroup\u0026rsquo; and @version=\u0026lsquo;1\u0026rsquo; and @PermissionSetName=\u0026lsquo;FullTrust\u0026rsquo; and @Name=\u0026lsquo;AX_Reports_Strong_Name\u0026rsquo;]\u0026quot;) if (-not($nodeAxStrongName)) { { $nodeCodeGroup = $doc.documentElement.SelectSingleNode(\u0026quot;//configuration/mscorlib/security/policy/PolicyLevel/CodeGroup[@class=\u0026lsquo;FirstMatchCodeGroup\u0026rsquo; and @version=\u0026lsquo;1\u0026rsquo; and @PermissionSetName=\u0026lsquo;Nothing\u0026rsquo;]\u0026quot;) if ($nodeCodeGroup) { { $subnodeCodeGroup = $doc.createElement(\u0026ldquo;CodeGroup\u0026rdquo;) $subnodeCodeGroup.InnerXml = \u0026ldquo;\u0026rdquo; $nodeCodeGroup.appendChild($subnodeCodeGroup.FirstChild) Write Write-Host \u0026ldquo;CodeGroup added\u0026rdquo; } } #Add nodeReportExpressionPermission #http://blogs.msdn.com/b/axsupport/archive/2012/02/02/microsoft-dynamics-ax-2012-reporting-extensions-error-system-security-permissions-environmentpermission-while-running-report.aspx $nodeReportExpressionPermission = $doc.documentElement.SelectSingleNode(\u0026quot;//configuration/mscorlib/security/policy/PolicyLevel/CodeGroup[@class=\u0026lsquo;FirstMatchCodeGroup\u0026rsquo; and @version=\u0026lsquo;1\u0026rsquo; and @PermissionSetName=\u0026lsquo;Nothing\u0026rsquo;]/CodeGroup[@class=\u0026lsquo;UnionCodeGroup\u0026rsquo; and @version=\u0026lsquo;1\u0026rsquo; and @PermissionSetName=\u0026lsquo;Execution\u0026rsquo; and @Name=\u0026lsquo;Report_Expressions_Default_Permissions\u0026rsquo;]\u0026quot;) if ($nodeReportExpressionPermission) { { $attribute = $doc.CreateAttribute(\u0026ldquo;PermissionSetName\u0026rdquo;) $attribute.set_Value(\u0026ldquo;FullTrust\u0026rdquo;) $nodeReportExpressionPermission.SetAttributeNode($attribute ) } } $doc.Save($webConfig) Write Write-Host \u0026ldquo;3) Modified the Report Server rssrvpolicy.config file\u0026rdquo;\n{% endhighlight %}\n","permalink":"https://nuxulu.com/posts/2015-07-02-modify-microsoft-dynamics-ax-2012-r3-ssrs-configurations/","summary":"\u003cp\u003e\u003ccode\u003ereportManagerWebConfig.ps1\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e{% highlight powershell %}\n#Modify the Report Server Web.config file. ie replace MSRS11.VAS with your folder name\nSet-ExecutionPolicy Unrestricted\n$webConfig = \u0026ldquo;C:Program FilesMicrosoft SQL ServerMSRS11.VASReporting ServicesReportManagerWeb.config\u0026rdquo;\n$currentDate = (get-date).tostring(\u0026ldquo;mm_dd_yyyy-hh_mm_s\u0026rdquo;) # month_day_year - hours_mins_seconds\n$backup = $webConfig + \u0026ldquo;_$currentDate\u0026rdquo;\n$doc = new-object System.Xml.XmlDocument\n$doc.Load($webConfig)\n#save a backup copy\n$doc.Save($backup)\nWrite-Host \u0026ldquo;Backup saved as \u0026quot; + $backup\n$node = $doc.get_DocumentElement().\u0026ldquo;system.web\u0026rdquo;.httpRuntime\n$attribute = $doc.CreateAttribute(\u0026ldquo;maxRequestLength\u0026rdquo;)\n$attribute.set_Value(\u0026ldquo;100000\u0026rdquo;)\n$node.SetAttributeNode($attribute )\n$doc.Save($webConfig)\nWrite-Host \u0026ldquo;1) Modified the Report Manager Web.config file\u0026rdquo;\n{% endhighlight %}\u003c/p\u003e","title":"Modify Microsoft Dynamics AX 2012 R3 SSRS configurations using PowerShell"},{"content":"If you have only 10 mins to build ax, try this\nIn AOS server, go to C:\\Program Files\\Microsoft Dynamics AX\\60\\Server\\DAX\\bin and open cmd from here then run this command\n{% highlight yaml %} axbuild.exe xppcompileall /s=01 /altbin=\u0026ldquo;C:\\Program Files (x86)\\Microsoft Dynamics AX\\60\\Client\\Bin\u0026rdquo; {% endhighlight %}\nResult\nOnce compile complete, you can import compile log file at C:\\Program Files\\Microsoft Dynamics AX\\60\\Server\\DAX\\log into compiler output of AX client\nReference from MSDN .\n","permalink":"https://nuxulu.com/posts/2015-07-01-axbuild-exe-for-parallel-compile-on-aos/","summary":"If you have only 10 mins to build ax, try this\nIn AOS server, go to C:\\Program Files\\Microsoft Dynamics AX\\60\\Server\\DAX\\bin and open cmd from here then run this command\n{% highlight yaml %} axbuild.exe xppcompileall /s=01 /altbin=\u0026ldquo;C:\\Program Files (x86)\\Microsoft Dynamics AX\\60\\Client\\Bin\u0026rdquo; {% endhighlight %}\nResult\nOnce compile complete, you can import compile log file at C:\\Program Files\\Microsoft Dynamics AX\\60\\Server\\DAX\\log into compiler output of AX client\nReference from MSDN .","title":"AxBuild.exe for Parallel Compile on AOS"},{"content":"Form interaction classes that allow user interface control logic to be shared across forms. For instance, controlling which buttons are available to a list page and the associated detail form.\nThe interaction classes are extending a base ListPageInteraction class. This has some methods supported by the kernel to interact e.g. with initializations of the list page form. Other classes can be build stand alone to execute e.g. a batch process or represent a web service or posting classes.\nForm interaction classes are not mandatory for list pages but should be used on data entry forms that require logic. This ensures consistency and allows easier maintenance of logic.\nThis class inherits from SysListPageInteractionBase, here is some methods we need to know\n  initializing: Called when the form is initializing – Similar to the form init method\n  intializeQuery: Also called when the form is initializing – Similar to the datasource init method.\n  selectionChanged: Called when the active record changes – Similar to the datasource active method.\n  setButtonEnabled: Should be overridden to dynamically enable/disable buttons based on the current selection. This is called from the selectionChanged method.\n  {% highlight csharp %} public void selectionChanged() { Requisition requisition = this.listPage().activeRecord(queryDataSourceStr(RequisitionQuery,Requisition)); super();\nif(requisition.WorkflowApprovalStatus == WorkflowApprovalStatus::Approved)\rthis.listPage().actionPaneControlEnabled(formControlStr(RequisitionListPage,Edit),true);\relse\rthis.listPage().actionPaneControlEnabled(formControlStr(RequisitionListPage,Edit),false);\r } {% endhighlight %}\n setButtonVisibility: Should be overridden to show/hide buttons when the form first opens. This is used more to do a one-off layout adjustment based on system configuration/parameters, as well as the menu-item used to open the form.  eg If you have a menu-item that opens a form based on status, you may want to hide the relevant status field to reduce clutter.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2015-06-09-list-page-interaction-class/","summary":"\u003cp\u003eForm interaction classes that allow user interface control logic to be shared across forms. For instance, controlling which buttons are available to a list page and the associated detail form.\u003c/p\u003e\n\u003cp\u003eThe interaction classes are extending a base ListPageInteraction class. This has some methods supported by the kernel to interact e.g. with initializations of the list page form. Other classes can be build stand alone to execute e.g. a batch process or represent a web service or posting classes.\u003c/p\u003e\n\u003cp\u003eForm interaction classes are not mandatory for list pages but should be used on data entry forms that require logic. This ensures consistency and allows easier maintenance of logic.\u003c/p\u003e\n\u003cp\u003eThis class inherits \u003ccode\u003efrom SysListPageInteractionBase\u003c/code\u003e, here is some methods we need to know\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003einitializing\u003c/strong\u003e: Called when the form is initializing – Similar to the form init method\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eintializeQuery\u003c/strong\u003e: Also called when the form is initializing – Similar to the datasource init method.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eselectionChanged\u003c/strong\u003e: Called when the active record changes – Similar to the datasource active method.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003esetButtonEnabled\u003c/strong\u003e: Should be overridden to dynamically enable/disable buttons based on the current selection. This is called from the selectionChanged method.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"List Page Interaction Class"},{"content":"This is to show a method to execute external database stored procedures from X++ code. Create a job in AOT with following code. Follow the steps as explained here in the code like replace Server Name, Database name , Stored Procedure name.\nThe below code is executed through the ODBC Connection.\n{% highlight csharp %}\nstatic void execExternalDatabase(Args _args) { LoginProperty loginProperty; ODBCConnection odbcConnection; Statement statement; ResultSet resultSet;\nResultSetMetaData resultSetMetaData;\rCounter counter;\rstr sql;\rSqlStatementExecutePermission perm;\r;\rloginProperty = new LoginProperty();\rloginProperty.setServer(\u0026quot;SERVERNAME Here\u0026quot;); // Replace your Database Server Name here\rloginProperty.setDatabase(\u0026quot;DemoDB\u0026quot;); //Replace your Database name here\rodbcConnection = new ODBCConnection(loginProperty); // setting odbc connection here.\r// ODBC Connection to create statement\rstatement = odbcConnection.createStatement();\r// Replace the StoredProcedure you want to execute.\rsql = strfmt('EXEC[myStoredProcedureName]');\r// Set code access permission to Execute\rperm = new SqlStatementExecutePermission(sql);\rperm.assert();\rtry\r{\r// if Stored Procedure has Select query use executeQuery method.\rresultSet = statement.executeQuery(sql);\rresultSet.next();\rresultSetMetaData = resultSet.getMetaData();\rfor (counter = 1; counter \u0026lt;= resultSetMetaData.getColumnCount(); counter++)\r{\rswitch(resultSetMetaData.getColumnType(counter))\r{\rcase 0,1 :\rinfo(resultSet.getString(counter));\rbreak;\rcase 3:\rinfo(date2StrUsr(resultSet.getdate(counter)));\rbreak;\r}\r}\r}\rcatch (exception::Error)\r{\rprint \u0026quot;An error occured in the query.\u0026quot;;\rpause;\r}\r// Code access permission scope ends here.\rCodeAccessPermission::revertAssert();\r }\n{% endhighlight %}\nUsed a simple select query in the stored procedure and the result will be displayed on the infolog.\n","permalink":"https://nuxulu.com/posts/2015-05-13-execute-external-database-stored-procedure-from-x-code-using-odbc-connectivity/","summary":"This is to show a method to execute external database stored procedures from X++ code. Create a job in AOT with following code. Follow the steps as explained here in the code like replace Server Name, Database name , Stored Procedure name.\nThe below code is executed through the ODBC Connection.\n{% highlight csharp %}\nstatic void execExternalDatabase(Args _args) { LoginProperty loginProperty; ODBCConnection odbcConnection; Statement statement; ResultSet resultSet;\nResultSetMetaData resultSetMetaData;\rCounter counter;\rstr sql;\rSqlStatementExecutePermission perm;\r;\rloginProperty = new LoginProperty();\rloginProperty.","title":"Execute external database Stored Procedure from X++ code using ODBC connectivity"},{"content":"Login to the AX AOS server and follow below steps.\n  Click Start \u0026gt; Administrative Tools.\n  Right-click the Microsoft Dynamics AX 2012 Management Shell option.\n  Click Run as administrator.\n  Go to PowerShell command prompt and run with command\nPublish-AXReport –ReportName *\nWait up to getting Deployment successful message in command prompt.\n","permalink":"https://nuxulu.com/posts/2015-05-13-how-to-deploy-all-the-reports-in-ax2012-by-using-management-shell/","summary":"Login to the AX AOS server and follow below steps.\n  Click Start \u0026gt; Administrative Tools.\n  Right-click the Microsoft Dynamics AX 2012 Management Shell option.\n  Click Run as administrator.\n  Go to PowerShell command prompt and run with command\nPublish-AXReport –ReportName *\nWait up to getting Deployment successful message in command prompt.","title":"How to deploy all the reports in AX2012 by using Management Shell"},{"content":"To execute a stored procedure from X++ use the Server method, the Client method does not have permissions. you don’t require any special privileges or permissions to execute a stored procedure. if used other then Server method, a message should appear like this Request for the permission of type SqlStatementExecutePermission failed.\n{% highlight csharp %} public static void main(AssemblyLoadEventArgs _args) { Connection con = new Connection(); Statement stmt = new Con.createStatement();\nResultSet r;\rstr sql;\rSqlStatementExecutePermission perm;\rsql = stmt('EXEC [StoreprocedureName]');\rperm = new SqlStatementExecutePermission(sql);\rperm.assert();\rtry\r{\rstmt.executeUpdate(sql);\r}\rcatch (Exception::Error)\r{\rprint \u0026quot;An error occured in the query\u0026quot;;\rpause;\r} CodeAccessPermission::revertAssert();\r } {% endhighlight %}\n","permalink":"https://nuxulu.com/posts/2015-05-12-execute-stored-procedure-from-x-code/","summary":"To execute a stored procedure from X++ use the Server method, the Client method does not have permissions. you don’t require any special privileges or permissions to execute a stored procedure. if used other then Server method, a message should appear like this Request for the permission of type SqlStatementExecutePermission failed.\n{% highlight csharp %} public static void main(AssemblyLoadEventArgs _args) { Connection con = new Connection(); Statement stmt = new Con.","title":"Execute Stored Procedure from X++ code"},{"content":"When creating new elements, ensure that you follow the recommended naming conventions. Any future development and maintenance will be much easier.\nAx Microsoft Dynamics AX typed data source\nAxd Microsoft Dynamics AX business document\nAsset Asset management\nBOM Bill of material\nCOS Cost accounting\nCust Customer\nDir Directory, global address book\nEcoRes Economic resources\nHRM/HCM Human resources\nInvent Inventory management\nJMG Shop flor control\nKM Knowledge management\nLedger General ledger\nPBA Product builder\nProd Production\nProj Project\nPurch Purchase\nReq Requirements\nSales Sales\nSMA Service management\nSMM Sales and marketing management, also called customer relationship management (CRM)\nSys Application frameworks and development tools\nTax Tax engine\nVend Vendor\nWeb Web framework\nWMS Warehouse management\n","permalink":"https://nuxulu.com/posts/2015-04-13-common-prefix-dyanmics-ax-2012/","summary":"When creating new elements, ensure that you follow the recommended naming conventions. Any future development and maintenance will be much easier.\nAx Microsoft Dynamics AX typed data source\nAxd Microsoft Dynamics AX business document\nAsset Asset management\nBOM Bill of material\nCOS Cost accounting\nCust Customer\nDir Directory, global address book\nEcoRes Economic resources\nHRM/HCM Human resources\nInvent Inventory management\nJMG Shop flor control\nKM Knowledge management\nLedger General ledger","title":"Common prefies Dyanmics AX 2012"},{"content":"Tip 1: Measure execution time of your code Measuring is knowing. Before you start changing code, make sure you have a set of data you can keep reusing for your tests. Measure the performance of your code on that data after each change in code so you know the impact of your changes.\nOne way to do this is by using the Winapi::getTickCount() or WinApiServer::getTickCount() if your code runs on server method.\n{% highlight csharp %} static void KlForTickCountSample(Args _args) { int ticks; ; // get the tickcount before the process starts ticks = winapi::getTickCount();\n// start the process\rsleep(2000); // simulate 2 seconds of processing\r// compare tickcount\rticks = winapi::getTickCount() – ticks;\r// display result\rinfo(strfmt('Number of ticks: %1', ticks));\r } {% endhighlight %}\nTip 2: limit the number of loops A LOT of time goes into loops. If you have a performance problem, start looking for loops. Code can run really fast, but it can get slow when it is executed too many time, eg, in a loop.\nTip 3: avoid if in while select When there is a if in a while select, see if you can rewrite it a a where statement in your select. Don’t be affraid use a join either. Consider the following example:\n{% highlight csharp %} static void KlForIfInLoop(Args _args) { VendTable vendTable; ; // usually slower while select vendTable { if(vendTable.VendGroup == \u0026lsquo;VG1\u0026rsquo;) { info(vendTable.AccountNum); } }\n// usually faster\rwhile select vendTable\rwhere vendTable.VendGroup == 'VG1'\r{\rinfo(vendTable.AccountNum);\r}\r } {% endhighlight %}\nTip 4: avoid double use of table methods Using table methods a lot can get really slow if you do it wrong. Consider the following example:\n{% highlight csharp %} static void klForTableMethodsSlow(Args _args) { SalesLine salesLine; InventDim inventDim; ;\n// select a salesline\rselect firstonly salesLine;\rinventDim.InventColorId = salesLine.inventDim().InventColorId;\rinventDim.InventSizeId = salesLine.inventDim().InventSizeId;\rinventDim.inventBatchId = salesLine.inventDim().inventBatchId;\r } {% endhighlight %}\nThis example code looks nice, but there’s a problem. The salesLine.inventDim() method contains the following:\n{% highlight csharp %} InventDim inventDim(boolean _forUpdate = false) { return InventDim::find(this.InventDimId, _forUpdate); } {% endhighlight %}\nThis means that the invendDim record is read three times from the database. It is better to declare the inventDim record locally and only retrieve it once:\n{% highlight csharp %} static void klForTableMethodsFast(Args _args) { SalesLine salesLine; InventDim inventDim; InventDim inventDimLoc; ;\n// select a salesline\rselect firstonly salesLine;\rinventDimLoc = salesLine.inventDim();\rinventDim.InventColorId = inventDimLoc.InventColorId;\rinventDim.InventSizeId = inventDimLoc.InventSizeId;\rinventDim.inventBatchId = inventDimLoc.inventBatchId;\r } {% endhighlight %}\nTip 5: Don’t put too much code on tables Code on tables is usually fast, but things can get slow if you use it to much. Say you have a table with an InventDimId field. If you have 5 methods that need the InventDim record, because you don’t have a classDeclaration method on your table, you need to call this function 5 times, once in every method:\n{% highlight csharp %} InventDim::find(this.inventDim) {% endhighlight %}\nWhen you put these methods on a class, you could optimise it by fetching the record only once and storing it in the classDeclaration, or better, passing it as a parameter to your methods. An other example is fetching parameters from parameter tables, eg InventParameters::find(). On a table, you have to fetch it each time you call a method. In a class, you would probably optimize your code to only fetch the parameter record once.\nTip 6: Use the fastest code For some tasks, there is special code that is faster than the code you would normally write. For example:\n{% highlight csharp %} // slower while select forupdate custTable where custTable.custGroup == \u0026lsquo;TST\u0026rsquo; { custTable.delete(); } // faster delete_from custTable where custTable.custGroup == \u0026lsquo;TST\u0026rsquo;; {% endhighlight %}\nThe same applies to update_recordset for updating records. Also, when adding values to the end of a container {% highlight csharp %} cont += \u0026ldquo;a value\u0026rdquo;; {% endhighlight %}\nis faster than\n{% highlight csharp %} cont = conins(cont, conlen(cont), \u0026ldquo;a value\u0026rdquo;); {% endhighlight %}\nTip 7: Every optimization counts Remember that every optimization you do to you code counts, even if it’s a little one. Small performance tweaks can have a huge effect once you process large quantities of data. So don’t be lazy, and optimize.\nShare from http://www.artofcreation.be/\n","permalink":"https://nuxulu.com/posts/2015-02-12-x-performance-tips/","summary":"\u003ch3 id=\"tip-1-measure-execution-time-of-your-code\"\u003eTip 1: Measure execution time of your code\u003c/h3\u003e\n\u003cp\u003eMeasuring is knowing. Before you start changing code, make sure you have a set of data you can keep reusing for your tests. Measure the performance of your code on that data after each change in code so you know the impact of your changes.\u003c/p\u003e\n\u003cp\u003eOne way to do this is by using the \u003ccode\u003eWinapi::getTickCount()\u003c/code\u003e or \u003ccode\u003eWinApiServer::getTickCount()\u003c/code\u003e if your code runs on server method.\u003c/p\u003e\n\u003cp\u003e{% highlight csharp %}\nstatic void KlForTickCountSample(Args _args)\n{\nint ticks;\n;\n// get the tickcount before the process starts\nticks = winapi::getTickCount();\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// start the process\r\nsleep(2000); // simulate 2 seconds of processing\r\n\r\n// compare tickcount\r\nticks = winapi::getTickCount() – ticks;\r\n\r\n// display result\r\ninfo(strfmt('Number of ticks: %1', ticks));\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e}\n{% endhighlight %}\u003c/p\u003e","title":"X++ Performance tips"},{"content":"In this post let\u0026rsquo;s explore creating XML using X++ code in Dynamics AX.\nThe following example shows how to create and write data to an XML file by using the XmlDocument, XmlElement, and XmlWriter classes. It loops through all of the records in the CarTable and find all the fields in the table automatically by using the DictTable and DictField classes.\n{% highlight csharp %} static void WriteXml(Args _args) {\nXmlDocument xmlDoc;\rXmlElement xmlRoot;\rXmlElement xmlField;\rXmlElement xmlRecord;\rXMLWriter xmlWriter;\rCarTable carTable;\rDictTable dTable = new DictTable(tablenum(CarTable));\rDictField dField;\rint i, fieldId;\rstr value;\r; #CarsXmlTags // Create a new object of the XmlDocument class\rxmlDoc = XmlDocument::newBlank();\r// Create the root node\rxmlRoot = xmlDoc.createElement(#CarRootNode);\r// Loop through all the records in the carTable\rwhile select carTable\r{\r// Create a XmlElement (record) to hold the\r// contents of the current record.\rxmlRecord = xmlDoc.createElement(#CarRecords);\r// Loop through all the fields in the record\rfor (i=1; i\u0026lt;=dTable.fieldCnt(); i++)\r{\r// Get the fieldId from the field-count\rfieldId = dTable.fieldCnt2Id(i);\r// Find the DictField object that matches the fieldId\rdField = dTable.fieldObject(fieldId);\r// Skip system fields\rif (dField.isSystem())\rcontinue;\r// Create a new XmlElement (field) and\r// have the name equal to the name of the\r// dictField\rxmlField = xmlDoc.createElement(dField.name());\r// Convert values to string. I have just added\r// a couple of conversion as an example.\r// Use tableName.(fieldId) instead of fieldname\r// to get the content of the field.\rswitch (dField.baseType())\r{\rcase Types::Int64 :\rvalue = int642str(carTable.(fieldId));\rbreak;\rcase Types::Integer :\rvalue = int2str(carTable.(fieldId));\rbreak;\rdefault :\rvalue = carTable.(fieldId);\rbreak;\r}\r// Set the innerText of the XmlElement (field)\r// to the value from the table\rxmlField.innerText(value);\r// Append the field as a child node to the record\rxmlRecord.appendChild(xmlField);\r}\r// Add the record as a child node to the root\rxmlRoot.appendChild(xmlRecord);\r}\r// Add the root to the XmlDocument\rxmlDoc.appendChild(xmlRoot);\r// Create a new object of the XmlWriter class\r// in order to be able to write the xml to a file\rxmlWriter = XMLWriter::newFile(@\u0026quot;c:tempcars.xml\u0026quot;);\r// Write the content of the XmlDocument to the\r// file as specified by the XmlWriter\rxmlDoc.writeTo(xmlWriter);\r } {% endhighlight %}\nThe file that is created looks like the one in the following screenshot(only first part of the file is shown):\nAs you can see, this file is based on a standard XML format with tags and values only. You can, however, use tag attributes as well. To put the values from the table into tag attributes instead of their own tags, simply change the following code snippet in the example above\n{% highlight csharp %}\n// Set the innerText of the XmlElement (field) // to the value from the table xmlField.innerText(value);\n// Append the field as a child node to the record xmlRecord.appendChild(xmlField);\n{% endhighlight %}\nWith these lines:\n{% highlight csharp %} // Add the attribute to the record xmlRecord.setAttribute(dField.name(), value); {% endhighlight %}\nThe file that is created now looks like the one in the following screenshot:\n","permalink":"https://nuxulu.com/posts/2015-02-04-create-and-write-to-xml-file-in-dynamics-ax/","summary":"\u003cp\u003eIn this post let\u0026rsquo;s explore creating XML using X++ code in Dynamics AX.\u003c/p\u003e\n\u003cp\u003eThe following example shows how to create and write data to an XML file by using the \u003ccode\u003eXmlDocument\u003c/code\u003e, \u003ccode\u003eXmlElement\u003c/code\u003e, and \u003ccode\u003eXmlWriter\u003c/code\u003e classes. It loops through all of the records in the \u003ccode\u003eCarTable\u003c/code\u003e and find all the fields in the table automatically by using the \u003ccode\u003eDictTable\u003c/code\u003e and \u003ccode\u003eDictField\u003c/code\u003e classes.\u003c/p\u003e\n\u003cp\u003e{% highlight csharp %}\nstatic void WriteXml(Args _args)\n{\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eXmlDocument xmlDoc;\r\nXmlElement xmlRoot;\r\nXmlElement xmlField;\r\nXmlElement xmlRecord;\r\nXMLWriter xmlWriter;\r\n\r\nCarTable carTable;\r\nDictTable dTable = new DictTable(tablenum(CarTable));\r\nDictField dField;\r\nint i, fieldId;\r\nstr value;\r\n; \r\n\r\n#CarsXmlTags \r\n\r\n// Create a new object of the XmlDocument class\r\nxmlDoc = XmlDocument::newBlank();\r\n\r\n// Create the root node\r\nxmlRoot = xmlDoc.createElement(#CarRootNode);\r\n\r\n// Loop through all the records in the carTable\r\nwhile select carTable\r\n{\r\n    // Create a XmlElement (record) to hold the\r\n    // contents of the current record.\r\n    xmlRecord = xmlDoc.createElement(#CarRecords);\r\n    // Loop through all the fields in the record\r\n\r\n    for (i=1; i\u0026lt;=dTable.fieldCnt(); i++)\r\n    {\r\n        // Get the fieldId from the field-count\r\n        fieldId = dTable.fieldCnt2Id(i);\r\n        \r\n        // Find the DictField object that matches the fieldId\r\n        dField = dTable.fieldObject(fieldId);\r\n        \r\n        // Skip system fields\r\n        if (dField.isSystem())\r\n        continue;\r\n\r\n        // Create a new XmlElement (field) and\r\n        // have the name equal to the name of the\r\n        // dictField\r\n        xmlField = xmlDoc.createElement(dField.name());\r\n        \r\n        // Convert values to string. I have just added\r\n        // a couple of conversion as an example.\r\n        // Use tableName.(fieldId) instead of fieldname\r\n        // to get the content of the field.\r\n        switch (dField.baseType())\r\n        {\r\n            case Types::Int64 :\r\n                value = int642str(carTable.(fieldId));\r\n            break;\r\n            case Types::Integer :\r\n                value = int2str(carTable.(fieldId));\r\n            break;\r\n            default :\r\n                value = carTable.(fieldId);\r\n            break;\r\n        }\r\n\r\n        // Set the innerText of the XmlElement (field)\r\n        // to the value from the table\r\n        xmlField.innerText(value);\r\n         \r\n        // Append the field as a child node to the record\r\n        xmlRecord.appendChild(xmlField);\r\n    }\r\n     // Add the record as a child node to the root\r\n    xmlRoot.appendChild(xmlRecord);\r\n}\r\n// Add the root to the XmlDocument\r\nxmlDoc.appendChild(xmlRoot);\r\n// Create a new object of the XmlWriter class\r\n// in order to be able to write the xml to a file\r\nxmlWriter = XMLWriter::newFile(@\u0026quot;c:tempcars.xml\u0026quot;);\r\n// Write the content of the XmlDocument to the\r\n// file as specified by the XmlWriter\r\nxmlDoc.writeTo(xmlWriter);\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e}\n{% endhighlight %}\u003c/p\u003e","title":"Create and write to XML file in Dynamics AX"},{"content":"The answer is that there is no difference, the difference is a conceptual one rather than a functional or a technical one. So I think you will make a better choice for your scenario base on functional side.\nDisplay Menu item This folder is used to contain menu items that reference runnable application objects that primarily present forms, ddialog and so on, to the user. May be this forms, dialog called from another forms.\nOutput Menu item An output menu item application objects whose primarily function is to print a result or report.\nAction Menu item You should create a menu item under this folder if your runnable application objects whose primarily function is to do some kind of a job, such as creating or updating transactions in the database.\n","permalink":"https://nuxulu.com/posts/2015-01-27-what-is-the-difference-difference-between-menu-item-display-output-and-action-in-dynamics-ax/","summary":"The answer is that there is no difference, the difference is a conceptual one rather than a functional or a technical one. So I think you will make a better choice for your scenario base on functional side.\nDisplay Menu item This folder is used to contain menu items that reference runnable application objects that primarily present forms, ddialog and so on, to the user. May be this forms, dialog called from another forms.","title":"What is the difference difference between menu item Display, Output and Action"},{"content":"The model store is the portion of the Microsoft Dynamics AX database where all Microsoft Dynamics AX application elements are stored, including customization. The model store replaces the AOD (application object definition) files used in previous releases of Microsoft Dynamics AX (I mean from 4.0 to 2009).\nLayer and model information are integral parts of the store. The AOS has access to the model store, handles layer-flattening, and provides model data to all the Microsoft Dynamics AX sub-systems, such as form- and report-rendering and X++ code.\nMicrosoft Dynamics AX contains sixteen layers. Each layer consists of one or more logical parts called models. A system generated model exists for each layer.\nFor example, the VAR Model is the system generated model for the VAR layer. You can use the system generated models to install and start working with the base Microsoft Dynamics AX system. You can leverage the capabilities of models, and tools and functionality that support the models, during customization of the Microsoft Dynamics AX application.\n  The model store is the portion of the Microsoft Dynamics AX database where all Microsoft Dynamics AX application elements are stored, including customization.\n  The model store replaces the AOD files used in previous releases of Microsoft Dynamics AX. It can be managed through the AXUtil command line utility, or by using Windows PowerShell.\n  The baseline model store database holds model store tables for the previous version of metadata. Use it only during an upgrade.\n  The baseline model store is similar to the old folder in previous releases of Microsoft Dynamics AX.\n  Thank you for reading!\n","permalink":"https://nuxulu.com/posts/2014-10-10-understand-model-store-architectural/","summary":"The model store is the portion of the Microsoft Dynamics AX database where all Microsoft Dynamics AX application elements are stored, including customization. The model store replaces the AOD (application object definition) files used in previous releases of Microsoft Dynamics AX (I mean from 4.0 to 2009).\nLayer and model information are integral parts of the store. The AOS has access to the model store, handles layer-flattening, and provides model data to all the Microsoft Dynamics AX sub-systems, such as form- and report-rendering and X++ code.","title":"Understand model store architectural in AX 2012"},{"content":" Understanding the internal architecture of Microsoft Dynamics AX 2012 can help you make decision when planning and developing a Microsoft Dynamics AX 2012 system. Here are some pointers on DAX 2012 architecture primarily for DAX 2012 architects \u0026amp; solution developers. This topic provides a high-level overview of the system architecture of Microsoft Dynamics AX.\n System architecture This diagram provides a high-level over of a Microsoft Dynamics AX 2012 system with all components installed, and describes how communications flow between the components.\n1. Application Object Server (AOS) architecture This diagram describes the functionality within the AOS Windows service, and describes how communications flow within it.\nNote: Clients communicate with an AOS by using remote procedure calls (RPCs), Windows Communication Foundation (WCF), or AOS services. In previous releases, other components and third-party programs could communicate with an AOS by using either .NET Business Connector or Application Integration Framework (AIF). For this release, we recommend that third-party programs use AOS services to communicate with AOS.\n2. Business Connector architecture The differences between the client kernel as it runs on a standard client and a Business Connector client are:\n  The Session Manager in the client kernel manages only a single instance–in the Business Connector kernel, it manages multiple instances.\n  he client kernel includes forms security, while the Business Connector kernel does not.\n  This diagram describes the architecture of the Business Connector version of the client kernel, and describes how communications flow within it.\n3. Application file server architecture Model store architecture Microsoft Dynamics AX contains sixteen layers. Each layer consists of one or more logical parts called models. A model is generated for each layer. For example, VAR Model is the model that the system generates for the VAR layer. The system generated models let you install and work with the base Microsoft Dynamics AX system.\nWhen you customize the Microsoft Dynamics AX program, you can take advantage of the capabilities of models.\nThe following table describes the application object layers in Microsoft Dynamics AX 2012:\nClient architecture This diagram describes the functionality within the client, and describes how communications flow within it.\nClient/server communication The client communicates with various Microsoft Dynamics AX components in the following ways:\n  The client uses the remote procedure call (RPC) protocol to communicate with Application Object Server (AOS). The client never accesses the database or metadata directly. AOS sends the application objects and data to the client.\n  The data layer that the client uses is based on data sources that are specified in metadata for forms and queries. In addition, any X++ code that is required to retrieve data can use the built-in language support to query and adjust data.\n  The client uses a report Web Part to interact with the report server. By calling the web services that are exposed by the report server, the report control in the Web Part displays information that is contained in Reporting Services reports. These reports can include either transnational data from the Microsoft Dynamics AX application or OLAP cubes from Microsoft SQL Server Analysis Services. Cubes provide business analytic and key performance indicators (KPIs).\n  The client provides workflow forms, alerts, and controls so that users can participate in the business process by using the Workflow system. The Workflow system is a Microsoft Dynamics AX component that enables workflow processes by using Windows Communication Foundation classes.\n  The client provides a Help viewer, which is an application that displays context-sensitive Help topics. The Help topics are retrieved from a Help server that is located on-premises.\n  The client also provides Role Centers, or role-based home pages, for users. Role Centers provide role-specific tasks, activities, alerts, reports, and business intelligence that help users increase their productivity. To interact with the Role Centers that are provided by Enterprise Portal and hosted on Internet Information Services (IIS), the client uses a browser control.\n  Services and AIF architecture  AX 2012 exposes its functionality through services that are based on Windows Communication Foundation (WCF) and hosted on Application Object Server (AOS). External applications and client applications on the local area network consume AX services by accessing them directly from AOS.\n   These clients and applications include AX components such as the AX client, Office Add-, and Enterprise Portal.\n  Internet-based external applications and clients access the AX services through Internet Information Services (IIS). IIS routes the incoming requests for AX services to AOS. All services requests, regardless of their origin, are handled by the WCF runtime that is hosted on AOS.\n  The AIF request preprocessor, if it is configured, can intercept the inbound request messages for custom preprocessing, such as message transforms or value substitutions. The AX service invokes the necessary business logic to process the inbound request message.\n  Similarly, the AIF response postprocessor, if it is configured, can intercept the outbound response messages for custom post-processing, such as message transforms or value substitutions. The AIF response postprocessor then returns the response to the client.\n  Enterprise Portal architecture This diagram provides a logical overview of a Microsoft Dynamics AX 2012 system with an Enterprise Portal server, and also describes the various components of the Enterprise Portal architecture.\nSecurity architecture This following diagram provides a high-level overview of the security architecture of Microsoft Dynamics AX 2012.\nWorkflow system architecture   This following diagram provides a high-level architecture of the workflow infrastructure.\n  The workflow infrastructure consists of two components that are hosted on Application Object Server (AOS): the X++ workflow run-time and the managed workflow run-time.\n  ![](/imagesposts/Workflow_system_arcitechture (1).gif)\nAnalytic architecture The following diagram shows the Microsoft SQL Server Analysis Services cubes that are included with Microsoft Dynamics AX, and the components that are used to access them.\nReporting architecture The following diagram illustrates the architecture of the reporting functionality in Microsoft Dynamics AX.\nThank you for reading!\n","permalink":"https://nuxulu.com/posts/2013-01-01-microsoft-dynamics-ax-2012-architecture-overview/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUnderstanding the internal architecture of Microsoft Dynamics AX 2012 can help you make decision when planning and developing a Microsoft Dynamics AX 2012 system. Here are some pointers on DAX 2012 architecture primarily for DAX 2012 architects \u0026amp; solution developers. This topic provides a high-level overview of the system architecture of Microsoft Dynamics AX.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"system-architecture\"\u003eSystem architecture\u003c/h2\u003e\n\u003cp\u003eThis diagram provides a high-level over of a Microsoft Dynamics AX 2012 system with all components installed, and describes how communications flow between the components.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/imagesposts/AXSystemArchitechture.gif\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Architecture overview Microsoft Dynamics AX 2012"},{"content":"I don’t know exactly how I ended up where I am. I mean, programming, and lovin’ it. Even if I’m not so good.\nI’ve heard a thousand of guys telling stories like: “When I was twelve my parents bought me my first computer and I started learning on my own.” or “I won a programming contest when I was in highschool”. Great. Well done. I’ve never noticed about what coding was until I was eighteen, when I had to decide what to study at the University.\nDo you want to know what was I thinking about? First option: Computer Science. Second option: Arts (wtf?). Third option: Philosophy (WTF?). I’ve ever liked computers, yes, but I’d never thought I could do that amazing things with them. I also liked maths, puzzles, things that made my brain think hard and get fun at the same time. You know, quizes, enigmas, games… The conclusion was that I wanted to do something creative, where I could put my imagination on, where I could challenge myself in order to create new stuff.\nMy first programming class was awful. I didn’t get nothing, and everyone seemed to be so cool on it, everyone with this big “Hello World” in their screens and a smile in their faces like ‘Yeah dude, I did it.”. I didn’t even know what to write down, or what was the teacher talking about. I was so frustrated, my first exam was… well, I don’t want to talk about my first exam.\nOne day, I don’t know how, something changed. Suddenly, I realized what programming was. I really abstracted my mind, I looked at it from another perspective, and it came with me so clear and so beautiful and I can not explain with words what I felt.\nWas it easier from then? Not at all. In fact, I left Computer Science after the first year, with almost all the subjects passed, because there was still something that made me think I was not made for it. I was so lost, I didn’t know what to do with my life. I really liked programming, but there were so much thinks that seemed to be so far away from me. And I felt that everyone around me was going good on it, it was so frustrating that I couldn’t stand it anymore.\nThen, I decided I had to change my situation because there should be something out there waiting for me. I looked for other degrees in a lot of universities.\nIt was. I do really love what I’m studying now, but the truth is that I think I’ve found my way. Actually, I will finish also Computer Science someday, but in a different place. And I’m still getting frustrated, of course, who doesn’t?, but I know I learned to love that frustration. Let me explain myself.\nProgramming is that. Is try it out thousand of times. Is try to find a mistake between a million lines of code, it’s to abstract your mind to find an answer, it’s… beautiful. I know really good programmers that honestly, I think I’ll never reach out, I have not that brilliant brain. But I keep on trying it, I’m not the best, I just love what I do.\nMaybe this post is not just about programming, maybe is more about to find out what do you really love. Even if sometimes you hate it, or it makes you cry, or it makes you feel so stupid because you are not able to understand it, or if it makes you think there is a lot of people smarter than you doing the same thing better. Just… enjoy your time, fight for what you want, challenge yourself to go to the next step.\n","permalink":"https://nuxulu.com/posts/2011-01-01-start-programming/","summary":"I don’t know exactly how I ended up where I am. I mean, programming, and lovin’ it. Even if I’m not so good.\nI’ve heard a thousand of guys telling stories like: “When I was twelve my parents bought me my first computer and I started learning on my own.” or “I won a programming contest when I was in highschool”. Great. Well done. I’ve never noticed about what coding was until I was eighteen, when I had to decide what to study at the University.","title":"Start programming"},{"content":"Base on JournalId on LedgerJournalTable you can use code below to post the transactions{% highlight csharp %} //Contract class [ DataMemberAttribute(\u0026lsquo;gJournalId\u0026rsquo;), SysOperationDisplayOrderAttribute(\u0026lsquo;1\u0026rsquo;) ] public LedgerJournalId parmJournalNum(LedgerJournalId _journalId = gJournalId) { gJournalId = _journalId;\nreturn gJournalId;\r }\n//Processing class [SysEntryPointAttribute] public void process(MAV_PostCustPaymentJourContract _contract) { LedgerJournalTable ledgerJournalTable; LedgerJournalCheckPost postCustPaymentJournal;\nledgerJournalTable = LedgerJournalTable::find(_contract.parmJournalNum());\rif (ledgerJournalTable)\r{\rpostCustPaymentJournal = LedgerJournalCheckPost::newLedgerJournalTable(ledgerJournalTable, NoYes::Yes);\rpostCustPaymentJournal.run();\r}\r } {% endhighlight %}\n","permalink":"https://nuxulu.com/posts/2016-12-26-posting-cust-payment-journal-using-x++/","summary":"Base on JournalId on LedgerJournalTable you can use code below to post the transactions{% highlight csharp %} //Contract class [ DataMemberAttribute(\u0026lsquo;gJournalId\u0026rsquo;), SysOperationDisplayOrderAttribute(\u0026lsquo;1\u0026rsquo;) ] public LedgerJournalId parmJournalNum(LedgerJournalId _journalId = gJournalId) { gJournalId = _journalId;\nreturn gJournalId;\r }\n//Processing class [SysEntryPointAttribute] public void process(MAV_PostCustPaymentJourContract _contract) { LedgerJournalTable ledgerJournalTable; LedgerJournalCheckPost postCustPaymentJournal;\nledgerJournalTable = LedgerJournalTable::find(_contract.parmJournalNum());\rif (ledgerJournalTable)\r{\rpostCustPaymentJournal = LedgerJournalCheckPost::newLedgerJournalTable(ledgerJournalTable, NoYes::Yes);\rpostCustPaymentJournal.run();\r}\r } {% endhighlight %}","title":"posting Cust payment journal using X++"}]