[{"content":"ODATA actions in Data Entities provide a way to inject behaviors into the data model, or expose custom business logic from Dynamics 365 Finance \u0026amp; Operations. You can add actions by adding a method to the data entity and then decorating the method with specific attributes [SysODataActionAttribute]\nI use this Odata actions mostly in automation job like after refreshing data from PROD to UAT, we need to enable users, assign company to users, enable batches \u0026hellip; Or simply consume it in Power Automate.\n1. Create an action to OData entity You can create a new entity following this standard docs or you can duplicate any standard entity. I created AutomationDataEntity. Right-click the enitity, select View code and add the code\npublic class AutomationDataEntity extends common { //1st example [SysODataActionAttribute(\u0026#34;assignUserToCompany\u0026#34;, false)] public static void assignUserToCompany(NetworkAlias _networkAlias, DataAreaName _company) { UserInfo userInfo; ttsbegin; select firstonly forupdate userInfo where userInfo.networkAlias == _networkAlias; userInfo.company = _company; userInfo.update(); ttscommit; } //2nd example [SysODataActionAttribute(\u0026#34;ReturnRental\u0026#34;, true)] public str ReturnRental() { return \u0026#34;Rental was successfully returned. Thanks for your business\u0026#34;; } //following 3rd example of an OData action takes in a parameter and returns a list [SysODataActionAttribute(\u0026#34;GetColors\u0026#34;, true), SysODataCollectionAttribute(\u0026#34;return\u0026#34;, Types::Record, \u0026#34;CarColor\u0026#34;)] public List GetColorsByAvailability(boolean onlyAvailableVehicles) { List returnList = new List(Types::Record); // do something  return returnList; } } In this example, the SysODataActionAttribute class decorates the assginUserToCompany method that is exposed as an action. The first argument of the attribute is the publicly exposed name of the action, and the second argument indicates whether this action need an entity instance or not. If you set the second argument to false, the method has to be static.\n You might need reset IIS service to update Odata endpoint.\n 2. Test Entity Odata actions with Postman and Power Automate 2.1. With Postman Please follow this document for basic configurations in Dynamics 365 Finance \u0026amp; Operation, Azure to work with Postman.\n2.1.1. Let\u0026rsquo;s use the first example. Specify Odata endpoint request with POST method into Postman application\n[finopsURL]/data/AutomationDatas/Microsoft.Dynamics.DataEntities.assignUserToCompany [finopsURL] = https://[yourenvironment].cloudax.dynamics.com\nHere is the Json file contains the parameters for assignUserToCompany method\n{ \u0026#34;_networkAlias\u0026#34;:\u0026#34;Max.Nguyen@Microsoft.com\u0026#34;, \u0026#34;_company\u0026#34;:\u0026#34;USMF\u0026#34; } Click Send and you will get your logic executed.\n2.1.2. Let\u0026rsquo;s try with the second example Everything should be remain the same, you just need to change the method to ReturnRental\n[finopsURL]/data/AutomationDatas/Microsoft.Dynamics.DataEntities.ReturnRental Click Send and you will get an error\n{ \u0026#34;Message\u0026#34;: \u0026#34;No HTTP resource was found that matches the request URI \u0026#39;https://[devaos].cloudax.dynamics.com/data/AutomationDatas/Microsoft.Dynamics.DataEntities.ReturnRental\u0026#39;. No route data was found for this request.\u0026#34; } The reason is that you set the second argument to true, that means you need an instance for AutomationDatas entity before you can use ReturnRental method. My entity created based on CustGroup table, so to get an instance I need DataAreaId and CustGroupID. The correct endpoint should be\n[finopsURL]/data/AutomationDatas(dataAreaId=\u0026#39;USMF\u0026#39;,CustomerGroupId=\u0026#39;BRIDGE\u0026#39;)/Microsoft.Dynamics.DataEntities.ReturnRental The result\n{ \u0026#34;@odata.context\u0026#34;: \u0026#34;https://[devaos].cloudax.dynamics.com/data/$metadata#Edm.String\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Rental was successfully returned. Thanks for your business\u0026#34; } 2.2. With Power Automate 2.2.1. With the first example Create a simple Power Automate with Dynamics 365 Finance \u0026amp; Operations connector, to consume Odata actions we use \u0026ldquo;Execute action\u0026rdquo; action following\n2.2.2. With the second example when specify action in Execute action, Dynamics 365 Finance \u0026amp; Operations connector understand that this needs an instance\n3. More In Odata actions, you can return a list\n[SysODataActionAttribute(\u0026#34;GetColors\u0026#34;, true), SysODataCollectionAttribute(\u0026#34;return\u0026#34;, Types::Record, \u0026#34;CarColor\u0026#34;)] public List GetColorsByAvailability(boolean onlyAvailableVehicles) { List returnList = new List(Types::Record); // do something  return returnList; } The following example of an OData action takes in list a parameter.\n[SysODataActionAttribute(\u0026#34;GetColorsByAvailability\u0026#34;, false), SysODataCollectionAttribute(\u0026#34;InventSiteIdList\u0026#34;, Types::String), SysODataCollectionAttribute(\u0026#34;return\u0026#34;, Types::String)] public static str GetColorsByAvailability(List InventSiteIdList) { str\tstrCommaSeperated; List list = new List(Types::String); ListEnumerator ListEnumerator; ListEnumerator = InventSiteIdList.getEnumerator(); while (ListEnumerator.moveNext()) { strCommaSeperated += strFmt(\u0026#39;%1, \u0026#39;, ListEnumerator.current()) ; } return strCommaSeperated; } In those examples,the SysODataCollectionAttribute class enables OData to expose strongly typed collections from X++. This class takes in three parameters:\n The name of the parameter that is a list (Use return for the return value of the method.). The X++ type of the members of this list. The public name of the OData resource that is contained in the collection.  You can find actions that are defined on data entities by searching for the SysODataActionAttribute attribute in metadatasearch.\nIf you want to check how many Odata actions available for an entity, you can go here and search for an entity.\nhttps://[devaos].cloudax.dynamics.com/data/$metadata \u0026lt;Action Name=\u0026#34;removeDeleteCT\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Collection(Microsoft.Dynamics.DataEntities.AutomationData)\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_entityName\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; \u0026lt;Action Name=\u0026#34;assginUserToCompany\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Collection(Microsoft.Dynamics.DataEntities.AutomationData)\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_networkAlias\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_company\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; \u0026lt;Action Name=\u0026#34;ReturnRental\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Microsoft.Dynamics.DataEntities.AutomationData\u0026#34;/\u0026gt; \u0026lt;ReturnType Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; \u0026lt;Action Name=\u0026#34;addToAllUserGroups\u0026#34; IsBound=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Parameter Name=\u0026#34;AutomationData\u0026#34; Type=\u0026#34;Collection(Microsoft.Dynamics.DataEntities.AutomationData)\u0026#34;/\u0026gt; \u0026lt;Parameter Name=\u0026#34;_userId\u0026#34; Type=\u0026#34;Edm.String\u0026#34;/\u0026gt; \u0026lt;/Action\u0026gt; At the time I\u0026rsquo;m writing this post, Odata actions does not support COC extension (A 18, P 42). So if you write some thing like this, it won\u0026rsquo;t work.\n[ExtensionOf(dataentityviewstr(CustCustomerGroupEntity))] final class CustCustomerGroupEntity_KA_Extension { [SysODataActionAttribute(\u0026#34;ReturnRental\u0026#34;, false)] public static str ReturnRental() { return \u0026#34;Rental was successfully returned. Thanks for your business\u0026#34;; } } Thank you for reading.\n","permalink":"https://nuxulu.com/posts/2021-05-25-all-about-odata-actions-in-dynamics-365-finance-and-operations/","summary":"ODATA actions in Data Entities provide a way to inject behaviors into the data model, or expose custom business logic from Dynamics 365 Finance \u0026amp; Operations. You can add actions by adding a method to the data entity and then decorating the method with specific attributes [SysODataActionAttribute]\nI use this Odata actions mostly in automation job like after refreshing data from PROD to UAT, we need to enable users, assign company to users, enable batches \u0026hellip; Or simply consume it in Power Automate.","title":"All about Odata actions in Dynamics 365 Finance And Operations"},{"content":"You\u0026rsquo;ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\nJekyll also offers powerful support for code snippets:\ndef print_hi(name) puts \u0026#34;Hi, #{name}\u0026#34; end print_hi(\u0026#39;Tom\u0026#39;) #=\u0026gt; prints \u0026#39;Hi, Tom\u0026#39; to STDOUT. Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.\n","permalink":"https://nuxulu.com/posts/2019-04-18-welcome-to-jekyll/","summary":"You\u0026rsquo;ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.","title":"Welcome to Jekyll!"},{"content":"This post has been written thanks to Joris de Gruyter‘s session in the past DynamicsCon: Azure Devops Automation for Finance and Operations Like You’ve Never Seen! And there’s also been some investigation and (a lot of) trial-and-error from my side until everything has been working.\nConfiguring the build VM in Azure DevTest Labs\nIf you want to know more about builds, releases, and the Dev ALM of Dynamics 365 you can read my full guide on MSDyn365 \u0026amp; Azure DevOps ALM.\nBut first… What I’m showing in this post is not a perfect blueprint. There’s a high probability that if you try exactly the same as I do here, you won’t get the same result. But it’s a good guide to get started and do some investigation on your own and learn.\nAzure DevTest Labs Azure DevTest Labs is an Azure tool/service that allows us to deploy virtual machines and integrate them with Azure DevOps pipelines, and many other things, but what I’m going to explain is just the VM and pipeline part.\nWhat will I show in this post? How to prepare a Dynamics 365 Finance and Operations VHD image to be used as the base to create a build virtual machine from an Azure DevOps pipeline, build our codebase, synchronize the DB, run tests, even deploy the reports, generate the deployable package and delete the VM.\nGetting and preparing the VHD This is by far the most tedious part of all the process because you need to download 11 ZIP files from LCS’ Shared Asset Library, and we all know how fast things download from LCS.\nHow is LCS download speed?\nAnd to speed it up we can create a blob storage account on Azure and once more turn to Mötz Jensen‘s d365fo.tools and use the Invoke-D365AzCopyTransfer cmdlet. Just go to LCS, click on the “Generate SAS link” button for each file, use it as the source parameter in the command and your blob SAS URL as the destination one. Once you have all the files in your blob you can download them to your local PC at a good speed.\nOnce you’ve unzipped the VHD you need to change it from Dynamic to Fixed using this PowerShell command:\nConvert-VHD –Path VHDLOCATION.vhd –DestinationPath NEWVHD.vhd –VHDType Fixed The reason is you can’t create an Azure VM from a dynamically-sized VHD. And it took me several attempts to notice this 🙂\nCreate a DevTest Labs account To do this part you need an Azure account. If you don’t have one you can sign up for a free Azure account with a credit of 180 Euros (200 US Dollars) to be spent during 30 days, plus many other free services during 12 months.\nSearch for DevTest Labs in the top bar and create a new DevTest Lab. Once it’s created open the details and you should see something like this:\nAzure DevTest Labs\nClick on the “Configuration and policies” menu item at the bottom of the list and scroll down in the menu until you see the “Virtual machine bases” section:\nDevTest Labs custom image\nAnd now comes the second funniest part of the process: we need to upload the 130GB VHD image to a blob storage account! So, click the “Add” button on top and in the new dialog that will open click the “Upload a VHD using PowerShell”. This will generate a PowerShell script to upload the VHD to the DevTest Labs blob. For example:\n\u0026lt;# Generated script to upload a local VHD to Azure. WARNING: The destination will be publicly available for 24 hours, after which it will expire. Ensure you complete your upload by then. Run the following command in a Azure PowerShell console after entering the LocalFilePath to your VHD. #\u0026gt; Add-AzureRmVhd \\-Destination \u0026#34;https://YOURBLOB.blob.core.windows.net/uploads/tempImage.vhd?sv=2019-07-07\u0026amp;st=2020-12-27T09%3A08%3A26Z\u0026amp;se=2020-12-28T09%3A23%3A26Z\u0026amp;sr=b\u0026amp;sp=rcw\u0026amp;sig=YTeXpxpVEJdSM7KZle71w8NVw9oznNizSnYj8Q3hngI%3D\u0026#34; \\-LocalFilePath \u0026#34;\u0026lt;Enter VHD location here\u0026gt;\u0026#34; DevTest Labs custom image upload\nAn alternative to this is using the Azure Storage Explorer as you can see in the image on the left.\nYou should upload the VHD to the uploads blob.\nAny of these methods is good to upload the VHD and I don’t really know which one is faster.\nOnce the VHD is uploaded open the “Custom images” option again and you should see the VHD in the drop-down:\nDevTest Labs custom image\nGive the image a name and click OK.\nWhat we have now is the base for a Dynamics 365 Finance and Operations dev VM which we need to prepare to use it as a build VM.\nCreating the VM We’ve got the essential, a VHD ready to be used as a base to create a virtual machine in Azure. Our next step is finding a way to make the deployment of this VM predictable and automated. We will attain this thanks to Azure ARM templates.\nGo back to your DevTest Labs overview page and click the “Add” button, on the “Choose base” page select the base you’ve just created, and on the next screen click on the “Add or Remove Artifacts” link:\nAdd artifacts to the VM\nSearch for WinRM, select “Configure WinRM”, and on the next screen enter “Shared IP address” as the hostname box and click “Add”.\nNote: if when the VM runs the artifacts can’t be installed check whether the Azure VM Agent is installed on the base VHD. Thanks to Joris for pointing this out!\n5.1. Configure Azure DevOps Agent Service 5.2. Option A: use an artifact Update: thanks to Florian Hopfner for reminding me this because I forgot… If you choose Option A to install the agent service you need to do some things first!\nThe first thing we need to do is running some PowerShell scripts that create registry entries and environment variables in the VM, go to C:\\DynamicsSDK and run these:\nImport-Module $(Join-Path \\-Path \u0026#34;C:\\\\DynamicsSDK\u0026#34; \\-ChildPath \u0026#34;DynamicsSDKCommon.psm1\u0026#34;) \\-Function \u0026#34;Write-Message\u0026#34;, \u0026#34;Set-AX7SdkRegistryValues\u0026#34;, \u0026#34;Set-AX7SdkEnvironmentVariables\u0026#34; Set\\-AX7SdkEnvironmentVariables \\-DynamicsSDK \u0026#34;C:\\\\DynamicsSDK\u0026#34; Set\\-AX7SdkRegistryValues \\-DynamicsSDK \u0026#34;c:\\\\DynamicsSDK\u0026#34; \\-TeamFoundationServerUrl \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-AosWebsiteName $AosWebsiteName \u0026#34;AosService\u0026#34; The first one will load the functions and make them available in the command-line and the other two create the registry entries and environment variables.\nNow we need to add an artifact for the Azure DevOps agent service. This will configure the agent service on the VM each time the VM is deployed. Search for “Azure Pipelines Agent” and click it. You will see this:\nDevTest Labs Azure DevOps Agent\nWe need to fill some information:\nOn “Azure DevOps Organization Name” you need to provide the name of your organization. For example if your AZDO URL is https://dev.azure.com/blackbeltcorp you need to use blackbeltcorp.\nOn “AZDO Personal Access Token” you need to provide a token generated from AZDO.\nOn “Agent Name” give your agent a name, like DevTestAgent. And on “Agent Pool” a name for your pool, a new like DevTestPool or an existing one as Default.\nOn “Account Name” use the same user that we’ll use in our pipeline later. Remember this. And on “Account Password” its password. Using secrets with a KeyVault is better, but I won’t explain this here.\nAnd, finally, set “Replace Agent” to true.\n5.3. Option B: Configure Azure DevOps Agent in the VM To do this you have to create a VM from the base image you created before and then go to C:\\DynamicsSDK and run the SetupBuildAgent script with the needed parameters:\nSetupBuildAgent.ps1 \\-VSO\\_ProjectCollection \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-ServiceAccountName \u0026#34;myUser\u0026#34; \\-ServiceAccountPassword \u0026#34;mYPassword\u0026#34; \\-AgentName \u0026#34;DevTestAgent\u0026#34; \\-AgentPoolName \u0026#34;DevTestPool\u0026#34; \\-VSOAccessToken \u0026#34;YOUR\\_VSTS\\_TOKEN\u0026#34; WARNING: If you choose option B you must create a new base image from the VM where you’ve run the script. Then repeat the WinRM steps to generate the new ARM template which we’ll see next.\n5.4. ARM template Then go to the “Advanced Settings” tab and click the “View ARM template” button:\nGet the ARM template\nThis will display the ARM template to create the VM from our pipeline. It’s something like this:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;newVMName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste001\u0026#34; }, \u0026#34;labName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aristeinfo\u0026#34; }, \u0026#34;size\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard\\_B4ms\u0026#34; }, \u0026#34;userName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;myUser\u0026#34; }, \u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\\[\\[\\[VmPassword\\]\\]\u0026#34; }, \u0026#34;Configure\\_WinRM\\_hostName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Public IP address\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;ariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_poolName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestPool\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;C\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: true } }, \u0026#34;variables\u0026#34;: { \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[concat(variables(\u0026#39;labVirtualNetworkName\u0026#39;), \u0026#39;Subnet\u0026#39;)\\]\u0026#34;, \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/virtualnetworks\u0026#39;, parameters(\u0026#39;labName\u0026#39;), variables(\u0026#39;labVirtualNetworkName\u0026#39;))\\]\u0026#34;, \u0026#34;labVirtualNetworkName\u0026#34;: \u0026#34;\\[concat(\u0026#39;Dtl\u0026#39;, parameters(\u0026#39;labName\u0026#39;))\\]\u0026#34;, \u0026#34;vmId\u0026#34;: \u0026#34;\\[resourceId (\u0026#39;Microsoft.DevTestLab/labs/virtualmachines\u0026#39;, parameters(\u0026#39;labName\u0026#39;), parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;\\[concat(parameters(\u0026#39;labName\u0026#39;), \u0026#39;/\u0026#39;, parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34; }, \u0026#34;resources\u0026#34;: \\[ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-15-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.DevTestLab/labs/virtualmachines\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmName\u0026#39;)\\]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;\\[resourceGroup().location\\]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[variables(\u0026#39;labVirtualNetworkId\u0026#39;)\\]\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;Dynamics365FnO10013AgentLessV2\u0026#34;, \u0026#34;customImageId\u0026#34;: \u0026#34;/subscriptions/6715778f-c852-453d-b6bb-907ac34f280f/resourcegroups/devtestlabs365/providers/microsoft.devtestlab/labs/devtestd365/customimages/dynamics365fno10013agentlessv2\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;\\[parameters(\u0026#39;size\u0026#39;)\\]\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;\\[parameters(\u0026#39;userName\u0026#39;)\\]\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\\[parameters(\u0026#39;password\u0026#39;)\\]\u0026#34;, \u0026#34;isAuthenticationWithSshKey\u0026#34;: false, \u0026#34;artifacts\u0026#34;: \\[ { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-winrm\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;hostName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Configure\\_WinRM\\_hostName\u0026#39;)\\]\u0026#34; } \\] }, { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-vsts-build-agent\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;vstsAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vstsPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentNameSuffix\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;poolName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_poolName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;RunAsAutoLogon\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;driveLetter\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;workDirectory\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;replaceAgent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#39;)\\]\u0026#34; } \\] } \\], \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[variables(\u0026#39;labSubnetName\u0026#39;)\\]\u0026#34;, \u0026#34;disallowPublicIpAddress\u0026#34;: true, \u0026#34;storageType\u0026#34;: \u0026#34;Premium\u0026#34;, \u0026#34;allowClaim\u0026#34;: false, \u0026#34;networkInterface\u0026#34;: { \u0026#34;sharedPublicIpAddressConfiguration\u0026#34;: { \u0026#34;inboundNatRules\u0026#34;: \\[ { \u0026#34;transportProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389 } \\] } } } } \\], \u0026#34;outputs\u0026#34;: { \u0026#34;labVMId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmId\u0026#39;)\\]\u0026#34; } } } NOTE: if you’re using option B you won’t have the artifact node for the VSTS agent.\nThis JSON file will be used as the base to create our VMs from the Azure DevOps pipeline. This is known as Infrastructure as Code (IaC) and it’s a way of defining our infrastructure in a file as it were code. It’s another part of the DevOps practice that should solve the “it works on my machine” issue.\nIf we take a look to the JSON’s parameters node there’s the following information:\n newVMName and labName will be the name of the VM and the DevTest Labs lab we’re using. The VM name is not really important because we’ll set the name later in the pipeline. size is the VM size, a D3 V2 in the example above, but we can change it and will do it later. userName \u0026amp; passWord will be the credentials to access the VM and must be the same we’ve used to configure the Azure DevOps agent. Configure_WinRM_hostName is the artifact we added to the VM template that will allow the pipelines to run in this machine.  To do it faster and for demo purposes I’m using a plain text password in the ARM template, changing the password node to something like this:\n\u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;yourPassword\u0026#34; }, I will do the same with all the secureString nodes, but you shouldn’t and should instead use an Azure KeyVault which comes with the DevTest Labs account.\nOf course you would never upload this template to Azure DevOps with a password in plain text. There’s plenty of resources online that teach how to use parameters, Azure KeyVault, etc. to accomplish this, for example this one: 6 Ways Passing Secrets to ARM Templates.\nOK, now grab that file and save it to your Azure DevOps repo. I’ve created a folder in my repo’s root called ARM where I’m saving all the ARM templates:\nARM templates on Azure DevOps\nPreparing the VM The VHD image you download can be used as a developer VM with no additional work, just run Visual Studio, connect it to your AZDO project and done. But if you want to use it as a build box you need to do several things first.\nRemember that the default user and password for these VHDs are Administrator and Pass@word1.\n6.1. Disable services First of all we will stop and disable services like the Batch, Management Reporter, SSAS, SSIS, etc. Anything you see that’s not needed to run a build.\n6.2. Create a new SQL user Open SSMS (as an Administrator) and create a new SQL user as a copy of the axdbadmin one. Then open the web.config file and update the DB user and password to use the one you’ve just created.\n6.3. Prepare SSRS (optional) If you want to deploy reports as part of your build pipeline you need to go to SSMS again (and as an Admin again), and open a new query in the reporting DB to execute the following query:\nexecDeleteEncryptedContent6.4. PowerShell Scripts The default build definition that runs on a build VM uses several PowerShell scripts to run some tasks. I’m adding an additional script called PrepareForAgent.\nThe scripts can be found in the C:\\DynamicsSDK folder of the VM.\n6.4.1. PrepareForBuild This script comes with the VM and we need to modify it to avoid one thing: the PackagesLocalDirectory backup which is usually done in the first build. We need to get rid of this or we’ll waste around an hour per run until the files are copied.\nWe don’t need this because our VM will be new each time we run the pipeline!\nSo open the script, go to line 696 and look for this piece of code:\n\\# Create packages backup (if it does not exist). $NewBackupCreated \\= Backup-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation \\# Restore packages backup (unless a new backup was just created). if (!$NewBackupCreated) { Restore-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation \\-RestoreAllFiles:$RestorePackagesAllFiles } if (!$DatabaseBackupToRestore) { $DatabaseBackupPath \\= Get-BackupPath \\-Purpose \u0026#34;Databases\u0026#34; Backup-AX7Database \\-BackupPath $DatabaseBackupPath } else { \\# Restore a database backup (if specified). Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We need to modify it until we end up with this:\nif ($DatabaseBackupToRestore) { Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We just need the DB restore part and skip the backup, otherwise we’ll be losing 45 minutes in each run for something we don’t need because the VM will be deleted when the build is completed.\n6.5. Optional (but recommended): install d365fo.tools Just run this:\nInstall-Module \\-Name d365fo.tools We can use the tools to do a module sync, partial sync or deploy just our reports instead of all.\nCreate a new image Once we’ve done all these prepare steps we can log out of this VM and stop it. Do not delete it yet! Go to “Create custom image”, give the new image a name, select “I have not generalized this virtual machine” and click the “OK” button.\nThis will generate a new image that you can use as a base image with all the changes you’ve done to the original VHD.\nAzure DevOps Pipelines We’re ready to setup our new build pipeline in Azure DevOps. This pipeline will consist of three steps: create a new VM, run all the build steps, and delete the VM:\nFirst of all check that your pipeline runs on Azure pipelines (aka Azure-hosted):\nDevTest Labs Azure Pipelines\nThe create and delete steps will run on the Azure Pipelines pool. The build step will run on our DevTestLabs pool, or the name you gave it when configuring the artifact on DevTest Labs or the script on the VM.\n8.1. Create Azure DevTest Labs VM Create a new pipeline and choose the “Use the classic editor” option. Make sure you’ve selected TFVC as your source and click “Continue” and “Empty job”. Add a new task to the pipeline, look for “Azure DevTest Labs Create VM”. We just need to fill in the missing parameters with our subscription, lab, etc.\nCreate VM Azure DevTest Labs\nRemember this step must run on the Azure-hosted pipeline.\n8.2. Build This is an easy one. Just export a working pipeline and import it. And this step needs to run on your self-hosted pool:\nRuns on self-hosted pool\n8.2.1. Optional: use SelectiveSync (not recommended, see next option) You can replace the Database Sync task for a PowerShell script that will only sync the tables in your models:\nSelectiveSync.ps1\nThanks Joris for the tip!\n8.2.2. Optional: use d365fo.tools to sync your packages/models This is a better option than the SelectiveSync above. You can synchronize your packages or models only to gain some time. This cmdlet uses sync.exe like Visual Studio does and should be better than SelectiveSync.\nAdd a new PowerShell task, select Inline Script and this is the command:\nInvoke-D365DbSyncModule \\-Module \u0026#34;Module1\u0026#34;, \u0026#34;Module2\u0026#34; \\-ShowOriginalProgress \\-Verbose 8.2.3. Optional: use d365fo.tools to deploy SSRS reports If you really want to add the report deployment step to your pipeline you can save some more extra time using d365fo.tools and just deploy the reports in your models like we’ve done with the DB sync.\nRun this in a new PowerShell task to do it:\nPublish-D365SsrsReport \\-Module YOUR\\_MODULE \\-ReportName \\* 8.3. Delete Azure DevTest Labs VM It’s almost the same as the create step, complete the subscription, lab and VM fields and done:\nDelete VM\nAnd this step, like the create one, will run on the Azure-hosted agent.\n8.4. Dependencies and conditions When all three steps are configured we need to add dependencies and conditions to some of them. For example, to make sure that the delete VM step runs when the build step fails, but it doesn’t when the create VM step fails.\n8.4.1. Build The build step depends on the create VM step, and will only run if the previous step succeeds:\nBuild step dependencies and conditions\n8.4.2. Delete VM The delete step depends on all previous steps and must run when the create VM step succeeds. If the create step fails there’s no VM and we don’t need to delete it:\nDependencies and conditions on delete VM step\nThis is the custom condition we’ll use:\nand(always(), eq(dependencies.Job\\_1.status, \u0026#39;Succeeded\u0026#39;)) If you need to know your first step’s job name just export the pipeline to YAML and you’ll find it there:\nExport pipeline to YAML\nJob name on YAML\nIf this step fails when the pipeline is run, wait to delete the VM manually, first change the VM name in the delete step, save your pipeline and then use the dropdown to show the VMs in the selected subscription, and save the pipeline.\nRun the build And, I think, we’re done and ready to run our Azure DevTest Labs pipeline for Dynamics 365 Finance and Operations… click “Run pipeline” and wait…\nTadaaaa!!\nTimes The pipeline from the image above is one with real code from a customer but I can’t compare the times with the Azure-hosted builds because there’s no sync, or tests there. Regarding the build time the Azure-hosted takes one minute less, but it needs to install the nugets first.\nBut for example this is a comparison I did:\nAzure DevTest Labs B2ms vs B4ms\nIt takes around 1 hour to create the VM, build, do a full DB synch, deploy reports, run tests, generate a Deployable Package and, finally, delete the VM:\nIf you skip deploying the SSRS reports your build will run in 15 minutes less, that’s around 45 minutes.\nIf you use the partial sync process instead of a full DB sync it’ll be 5-7 minutes less.\nThis would leave us with a 35-40 minutes build.\n10.1. Comparison 1 No DB Sync\nThe image above shows a simple package being compiled, without any table, so the selective sync goes really fast. The build times improve with VM size.\n10.2. Comparison 2 Same code Full DB Sync\nThis one is compiling the same codebase but is doing a full DB sync. The sync time improves in the B4ms VM compared to the B2ms but it’s almost the same in the B8ms. Build times are better for larger VM sizes.\n10.3. Comparison 3 Real code + full sync\nAnd in the image above we see a more realistic build. The codebase is larger and we’re doing a full DB sync.\nSimilar as the comparison before there a good enhancement between a B2ms and a B4ms, but not between a B4ms and B8ms.\nShow me the money! I think this is the interesting comparison. How did a Tier-1 MS-hosted build VM cost? Around 400€? How does it compare to using the Azure DevTest Labs alternative?\nThere’s only one fix cost when using Azure DevTest Labs: the blob storage where the VHD is uploaded. The VHD’s size is around 130GB and this should have a cost of, more or less, 5 euros/month. Keep in mind that you need to clean up your custom images when yours is prepared, the new ones are created as snapshots and also take space in the storage account.\nThen we have the variable costs that come with the deployment of a VM each build but it’s just absurd. Imagine we’re using a B4ms VM, with a 256GB Premium SSD disk, we would pay 0.18€/hour for the VM plus the proportional part of 35.26€/month of the SSD disk, which would be like 5 cents/hour?\nBut this can run on a B2ms VM too which is half the compute price of the VM, down to 9 cents per hour.\nIf we run this build once a day each month, 30 times, the cost of a B4ms would be like… 7€? Add the blob storage and we’ll be paying 12€ per month to run our builds with DB sync and tests.\nIs it cheaper than deploying a cloud-hosted environment, and starting and stopping it using the new d365fo.tools Cmdlets each time we run the build? Yes it is! Because if we deploy a CHE we’ll be paying the price of the SSD disk for the whole month!\nSome final remarks  I have accomplished this mostly through trial-and-error. There’s lots of enhancements and best practices to be applied to all the process, specially using an Azure Key Vault to store all the secrets to be used in the Azure DevOps Agent artifact and the pipeline. This in another clear example that X++ developers need to step outside of X++ and Dynamics 365 FnO. We’re not X++ only developers anymore, we’re very lucky to be working on a product that is using Azure. I’m sure there’s scenarios where using DevTest Labs to create a build VM is useful. Maybe not for an implementation partner, but maybe it is for an ISV partner. It’s just an additional option. The only bad thing to me is that we need to apply the version upgrades manually to the VHDs because they’re published only twice a year. As I said at the beginning of the post, it may have worked to me with all these steps, but if you try you maybe need to change some things. But it’s a good way to start.  ","permalink":"https://nuxulu.com/posts/2021-05-03-azure-devtest-labs-powered-builds-for-dynamics-365-finops/","summary":"This post has been written thanks to Joris de Gruyter‘s session in the past DynamicsCon: Azure Devops Automation for Finance and Operations Like You’ve Never Seen! And there’s also been some investigation and (a lot of) trial-and-error from my side until everything has been working.\nConfiguring the build VM in Azure DevTest Labs\nIf you want to know more about builds, releases, and the Dev ALM of Dynamics 365 you can read my full guide on MSDyn365 \u0026amp; Azure DevOps ALM.","title":"Azure DevTest Labs powered builds for Dynamics 365 FinOps"},{"content":"Dual-write has been around for almost two years now. It’s one of the ways of integrating Dynamics 365 Finance and Operations and Dataverse along with Virtual Entities.\nThe standard solution comes with many out-of-the-box entities available to synchronize. This has been one of the great improvements since Dual-write was made available in preview, when Juanan and I demoed it in the 2019 Dynamics Saturday in Madrid.\nThis is how Dual write really works\nBut what if we need to develop a new custom Data Entity in MSDyn365FO and use it in Dual-write? It’s easy but there’s some things we need to remember when doing it.\nDual-write Dual-write is a bidirectional integration that will synchronously write in Dataverse when data is created, updated or deleted in MSDyn365FO in near-real-time. On the Finance and Operations side it uses data entities to export data to Dataverse.\nRight now there’s a set of several OOB entities that come ready to be synchronized, and thanks to Initial Sync we can populate data in Dataverse choosing FnO as the source when starting the sync, or also choose Dataverse as the source.\nIf you want to learn more about Dual-write you can:\n Read the docs which have plenty of information. Read the docs. Always. Guidance for Dual-Write setup System requirements and prerequisites Watch some of Faisal Fareed‘s sessions about Dual-write: DynamicsCon 2020: The Power of Dual-write or Scottish Summit 2021: D365 FO integration with Dataverse – Dual write, Virtual Entities, OR Data Integrator. He’s got some more which you can find on Youtube.  Create the Data-entity In Visual Studio we need to create the entity from our table. I’ve created a new table called AASBookTable with just four fields: BookId, Author, Name and ISBN. Its primary key is the BookId field which is also its alternate key and will be used as natural key in the entity.\nNext, we create the data entity and make sure we’re marking the “Enable data management capabilities” checkbox:\nEnable data management capabilities must be checked\nIf the entity doesn’t have data management enabled it won’t be displayed in the list in the Dual-write tables setup.\nCreate a table in Dataverse Now we need to create a table in our Dataverse environment. This table must have at least some of the fields we want to synchronize to Dataverse AND a company field. The company concept doesn’t exist in Dataverse/CRM but thanks to the OOB mappings and Initial Sync we can solve this with just a few clicks and will have a company table in Dataverse with all our FnO legal entities.\nCompany field related to table company\nFollowing my example I’ve created a table with the same four fields and a company field with the data type Lookup and its related table Company, where the FnO legal entities are synchronized.\nAs I said, if we don’t create this field we won-t be able to setup Dual-write for this table.\nCreate table map Our table and data entity are ready, and now we need to create a mapping between them in the Dual-write workspace in FnO. Click the “Add table map” button:\nDynamics 365 Dual-write add table map\nA new dialog will open and we need to select the FnO entity and the Dataverse table:\nEntity map\nSelect the entity and table we’ve created and click save. Then we can define the field mapping:\nDynamics 365 Dual-write field maps\nBecause I’ve created both it’s clear what to map. And after doing this we can click save and it’s done, right? No! WRONG! If we do just this we’ll get an error, this error:\n Project validation failed. SourceEntitySchema: Books has a primaryCompanyField set to DataAreaId and DestinationEntitySchema: cr008_bookses doesn’t have primary company field set. Dual-write only supports mapping between cross-company entities or company-specific entities from both sides..keys are missing for either AX or CRM.keys are missing for either CRM or AX\n Or we can also get an error regarding a missing integration key for the company field. In the end both are caused because we’ve missed defining the integration key for this new Dataverse table:\nIntegration key Go back to the main Dual-write form and click on the “Integration key” button:\nDual-write integration key\nThe integration key will be the same as the FnO data entity key, plus the company if your data entity has a company context. Remember that when we create indexes in FnO the DataAreaId field isn’t included in the field, but it is in the SQL Server index along the partition field.\nThe integration key for our custom Dual-write mapping will look like this:\nDual/write integration key\nRemember we’ve added the company field to our Dataverse table? You can see in the image above that the field includes the relation to the Company table in Dataverse. We won’t be able to save the field mapping if we create the key using our Dataverse table’s company field instead of its Company table relation, like this:\nDual/write integration key\nSee the difference? In the first image the field reads c008_company.cdm_companycode while in the second one it’s c008_company. If we set the integration key using the field in our table instead of the related table and save the fields’ mapping we’ll get an error saying the company is missing in the key because it’s expecting the relation!\nAction! The table and field mappings are ready, just click run and go create a new book in the FnO form:\nFinance and Operations form\nThen we go to our Dataverse table and check its data…\nDataverse table data\nIt’s there! And of course it’s working in both directions. If I create a record in Dataverse it’ll be created in FnO too. I’ll use the Excel add-in to add a new book:\nDataverse Excel add-in\nAnd after refreshing the form in FnO we can see it there too:\nThis is a really simple example of how we can create a custom table, use it in a data entity and then use this entity in our dual-write setup. It’s something that can be easily done but we need to remember the “company thing”, otherwise this will never work!\nDual-write is even easier to configure nowadays thanks to LCS allowing us to create and link a new Dataverse environment when we deploy a new Finance and Operations environment.\n","permalink":"https://nuxulu.com/posts/2021-05-02-develop-custom-data-entities-for-dual-write/","summary":"Dual-write has been around for almost two years now. It’s one of the ways of integrating Dynamics 365 Finance and Operations and Dataverse along with Virtual Entities.\nThe standard solution comes with many out-of-the-box entities available to synchronize. This has been one of the great improvements since Dual-write was made available in preview, when Juanan and I demoed it in the 2019 Dynamics Saturday in Madrid.\nThis is how Dual write really works","title":"Develop custom Data Entities for Dual-write"},{"content":"Dynamics 365 for Finance \u0026amp; Operations and Azure DevOps 0.1.1. Azure DevOps Azure DevOps will be the service we will use for source control. Microsoft Dynamics 365 for Finance and Operations supports TFVC out of the box as its version-control system.\nBut Azure DevOps does not only offer a source control tool. Of course, developers will be the most benefited of using it, but from project management to the functional team and customers, everybody can be involved in using Azure DevOps. BPM synchronization and task creation, team planning, source control, automated builds and releases, are some of the tools it offers. All these changes will need some learning from the team, but in the short-term all of this will help to better manage implementations.\nAs I said it looks like the technical team is the most affected by the addition of source control to Visual Studio, but it’s the most benefited too…\n0.1.2. First steps To use all the features described in this guide we need to create an Azure DevOps project and connect it to LCS. This will be the first step and it’s mandatory so let’s see how we have to do everything.\n0.1.2.1. Create an Azure DevOps organization You might or might not have to do this. If you or your customer already have an account, you can use it and just create a new project in it. Otherwise head to https://dev.azure.com and create a new organization:\nAzure DevOps sign up\nAfter creating it you need to create a new project with the following options:\nCreate Azure DevOps project\nPress the “Create project” button and you’re done. Now let’s connect this Azure DevOps project to our LCS project.\nWhen a customer signs up for Finance and Operations the LCS project is of type “Implementation project” is created automatically. Your customers need to invite you to their project. If you’re an ISV you can use the “Migrate, create solutions, and learn” projects.\nIn any of both cases you need to go to “Project settings” and select the “Visual Studio Team Services” Tab. Scroll down and you should see two fields. Fill the field with your DevOps URL without the project part. If you got a https://dev.azure.com/YOUR_ORG URL type you need to change it to https://YOUR_ORG.visualstudio.com:\nAzure DevOps setup on LCS\nAnd to get the “Personal access token” we go back to our Azure DevOps project, click on the user settings icon, and then select “Personal access tokens”:\nWe add a new token, set its expiration and give it full access. Finally press the “Create” button and a new dialog will appear with your token, copy it, and paste it in LCS.\nBack to LCS, once you’ve pasted the token press the “Continue” button. On the next step just select your project, press “Continue” and finally “Save” on the last step.\nIf you have any problem you can take a look at the docs where everything is really well documented.\n0.1.3. The build server Once we’ve linked LCS and Azure DevOps we’ll have to deploy the build server. This will be the heart of our CI/CD processes.\nEven though the build virtual machine has the same topology as a developer box, it really isn’t a developer VM and should never be used as one, do not use it as a developer VM! It has Visual Studio installed in it, the AosService folder with all the standard packages and SQL Server with an AxDB, just like all other developer machines, but that’s not its purpose.\nWe won’t be using any of those features. The “heart” of the build machine is the build agent, an application which Azure DevOps uses to execute the build definition’s tasks from Azure DevOps.\nWe can also use Azure hosted build agents. Azure hosted agents allow us to run a build without a VM, the pipeline runs on Azure. We’ll see this later.\n0.1.3.1. The build VM This VM is usually the dev box on Microsoft’s subscription but you can also use a regular cloud-hosted environment as a build VM.\nWhen this VM is deployed there’s two things happening: the basic source code structure and the default build definition are created.\n0.1.4. Visual Studio We have the basics to start working. Log into your dev VM and start Visual Studio, we must map the Main folder to the development machine’s packages folder. Open the team explorer and select “Connect to a Project…”:\nIt will ask for your credentials and then show all projects available with the account you’ve used. Select the project we have created in the steps earlier and click on “Connect”:\nNow open the “Source Control Explorer”, select the Main folder and click on the “Not mapped” text:\nMap the Main folder to the K:\\AosService\\PackagesLocalDirectory folder on your service drive (this could be drive C if you’re using a local VM instead of a cloud-hosted environment):\nWhat we’ve done in this step is telling Visual Studio that what’s in our Azure DevOps project, inside the Main folder, will go into the K:\\AosService\\PackagesLocalDirectory folder of our development VM.\nThe Main folder we have in our source control tree is a regular folder, but we can convert it into a branch if we need it.\nIn the image above, you can see the icon for Main changes when it’s converted to a branch. Branches allow us to perform some actions that aren’t available to folders. Some differences can be seen in the context menu:\nFolder context menu\nBranch context menu\nFor instance, branches can display the hierarchy of all the project branches (in this case it’s only Main and Dev so it’s quite simple).\nProperties dialogs are different too. The folder one:\nAnd the branch one, where we can see the different relationships between the other branches created from Main:\nThis might be not that interesting or useful, but one of the things converting a folder into a branch is seeing where has a changeset been merge into.\n0.1.5. Some advice I strongly recommend moving the Projects folder out of the Main branch (or whatever you call it) into the root of the project, at the same level as BuildProcessTemplates and Trunk. In fact, and this is my personal preference, I would keep anything that’s not code outside of a branch. By doing this you only need to take care of the code when merging and branching.\nThose who have been working with AX for several years were used to not use version-control systems. MSDyn365FO has taken us to uncharted territory, so it is not uncommon for different teams to work in different ways, depending on their experience and what they’ve found in the path. Each team will need to invest some time to discover what’s better for them regarding code, branching and methodologies. Many times, this will be based on experimentation and trial and error, and with the pace of implementation projects trial and error turns out bad.\n0.1.6. Branching strategies I want to make it clear in advance: I’m not an expert in managing code nor Azure DevOps, at all. All that I’ve written here is product of my experience (good and bad) of over 4 years working with Finance and Operations. In this article on branching strategies from the docs there’s more information regarding branching and links to articles of the DevOps team. And there’s even more info in the DevOps Rangers’ Library of tooling and guidance solutions!\n0.1.6.1. Main-Release One possible strategy is using a Main and a Release branch. We have already learnt that the Main branch is created when the Build VM is deployed. The usual is that in an implementation project all development will be done on that branch until the Go Live, and just before that a new Release branch will be created.\nWe will keep development work on the Main branch, and when that passes validation, we’ll move it to Release. This branching strategy is really simple and will keep us mostly worry-free.\n0.1.6.2. Dev – Main – Release This strategy is similar to the Main – Release one but includes a Dev branch for each developer. This dev branch must be maintained by the developer using it. He can do as many check-ins as he wants during a development, and when it’s done merge all these changes to the Main branch in a single changeset. Of course, this adds some bureaucracy because we also need to forward integrate changes from Main into our Dev branch, but it will allow us to have a cleaner list of changesets when merging them from Main to the Release branch.\nWhatever branching strategy you choose try to avoid having pending changesets to be merged for a long time. The amount of merge conflicts that will appear is directly proportional to the time the changeset has been waiting to be merged.\nI wrote all of this based on my experience. It’s obviously not the same working for an ISV than for an implementation partner. An ISV has different needs, it must maintain different code versions to support all their customers and they don’t necessarily need to work in a Main – Release manner. They could have one (or more) branch for each version. However, since the end of overlayering this is not necessary. More ideas about this can be found in the article linked at the beginning.\nAzure Pipelines 0.2.1. Builds We’ve already seen that the default build definition has all the default steps active. We can disable (or remove) all the steps we’re not going to use. For example, the testing steps can be removed if we have no unit testing. We can also create new build definitions from scratch, however it’s easier to clone the default one and modify it to other branches or needs.\nSince version 8.1 all the X++ hotfixes are gone, the updates are applied in a single deployable package as binaries. This implies that the source-controlled Metadata folder will only contain our custom packages and models, no standard packages anymore.\n0.2.2. Continuous Integration Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. (source)\nShould your project/team use CI? Yes, yes, yes. This is one of the key feature of using an automated build process.\nThis is how a build definition for CI that will only compile our codebase looks like:\nOnly the prepare and build steps. Then we need to go to the “Triggers” tab and enable the CI option:\nRight after each developer check-in, a build will be queued, and the code compiled. In case there’s a compilation error we’ll be notified about it. Of course, we all build the solutions before checking them in and don’t need this CI build. Right?\n![tysonjaja](./MSDyn365 \u0026amp; Azure DevOps ALM - ariste.info_files/tysonjaja.gif \u0026ldquo;MSDyn365 \u0026amp; Azure DevOps ALM 23\u0026rdquo;)\nAnd because we all know that “Slow and steady wins the race”, but at some point during a project that’s not possible, so this kind of build definition can help us out. Especially when merging code between branches. This will allow us to be 100% sure when creating a DP to release to production that it’ll work. I can tell you that having to do a release to prod in a hurry and seeing the Main build failing is not nice.\n0.2.3. Gated check-ins A gated check-in is a bit different than a CI build. The gated check-in will trigger an automated build BEFORE checking-in the code. If it fails, the changeset is not cheked-in until the errors are fixed and checked-in again.\nThis option might seem perfect for the merge check-ins to the Main branch. I’ve found some issues trying to use it, for example:\n If multiple merges \u0026amp; check-ins from the same development are done and the first fails but the second doesn’t, you’ll still have pending merges to be done. You can try batching the builds, but I haven’t tried that. Issues with error notifications and pending code on dev VMs. If many check-ins are made, you’ll end up with lots of queued builds (and we only have one available agent per DevOps project). This can also be solved using the “Batch changes while a build is in progress”.  I think the CI option is working perfectly to validate code. As I’ve already said several times, choose the strategy that better suits your team and your needs. Experiment with CI and Gated check-in builds and decide what is better for you.\n0.2.4. Set up the new Azure DevOps tasks for Packaging and Model Versioning Almost all the tasks of the default build definition use PowerShell scripts that run on the Build VM. We can change 3 of those steps for newer tasks. In order to use these newer tasks, we need to install the “Dynamics 365 Unified Operations Tools”. We’ll be using them to set up our release pipeline too so consider doing it now.\n0.2.4.1. Update Model Version task This one is the easiest, just add it to your build definition under the current model versioning task, disable the original one and you’re done. If you have any filters in your current task, like excluding any model, you must add the filter in the Descriptor Search Pattern field using Azure DevOps pattern syntax.\n0.2.4.2. Create Deployable Package task This task will replace the Generate packages from the current build definitions. To set it up we just need to do a pair of changes to the default values:\n0.2.4.2.1. X++ Tools Path This is your build VM’s physical bin folder, the AosService folder is usually on the unit K for cloud-hosted VMs. I guess this will change when we go VM-less to do the builds.\nUpdate!: the route to the unit can be changed for $(ServiceDrive), getting a path like $(ServiceDrive)\\AOSService\\PackagesLocalDirectory\\bin.\n0.2.4.2.2. Location of the X++ binaries to package The task comes with this field filled in as $(Build.BinariesDirectory) but this didn’t work out for our build definitions, maybe the variable isn’t set up on the proj file. After changing this to $(Agent.BuildDirectory)\\Bin the package is generated.\n0.2.4.2.3. Filename and path for the deployable package The path on the image should be changed to $(Build.ArtifactStagingDirectory)\\Packages\\AXDeployableRuntime_$(Build.BuildNumber).zip. You can leave it without the Packages folder in the path, but if you do that you will need to change the Path to Publish field in the Publish Artifact: Package step of the definition.\n0.2.4.3. Add Licenses to Deployable Package task This task will add the license files to an existing Deployable Package. Remember that the path of the deployable package must be the same as the one in the Create Deployable Package task.\nAzure hosted build for Dynamics 365 Finance \u0026amp; SCM The day we’ve been waiting for has come! The Azure hosted builds are in public preview since PU35!! We can now stop asking Joris when will this be available, because it already is! Check the docs!\nI’ve been able to write this because, thanks to Antonio Gilabert, we’ve been testing this at Axazure for a few months with access to the private preview. And of course thanks to Joris for inviting us to the preview!\nRiding the Azure Pipelines by Caza Pelusas\nWhat does this mean? We no longer need a VM to run the build pipelines! Nah, we still need! If you’re running tests or synchronizing the DB as a part of your build pipeline you still need the VM. But we can move CI builds to the Azure hosted agent!\nYou can also read my full guide on MSDyn365FO \u0026amp; Azure DevOps ALM.\nRemember this is a public preview. If you want to join the preview you first need to be part of the Dynamics 365 Insider Program where you can join the “Dynamics 365 for Finance and Operations Insider Community“. Once invited you should see a new LCS project called PEAP Assets, and inside its Asset Library you’ll find the nugets in the Nuget package section.\n0.3.1. Azure agents With the capacity to run an extra Azure-hosted build we get another agent to run a pipeline and can run multiple pipelines at the same time. But it still won’t be parallel pipelines, because we only get one VM-less agent. This means we can run a self-hosted and azure hosted pipeline at the same time, but we cannot run two of the same type in parallel. If we want that we need to purchase extra agents.\nWith a private Azure DevOps project we get 2GB of Artifacts space (we’ll see that later) and one self-hosted and one Microsoft hosted agent with 1800 free minutes:\nAzure hosted build: Azure DevOps project pricing\nWe’ll still keep the build VM, so it’s difficult to tell a customer we need to pay extra money without getting rid of its cost. Plus we’ve been doing everything with one agent until now and it’s been fine, right? So take this like extra capacity, we can divide the build between both agents and leave the MS hosted one for short builds to squeeze the 1800 free minutes as much as possible.\n0.3.2. How does it work? There’s really no magic in this. We move from a self-hosted agent in the build VM to a Microsoft-hosted agent.\nThe Azure hosted build relies on nuget packages to compile our X++ code. The contents of the PackagesLocalDirectory folder, platform and the compiler tools have basically been put into nugets and what we have in the build VM is now on 3 nugets.\nWhen the build runs it downloads \u0026amp; installs the nugets and uses them to compile our code on the Azure hosted build along the standard packages.\n0.3.3. What do I need? To configure the Azure hosted build we need:\n  The 3 nuget packages from LCS: Compiler tools, Platform X++ and Application X++.\n  nuget.exe\n  A user with rights at the organization level to upload the nugets to Azure DevOps.\n  Some patience to get everything running 🙂\n  So the first step is going to the PEAP LCS’ Asset Library and downloading the 3 nuget packages:\nNugets for the Azure Hosted Build\n0.3.4. Azure DevOps artifact All of this can be done on your PC or in a dev VM, but you’ll need to add some files and a VS project to your source control so you need to use the developer box for sure.\nHead to your Azure DevOps project and go to the Artifacts section. Here we’ll create a new feed and give it a name:\nYou get 2GB for artifacts, the 3 nuget packages’ size is around 500MB, you should have no issues with space unless you have other artifacts in your project.\nNow press the “Connect to feed” button and select nuget.exe. You’ll find the instructions to continue there but I’ll explain it anyway.\nThen you need to download nuget.exe and put it in the Windows PATH. You can also get the nugets and nuget.exe in the same folder and forget about the PATH. Up to you. Finally, install the credential provider: download this Powershell script and run it. If the script keeps asking for your credentials and fails try adding -AddNetfx as a parameter. Thanks to Erik Norell for finding this and sharing in the comments of the original post!\nCreate a new file called nuget.config in the same folder where you’ve downloaded the nugets. It will have the content you can see in the “Connect to feed” page, something like this:\n\u0026lt;?xml version\\=\u0026#34;1.0\u0026#34; encoding\\=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;configuration\\\u0026gt; \u0026lt;packageSources\\\u0026gt; \u0026lt;clear /\\\u0026gt; \u0026lt;add key\\=\u0026#34;AASBuild\u0026#34; value\\=\u0026#34;https://pkgs.dev.azure.com/aariste/aariste365FO/\\_packaging/AASBuild/nuget/v3/index.json\u0026#34; /\\\u0026gt; \u0026lt;/packageSources\\\u0026gt; \u0026lt;/configuration\\\u0026gt; This file’s content has to be exactly the same as what’s displayed in your “Connect to feed” page.\nAnd finally, we’ll push (upload) the nugets to our artifacts feed. We have to do this for each one of the 3 nugets we’ve downloaded:\nnuget.exe push -Source \u0026#34;AASBuild\u0026#34; -ApiKey az \u0026lt;packagePath\u0026gt; You’ll get prompted for the user. Remember it needs to have enough rights on the project.\nOf course, you need to change “AASBuild” for your artifact feed name. And we’re done with the artifacts.\n0.3.5. Prepare Azure DevOps This new agent needs a solution to build our packages. This means we have to create an empty solution in Visual Studio and set the package of the project to our main package. Like this:\nVisual Studio solution\nIf you have more than one package or models, you need to add a project to this solution for each separate model you have.\nWe have to create another file called packages.config with the following content:\n\u0026lt;?xml version\\=\u0026#34;1.0\u0026#34; encoding\\=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;packages\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;7.0.5644.16778\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Application.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;10.0.464.13\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.CompilerPackage\u0026#34; version\\=\u0026#34;7.0.5644.16778\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\u0026gt; \u0026lt;/packages\u0026gt; The version tag will depend on when you’re reading this, but the one above is the correct one for PU35. We’ll need to update this file each time a new version of the nugets is published.\nAnd, to end with this part, we need to add the solution, the nuget.config and the packages.config files to TFVC. This is what I’ve done:\nAzure DevOps\nYou can see I’ve created a Build folder in the root of my DevOps project. That’s only my preference, but I like to only have code in my branches, even the projects are outside of the branches, I only want the code to move between merges and branches. Place the files and solution inside the Build folder (or wherever you decide).\n0.3.6. Configure pipeline Now we need to create a new pipeline, you can just import this template from the newly created X++ (Dynamics 365) Samples and Tools Github project. After importing the template we’ll modify it a bit. Initially, it will look like this:\nAzure hosted build: Default imported pipeline\nAs you can see the pipeline has all the steps needed to generate the DP, but some of them, the ones contained in the Dynamics 365 tasks, won’t load correctly after the import. You just need to add those steps to your pipeline manually and complete its setup.\n0.3.6.1. Pipeline root You need to select the Hosted Azure Pipelines for the Agent pool, and vs2017-win2016 as Agent Specification.\n0.3.6.2. Get sources Azure hosted build: Our mappings\nI’ve mapped 2 things here: our codebase in the first mapping and the Build folder where I’ve added the solution and config files. If you’ve placed these files inside your Metadata folder you don’t need the extra mapping.\n0.3.6.3. NuGet install Packages This step gets the nugets from our artifacts feeds and the installs to be used in each pipeline execution.\nAzure hosted build: nuget install\nThe command uses the config files we have uploaded to the Build folder, and as you can see it’s fetching the files from the $(build.sourcesDirectory)\\Build directory we’ve configured in the Get sources step. If you’ve placed those files in a diferent place you need to change the paths as needed.\n0.3.6.4. Update Model Version This is one of the steps that are displaying issues even though I got the Dynamics 365 tools installed from the Azure DevOps marketplace. If you got it right you probably don’t need to change anything. If you have the same issue as me, just add a new step and select the “Update Model Version” task and change the fields so it looks like this:\nAzure hosted build: Update Model Version\n0.3.6.5. Build solution Build solution step\nIn the build solution step, you have a wildcard in the solution field: **\\\\*.sln. If you leave this wildcard it will build all the projects you have in the repo and, depending on the number of projects you have, the build could time out.\nI solve this by selecting a solution, that contains all the models I have, that I have placed in the Build folder in my repo, and update that solution if you add or remove any model.\nThanks to Ievgen Miroshnikov for pointing this out!\nThere could be an additional issue with the rnrproj files as Josh Williams points out in a comment. If your project was created pre-PU27 try creating a new solution to avoid problems.\n0.3.6.6. Create Deployable Package This is another one of the steps that are not loading correctly for me. Again, add it and change as needed:\nAzure hosted build: Create Deployable Package\n0.3.6.7. Add Licenses to Deployable Package Another step with issues. Do the same as with the others:\nAzure hosted build: Add Licenses to Deployable Package\nAnd that’s all. You can queue the build to test if it’s working. For the first runs you can disable the steps after the “Build solution” one to see if the nugets are downloaded correctly and your code built. After that try generating the DP and publishing the artifact.\nYou’ve configured your Azure hosted build, now it’s your turn to decide in which cases will you use the self-hosted or the azure hosted build.\n0.3.7. Update for version 10.0.18 Since version 10.0.18 we’ll be getting 4 NuGet packages instead of 3 because of the Microsoft.Dynamics.AX.Application.DevALM.BuildXpp NuGet size is getting near or over the max size which is 500MB and will come as 2 NuGet packages from now on.\nYou can read about this in the docs.\nThere just 2 small changes we need to do to the pipeline if we’re already using it, one to the packages.config file and another one to the pipeline.\n0.3.7.1. packages.config The packages.config file will have an additional line for the Application Suite NuGet.\n\u0026lt;?xml version\\=\u0026#34;1.0\u0026#34; encoding\\=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;packages\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;7.0.5968.16973\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Application.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;10.0.793.16\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.ApplicationSuite.DevALM.BuildXpp\u0026#34; version\\=\u0026#34;10.0.793.16\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;package id\\=\u0026#34;Microsoft.Dynamics.AX.Platform.CompilerPackage\u0026#34; version\\=\u0026#34;7.0.5968.16973\u0026#34; targetFramework\\=\u0026#34;net40\u0026#34; /\\\u0026gt; \u0026lt;/packages\\\u0026gt; 0.3.7.2. Pipeline We need to add a new variable to the pipeline variables called AppSuitePackage with the value Microsoft.Dynamics.AX.ApplicationSuite.DevALM.BuildXpp.\nNew Azure DevOps pipeline variable\nAnd then use it in the build step and change it to:\n/p:BuildTasksDirectory\\=\u0026#34;$(NugetsPath)\\\\$(ToolsPackage)\\\\DevAlm\u0026#34; /p:MetadataDirectory\\=\u0026#34;$(MetadataPath)\u0026#34; /p:FrameworkDirectory\\=\u0026#34;$(NuGetsPath)\\\\$(ToolsPackage)\u0026#34; /p:ReferenceFolder\\=\u0026#34;$(NuGetsPath)\\\\$(PlatPackage)\\\\ref\\\\net40;$(NuGetsPath)\\\\$(AppPackage)\\\\ref\\\\net40;$(MetadataPath);$(Build.BinariesDirectory);$(NuGetsPath)\\\\$(AppSuitePackage)\\\\ref\\\\net40\u0026#34; /p:ReferencePath\\=\u0026#34;$(NuGetsPath)\\\\$(ToolsPackage)\u0026#34; /p:OutputDirectory\\=\u0026#34;$(Build.BinariesDirectory)\u0026#34; Azure DevTest Labs powered builds The end of Tier-1 Microsoft-managed build VMs is near, and this will leave us without the capacity to synchronize the DB or run tests in a pipeline, unless we deploy a new build VM in our, or our customer’s, Azure subscription. Of course, there might be a cost concern with it, and there’s where Azure DevTest Labs can help us!\nThis post has been written thanks to Joris de Gruyter‘s session in the past DynamicsCon: Azure Devops Automation for Finance and Operations Like You’ve Never Seen! And there’s also been some investigation and (a lot of) trial-and-error from my side until everything has been working.\nConfiguring the build VM in Azure DevTest Labs\nIf you want to know more about builds, releases, and the Dev ALM of Dynamics 365 you can read my full guide on MSDyn365 \u0026amp; Azure DevOps ALM.\n0.4.1. But first… What I’m showing in this post is not a perfect blueprint. There’s a high probability that if you try exactly the same as I do here, you won’t get the same result. But it’s a good guide to get started and do some investigation on your own and learn.\n0.4.2. Azure DevTest Labs Azure DevTest Labs is an Azure tool/service that allows us to deploy virtual machines and integrate them with Azure DevOps pipelines, and many other things, but what I’m going to explain is just the VM and pipeline part.\nWhat will I show in this post? How to prepare a Dynamics 365 Finance and Operations VHD image to be used as the base to create a build virtual machine from an Azure DevOps pipeline, build our codebase, synchronize the DB, run tests, even deploy the reports, generate the deployable package and delete the VM.\n0.4.3. Getting and preparing the VHD This is by far the most tedious part of all the process because you need to download 11 ZIP files from LCS’ Shared Asset Library, and we all know how fast things download from LCS.\nHow is LCS download speed?\nAnd to speed it up we can create a blob storage account on Azure and once more turn to Mötz Jensen‘s d365fo.tools and use the Invoke-D365AzCopyTransfer cmdlet. Just go to LCS, click on the “Generate SAS link” button for each file, use it as the source parameter in the command and your blob SAS URL as the destination one. Once you have all the files in your blob you can download them to your local PC at a good speed.\nOnce you’ve unzipped the VHD you need to change it from Dynamic to Fixed using this PowerShell command:\nConvert-VHD –Path VHDLOCATION.vhd –DestinationPath NEWVHD.vhd –VHDType Fixed The reason is you can’t create an Azure VM from a dynamically-sized VHD. And it took me several attempts to notice this 🙂\n0.4.4. Create a DevTest Labs account To do this part you need an Azure account. If you don’t have one you can sign up for a free Azure account with a credit of 180 Euros (200 US Dollars) to be spent during 30 days, plus many other free services during 12 months.\nSearch for DevTest Labs in the top bar and create a new DevTest Lab. Once it’s created open the details and you should see something like this:\nAzure DevTest Labs\nClick on the “Configuration and policies” menu item at the bottom of the list and scroll down in the menu until you see the “Virtual machine bases” section:\nDevTest Labs custom image\nAnd now comes the second funniest part of the process: we need to upload the 130GB VHD image to a blob storage account! So, click the “Add” button on top and in the new dialog that will open click the “Upload a VHD using PowerShell”. This will generate a PowerShell script to upload the VHD to the DevTest Labs blob. For example:\n\u0026lt;# Generated script to upload a local VHD to Azure. WARNING: The destination will be publicly available for 24 hours, after which it will expire. Ensure you complete your upload by then. Run the following command in a Azure PowerShell console after entering the LocalFilePath to your VHD. #\u0026gt; Add-AzureRmVhd -Destination \u0026#34;https://YOURBLOB.blob.core.windows.net/uploads/tempImage.vhd?sv=2019-07-07\u0026amp;st=2020-12-27T09%3A08%3A26Z\u0026amp;se=2020-12-28T09%3A23%3A26Z\u0026amp;sr=b\u0026amp;sp=rcw\u0026amp;sig=YTeXpxpVEJdSM7KZle71w8NVw9oznNizSnYj8Q3hngI%3D\u0026#34; -LocalFilePath \u0026#34;\u0026lt;Enter VHD location here\u0026gt;\u0026#34; Generated script to upload a local VHD to Azure.\nWARNING: The destination will be publicly available for 24 hours, after which it will expire.\nEnsure you complete your upload by then.\nRun the following command in a Azure PowerShell console after entering\nthe LocalFilePath to your VHD.\nAdd-AzureRmVhd \\-Destination \u0026#34;https://YOURBLOB.blob.core.windows.net/uploads/tempImage.vhd?sv=2019-07-07\u0026amp;st=2020-12-27T09%3A08%3A26Z\u0026amp;se=2020-12-28T09%3A23%3A26Z\u0026amp;sr=b\u0026amp;sp=rcw\u0026amp;sig=YTeXpxpVEJdSM7KZle71w8NVw9oznNizSnYj8Q3hngI%3D\u0026#34; \\-LocalFilePath \u0026#34;\u0026lt;Enter VHD location here\u0026gt;\u0026#34; DevTest Labs custom image upload\nAn alternative to this is using the Azure Storage Explorer as you can see in the image on the left.\nYou should upload the VHD to the uploads blob.\nAny of these methods is good to upload the VHD and I don’t really know which one is faster.\nOnce the VHD is uploaded open the “Custom images” option again and you should see the VHD in the drop-down:\nDevTest Labs custom image\nGive the image a name and click OK.\nWhat we have now is the base for a Dynamics 365 Finance and Operations dev VM which we need to prepare to use it as a build VM.\n0.4.5. Creating the VM We’ve got the essential, a VHD ready to be used as a base to create a virtual machine in Azure. Our next step is finding a way to make the deployment of this VM predictable and automated. We will attain this thanks to Azure ARM templates.\nGo back to your DevTest Labs overview page and click the “Add” button, on the “Choose base” page select the base you’ve just created, and on the next screen click on the “Add or Remove Artifacts” link:\nAdd artifacts to the VM\nSearch for WinRM, select “Configure WinRM”, and on the next screen enter “Shared IP address” as the hostname box and click “Add”.\nNote: if when the VM runs the artifacts can’t be installed check whether the Azure VM Agent is installed on the base VHD. Thanks to Joris for pointing this out!\n0.4.5.1. Configure Azure DevOps Agent Service 0.4.5.1.1. Option A: use an artifact Update: thanks to Florian Hopfner for reminding me this because I forgot… If you choose Option A to install the agent service you need to do some things first!\nThe first thing we need to do is running some PowerShell scripts that create registry entries and environment variables in the VM, go to C:\\DynamicsSDK and run these:\nImport-Module $(Join-Path \\-Path \u0026#34;C:\\\\DynamicsSDK\u0026#34; \\-ChildPath \u0026#34;DynamicsSDKCommon.psm1\u0026#34;) \\-Function \u0026#34;Write-Message\u0026#34;, \u0026#34;Set-AX7SdkRegistryValues\u0026#34;, \u0026#34;Set-AX7SdkEnvironmentVariables\u0026#34; Set\\-AX7SdkEnvironmentVariables \\-DynamicsSDK \u0026#34;C:\\\\DynamicsSDK\u0026#34; Set\\-AX7SdkRegistryValues \\-DynamicsSDK \u0026#34;c:\\\\DynamicsSDK\u0026#34; \\-TeamFoundationServerUrl \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-AosWebsiteName $AosWebsiteName \u0026#34;AosService\u0026#34; The first one will load the functions and make them available in the command-line and the other two create the registry entries and environment variables.\nNow we need to add an artifact for the Azure DevOps agent service. This will configure the agent service on the VM each time the VM is deployed. Search for “Azure Pipelines Agent” and click it. You will see this:\nDevTest Labs Azure DevOps Agent\nWe need to fill some information:\nOn “Azure DevOps Organization Name” you need to provide the name of your organization. For example if your AZDO URL is https://dev.azure.com/blackbeltcorp you need to use blackbeltcorp.\nOn “AZDO Personal Access Token” you need to provide a token generated from AZDO.\nOn “Agent Name” give your agent a name, like DevTestAgent. And on “Agent Pool” a name for your pool, a new like DevTestPool or an existing one as Default.\nOn “Account Name” use the same user that we’ll use in our pipeline later. Remember this. And on “Account Password” its password. Using secrets with a KeyVault is better, but I won’t explain this here.\nAnd, finally, set “Replace Agent” to true.\n0.4.5.1.2. Option B: Configure Azure DevOps Agent in the VM To do this you have to create a VM from the base image you created before and then go to C:\\DynamicsSDK and run the SetupBuildAgent script with the needed parameters:\nSetupBuildAgent.ps1 \\-VSO\\_ProjectCollection \u0026#34;https://dev.azure.com/YOUR\\_ORG\u0026#34; \\-ServiceAccountName \u0026#34;myUser\u0026#34; \\-ServiceAccountPassword \u0026#34;mYPassword\u0026#34; \\-AgentName \u0026#34;DevTestAgent\u0026#34; \\-AgentPoolName \u0026#34;DevTestPool\u0026#34; \\-VSOAccessToken \u0026#34;YOUR\\_VSTS\\_TOKEN\u0026#34; WARNING: If you choose option B you must create a new base image from the VM where you’ve run the script. Then repeat the WinRM steps to generate the new ARM template which we’ll see next.\n0.4.5.2. ARM template Then go to the “Advanced Settings” tab and click the “View ARM template” button:\nGet the ARM template\nThis will display the ARM template to create the VM from our pipeline. It’s something like this:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;newVMName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste001\u0026#34; }, \u0026#34;labName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aristeinfo\u0026#34; }, \u0026#34;size\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard\\_B4ms\u0026#34; }, \u0026#34;userName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;myUser\u0026#34; }, \u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\\[\\[\\[VmPassword\\]\\]\u0026#34; }, \u0026#34;Configure\\_WinRM\\_hostName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Public IP address\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;ariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_poolName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestPool\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;aariste\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;C\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;DevTestAgent\u0026#34; }, \u0026#34;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: true } }, \u0026#34;variables\u0026#34;: { \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[concat(variables(\u0026#39;labVirtualNetworkName\u0026#39;), \u0026#39;Subnet\u0026#39;)\\]\u0026#34;, \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/virtualnetworks\u0026#39;, parameters(\u0026#39;labName\u0026#39;), variables(\u0026#39;labVirtualNetworkName\u0026#39;))\\]\u0026#34;, \u0026#34;labVirtualNetworkName\u0026#34;: \u0026#34;\\[concat(\u0026#39;Dtl\u0026#39;, parameters(\u0026#39;labName\u0026#39;))\\]\u0026#34;, \u0026#34;vmId\u0026#34;: \u0026#34;\\[resourceId (\u0026#39;Microsoft.DevTestLab/labs/virtualmachines\u0026#39;, parameters(\u0026#39;labName\u0026#39;), parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;\\[concat(parameters(\u0026#39;labName\u0026#39;), \u0026#39;/\u0026#39;, parameters(\u0026#39;newVMName\u0026#39;))\\]\u0026#34; }, \u0026#34;resources\u0026#34;: \\[ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-15-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.DevTestLab/labs/virtualmachines\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmName\u0026#39;)\\]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;\\[resourceGroup().location\\]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;\\[variables(\u0026#39;labVirtualNetworkId\u0026#39;)\\]\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;Dynamics365FnO10013AgentLessV2\u0026#34;, \u0026#34;customImageId\u0026#34;: \u0026#34;/subscriptions/6715778f-c852-453d-b6bb-907ac34f280f/resourcegroups/devtestlabs365/providers/microsoft.devtestlab/labs/devtestd365/customimages/dynamics365fno10013agentlessv2\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;\\[parameters(\u0026#39;size\u0026#39;)\\]\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;\\[parameters(\u0026#39;userName\u0026#39;)\\]\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\\[parameters(\u0026#39;password\u0026#39;)\\]\u0026#34;, \u0026#34;isAuthenticationWithSshKey\u0026#34;: false, \u0026#34;artifacts\u0026#34;: \\[ { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-winrm\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;hostName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Configure\\_WinRM\\_hostName\u0026#39;)\\]\u0026#34; } \\] }, { \u0026#34;artifactId\u0026#34;: \u0026#34;\\[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;public repo\u0026#39;, \u0026#39;windows-vsts-build-agent\u0026#39;)\\]\u0026#34;, \u0026#34;parameters\u0026#34;: \\[ { \u0026#34;name\u0026#34;: \u0026#34;vstsAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vstsPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_vstsPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agentNameSuffix\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_agentNameSuffix\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;poolName\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_poolName\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;RunAsAutoLogon\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_RunAsAutoLogon\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonAccount\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonAccount\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;windowsLogonPassword\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_windowsLogonPassword\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;driveLetter\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_driveLetter\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;workDirectory\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_workDirectory\u0026#39;)\\]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;replaceAgent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[parameters(\u0026#39;Azure\\_Pipelines\\_Agent\\_replaceAgent\u0026#39;)\\]\u0026#34; } \\] } \\], \u0026#34;labSubnetName\u0026#34;: \u0026#34;\\[variables(\u0026#39;labSubnetName\u0026#39;)\\]\u0026#34;, \u0026#34;disallowPublicIpAddress\u0026#34;: true, \u0026#34;storageType\u0026#34;: \u0026#34;Premium\u0026#34;, \u0026#34;allowClaim\u0026#34;: false, \u0026#34;networkInterface\u0026#34;: { \u0026#34;sharedPublicIpAddressConfiguration\u0026#34;: { \u0026#34;inboundNatRules\u0026#34;: \\[ { \u0026#34;transportProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389 } \\] } } } } \\], \u0026#34;outputs\u0026#34;: { \u0026#34;labVMId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\\[variables(\u0026#39;vmId\u0026#39;)\\]\u0026#34; } } } NOTE: if you’re using option B you won’t have the artifact node for the VSTS agent.\nThis JSON file will be used as the base to create our VMs from the Azure DevOps pipeline. This is known as Infrastructure as Code (IaC) and it’s a way of defining our infrastructure in a file as it were code. It’s another part of the DevOps practice that should solve the “it works on my machine” issue.\nIf we take a look to the JSON’s parameters node there’s the following information:\n newVMName and labName will be the name of the VM and the DevTest Labs lab we’re using. The VM name is not really important because we’ll set the name later in the pipeline. size is the VM size, a D3 V2 in the example above, but we can change it and will do it later. userName \u0026amp; passWord will be the credentials to access the VM and must be the same we’ve used to configure the Azure DevOps agent. Configure_WinRM_hostName is the artifact we added to the VM template that will allow the pipelines to run in this machine.  To do it faster and for demo purposes I’m using a plain text password in the ARM template, changing the password node to something like this:\n\u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;yourPassword\u0026#34; }, I will do the same with all the secureString nodes, but you shouldn’t and should instead use an Azure KeyVault which comes with the DevTest Labs account.\nOf course you would never upload this template to Azure DevOps with a password in plain text. There’s plenty of resources online that teach how to use parameters, Azure KeyVault, etc. to accomplish this, for example this one: 6 Ways Passing Secrets to ARM Templates.\nOK, now grab that file and save it to your Azure DevOps repo. I’ve created a folder in my repo’s root called ARM where I’m saving all the ARM templates:\nARM templates on Azure DevOps\n0.4.6. Preparing the VM The VHD image you download can be used as a developer VM with no additional work, just run Visual Studio, connect it to your AZDO project and done. But if you want to use it as a build box you need to do several things first.\nRemember that the default user and password for these VHDs are Administrator and Pass@word1.\n0.4.6.1. Disable services First of all we will stop and disable services like the Batch, Management Reporter, SSAS, SSIS, etc. Anything you see that’s not needed to run a build.\n0.4.6.2. Create a new SQL user Open SSMS (as an Administrator) and create a new SQL user as a copy of the axdbadmin one. Then open the web.config file and update the DB user and password to use the one you’ve just created.\n0.4.6.3. Prepare SSRS (optional) If you want to deploy reports as part of your build pipeline you need to go to SSMS again (and as an Admin again), and open a new query in the reporting DB to execute the following query:\nexecDeleteEncryptedContent0.4.6.4. PowerShell Scripts The default build definition that runs on a build VM uses several PowerShell scripts to run some tasks. I’m adding an additional script called PrepareForAgent.\nThe scripts can be found in the C:\\DynamicsSDK folder of the VM.\n0.4.6.4.1. PrepareForBuild This script comes with the VM and we need to modify it to avoid one thing: the PackagesLocalDirectory backup which is usually done in the first build. We need to get rid of this or we’ll waste around an hour per run until the files are copied.\nWe don’t need this because our VM will be new each time we run the pipeline!\nSo open the script, go to line 696 and look for this piece of code:\n# Create packages backup (if it does not exist). $NewBackupCreated \\= Backup-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation # Restore packages backup (unless a new backup was just created). if (!$NewBackupCreated) { Restore-AX7Packages \\-BackupPath $PackagesBackupPath \\-DeploymentPackagesPath $DeploymentPackagesPath \\-LogLocation $LogLocation \\-RestoreAllFiles:$RestorePackagesAllFiles } if (!$DatabaseBackupToRestore) { $DatabaseBackupPath \\= Get-BackupPath \\-Purpose \u0026#34;Databases\u0026#34; Backup-AX7Database \\-BackupPath $DatabaseBackupPath } else { # Restore a database backup (if specified). Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We need to modify it until we end up with this:\nif ($DatabaseBackupToRestore) { Restore-AX7Database \\-DatabaseBackupToRestore $DatabaseBackupToRestore } We just need the DB restore part and skip the backup, otherwise we’ll be losing 45 minutes in each run for something we don’t need because the VM will be deleted when the build is completed.\n0.4.6.5. Optional (but recommended): install d365fo.tools Just run this:\nInstall-Module -Name d365fo.tools We can use the tools to do a module sync, partial sync or deploy just our reports instead of all.\n0.4.7. Create a new image Once we’ve done all these prepare steps we can log out of this VM and stop it. Do not delete it yet! Go to “Create custom image”, give the new image a name, select “I have not generalized this virtual machine” and click the “OK” button.\nThis will generate a new image that you can use as a base image with all the changes you’ve done to the original VHD.\n0.4.8. Azure DevOps Pipelines We’re ready to setup our new build pipeline in Azure DevOps. This pipeline will consist of three steps: create a new VM, run all the build steps, and delete the VM:\nFirst of all check that your pipeline runs on Azure pipelines (aka Azure-hosted):\nDevTest Labs Azure Pipelines\nThe create and delete steps will run on the Azure Pipelines pool. The build step will run on our DevTestLabs pool, or the name you gave it when configuring the artifact on DevTest Labs or the script on the VM.\n0.4.8.1. Create Azure DevTest Labs VM Create a new pipeline and choose the “Use the classic editor” option. Make sure you’ve selected TFVC as your source and click “Continue” and “Empty job”. Add a new task to the pipeline, look for “Azure DevTest Labs Create VM”. We just need to fill in the missing parameters with our subscription, lab, etc.\nCreate VM Azure DevTest Labs\nRemember this step must run on the Azure-hosted pipeline.\n0.4.8.2. Build This is an easy one. Just export a working pipeline and import it. And this step needs to run on your self-hosted pool:\nRuns on self-hosted pool\n0.4.8.2.1. Optional: use SelectiveSync (not recommended, see next option) You can replace the Database Sync task for a PowerShell script that will only sync the tables in your models:\nSelectiveSync.ps1\nThanks Joris for the tip!\n0.4.8.2.2. Optional: use d365fo.tools to sync your packages/models This is a better option than the SelectiveSync above. You can synchronize your packages or models only to gain some time. This cmdlet uses sync.exe like Visual Studio does and should be better than SelectiveSync.\nAdd a new PowerShell task, select Inline Script and this is the command:\nInvoke-D365DbSyncModule -Module \u0026#34;Module1\u0026#34;, \u0026#34;Module2\u0026#34; -ShowOriginalProgress -Verbose 0.4.8.2.3. Optional: use d365fo.tools to deploy SSRS reports If you really want to add the report deployment step to your pipeline you can save some more extra time using d365fo.tools and just deploy the reports in your models like we’ve done with the DB sync.\nRun this in a new PowerShell task to do it:\nPublish-D365SsrsReport -Module YOUR\\_MODULE -ReportName \\* 0.4.8.3. Delete Azure DevTest Labs VM It’s almost the same as the create step, complete the subscription, lab and VM fields and done:\nDelete VM\nAnd this step, like the create one, will run on the Azure-hosted agent.\n0.4.8.4. Dependencies and conditions When all three steps are configured we need to add dependencies and conditions to some of them. For example, to make sure that the delete VM step runs when the build step fails, but it doesn’t when the create VM step fails.\n0.4.8.4.1. Build The build step depends on the create VM step, and will only run if the previous step succeeds:\nBuild step dependencies and conditions\n0.4.8.4.2. Delete VM The delete step depends on all previous steps and must run when the create VM step succeeds. If the create step fails there’s no VM and we don’t need to delete it:\nDependencies and conditions on delete VM step\nThis is the custom condition we’ll use:\nand(always(), eq(dependencies.Job\\_1.status, \u0026#39;Succeeded\u0026#39;)) If you need to know your first step’s job name just export the pipeline to YAML and you’ll find it there:\nExport pipeline to YAML\nJob name on YAML\nIf this step fails when the pipeline is run, wait to delete the VM manually, first change the VM name in the delete step, save your pipeline and then use the dropdown to show the VMs in the selected subscription, and save the pipeline.\n0.4.9. Run the build And, I think, we’re done and ready to run our Azure DevTest Labs pipeline for Dynamics 365 Finance and Operations… click “Run pipeline” and wait…\nTadaaaa!!\n0.4.10. Times The pipeline from the image above is one with real code from a customer but I can’t compare the times with the Azure-hosted builds because there’s no sync, or tests there. Regarding the build time the Azure-hosted takes one minute less, but it needs to install the nugets first.\nBut for example this is a comparison I did:\nAzure DevTest Labs B2ms vs B4ms\nIt takes around 1 hour to create the VM, build, do a full DB synch, deploy reports, run tests, generate a Deployable Package and, finally, delete the VM:\nIf you skip deploying the SSRS reports your build will run in 15 minutes less, that’s around 45 minutes.\nIf you use the partial sync process instead of a full DB sync it’ll be 5-7 minutes less.\nThis would leave us with a 35-40 minutes build.\n0.4.10.1. Comparison 1 No DB Sync\nThe image above shows a simple package being compiled, without any table, so the selective sync goes really fast. The build times improve with VM size.\n0.4.10.2. Comparison 2 Same code Full DB Sync\nThis one is compiling the same codebase but is doing a full DB sync. The sync time improves in the B4ms VM compared to the B2ms but it’s almost the same in the B8ms. Build times are better for larger VM sizes.\n0.4.10.3. Comparison 3 Real code + full sync\nAnd in the image above we see a more realistic build. The codebase is larger and we’re doing a full DB sync.\nSimilar as the comparison before there a good enhancement between a B2ms and a B4ms, but not between a B4ms and B8ms.\n0.4.11. Show me the money! I think this is the interesting comparison. How did a Tier-1 MS-hosted build VM cost? Around 400€? How does it compare to using the Azure DevTest Labs alternative?\nThere’s only one fix cost when using Azure DevTest Labs: the blob storage where the VHD is uploaded. The VHD’s size is around 130GB and this should have a cost of, more or less, 5 euros/month. Keep in mind that you need to clean up your custom images when yours is prepared, the new ones are created as snapshots and also take space in the storage account.\nThen we have the variable costs that come with the deployment of a VM each build but it’s just absurd. Imagine we’re using a B4ms VM, with a 256GB Premium SSD disk, we would pay 0.18€/hour for the VM plus the proportional part of 35.26€/month of the SSD disk, which would be like 5 cents/hour?\nBut this can run on a B2ms VM too which is half the compute price of the VM, down to 9 cents per hour.\nIf we run this build once a day each month, 30 times, the cost of a B4ms would be like… 7€? Add the blob storage and we’ll be paying 12€ per month to run our builds with DB sync and tests.\nIs it cheaper than deploying a cloud-hosted environment, and starting and stopping it using the new d365fo.tools Cmdlets each time we run the build? Yes it is! Because if we deploy a CHE we’ll be paying the price of the SSD disk for the whole month!\n0.4.12. Some final remarks  I have accomplished this mostly through trial-and-error. There’s lots of enhancements and best practices to be applied to all the process, specially using an Azure Key Vault to store all the secrets to be used in the Azure DevOps Agent artifact and the pipeline. This in another clear example that X++ developers need to step outside of X++ and Dynamics 365 FnO. We’re not X++ only developers anymore, we’re very lucky to be working on a product that is using Azure. I’m sure there’s scenarios where using DevTest Labs to create a build VM is useful. Maybe not for an implementation partner, but maybe it is for an ISV partner. It’s just an additional option. The only bad thing to me is that we need to apply the version upgrades manually to the VHDs because they’re published only twice a year. As I said at the beginning of the post, it may have worked to me with all these steps, but if you try you maybe need to change some things. But it’s a good way to start.  Add and build .NET projects I bet that most of us have had to develop some .NET class library to solve something in Dynamics 365 Finance and Operations. You create a C# project, build it, and add the DLL as a reference in your FnO project. Don’t do that anymore! You can add the .NET project to source control, build it in your pipeline, and the DLL gets added to the deployable package!\nI’ve been trying this during the last days after a conversation on Yammer, and while I’ve managed to build .NET and X++ code in the same pipeline, I’ve found some issues or limitations.\n0.5.1. Build .NET in your pipeline Note: what I show in this post is done using the Azure-hosted pipeline but it should also be possible to do it using a self-hosted agent (aka old build VM).\nThe build step of the pipeline invokes msbuild.exe which can build .NET code. If we check the logs of the build step we will see it:\nmsbuild.exe builds C# projects and our X++ ones too!\nRemember that X++ is part of the .NET family after all… a second cousin or something like it.\nBuild folder\nIf you’ve read the blog post about Azure-hosted builds you must’ve seen I’m putting the solution that references all my models in a folder called Build at the root of my source control tree (left image).\nThat’s just a personal preference that helps me keep the .config files and the solution I use to build all the models in a single, separate place.\nBy using a solution and pointing the build process to use it I also keep control of what’s being built in a single place.\n0.5.2. Add a C# project to FnO Our first step will usually be creating a Finance and Operations project. Once it’s created we right-click on the solution and select “Add new project”. Then we select a Visual C# Class Library project:\nC# project in Dynamics 365\nNow we should have a solution with a FnO Project and a C# project (right image).\nTo demo this I’ll create a class called Calculator with a single method that accepts two decimal values as parameters and returns it’s sum. An add method.\npublic class Calculator { public decimal Add(decimal a, decimal b) { return a + b; } } public class Calculator { public decimal Add(decimal a, decimal b) { return a + b; } } Now compile the C# project alone, not the whole solution. This will create the DLL in the bin folder of the project. We have to do this before adding the C# project as a reference to the FnO project.\nRight click on the References node of the FnO project and select “Add Reference…”:\nAdd reference to FnO project\nA window will open and you should see the C# project in the “Projects” tab:\nAdd C# project reference to FnO project\nSelect it and click the Ok button. That will add the C# project as a reference to our FnO project, but we still need to do something or this won’t compile in our pipeline. We have to manually add the reference to the project that has been created in the AOT. So, right-click on the reference and select “Add to source control”:\nAdd the reference to source control\nIn the FnO project add a Runnable Class, we’ll call the C# library there:\nusing AASBuildNetDemoLibrary; class AASBuildNetTest { public static void main(Args \\_args) { var calc = new Calculator(); calc.Add(4, 5); } } using AASBuildNetDemoLibrary; class AASBuildNetTest { public static void main(Args \\_args) { var calc \\= new Calculator(); calc.Add(4, 5); } } Add the solution to source control if you haven’t, make sure all the objects are also added and check it in.\n0.5.3. Build pipeline If I go to my Azure DevOps repo we’ll see the following:\nProjects and objects\nYou can see I’ve checked-in the solution under the Build folder, as I said earlier this is my personal preference and I do that to keep the solutions I’ll use to build the code under control.\nIn my build pipeline I make sure I’m using this solution to build the code:\nBuild Dynamics 365 solution\nRun the pipeline and when it’s done you can check the build step and you’ll see a line that reads:\nCopying file from \u0026#34;D:\\\\a\\\\9\\\\s\\\\Build\\\\AASBuildNetDemo\\\\AASBuildNetDemoLibrary\\\\bin\\\\Debug\\\\AASBuildNetDemoLibrary.dll\u0026#34; to \u0026#34;D:\\\\a\\\\9\\\\b\\\\AASDemo\\\\bin\\\\AASBuildNetDemoLibrary.dll\u0026#34;. And if you download the DP, unzip it, navigate to AOSService\\Packages\\files and unzip the file in there, then open the bin folder, you’ll see our library’s DLL there:\nVictory!\n0.5.4. Things I don’t like/understand/need to investigate I’ve always done this with a single solution and only one C# project. I have some doubts about how this will work with many C# projects, models, solutions, etc.\nFor example, if a model has a dependency on the DLL but it’s built before the DLL the build will fail. I’m sure there’s a way to set an order to solve dependencies like there is for FnO projects within a solution.\nOr maybe I could try building all the C#/.NET projects before, pack them in a nuget and use the DLLs later in the FnO build, something similar to what Paul Heisterkamp explained in his blog.\nAnyway, it’s your choice how to manage your C# projects and what solution fits your workflow the best, but at least you’ve got an example here 🙂\nSetup Release Pipelines We’ve seen how the default build definition is created and how we can modify it. Now we’ll see how to configure our release pipelines!\nThe release pipelines allow us to automatically deploy our Deployable Packages to a Tier 2+ environment. This is part of the Continuous Delivery (CD) strategy. We can only do this for the UAT environments, it’s not possible to automate the deployment to the production environment.\n0.6.1. Setting up Release Pipeline in Azure DevOps for Dynamics 365 for Finance and Operations To configure the release pipeline, we need:\n AAD app registration LCS project An Azure DevOps project linked to the LCS project above A service account  I recommend a service account to do this, with a non-expiring password and no MFA enabled. It must have enough privileges on LCS, Azure and Azure DevOps too. This is not mandatory and can be done even with your user (if it has enough rights) for testing purposes, but if you’re setting this up don’t use your user and go for a service account.\n0.6.2. AAD app creation The first step to take is creating an app registration on Azure Active Directory to upload the generated deployable package to LCS. Head to Azure portal and once logged in go to Azure ActiveDirectory, then App Registrations and create a new Native app:\nNext go to “Settings” and “Required permissions” to add the Dynamics Lifecycle Services API:\nIn the dialog that will open change to the “APIs my organization uses” tab and select “Dynamics Lifecycle Services”:\nSelect the only available permission in the next screen and click on the “Add permissions” button. Finally press the “Grant admin consent” button to apply the changes. This last step can be easily forgotten and the package upload to LCS cannot be done if not granted. Once done take note of the Application ID, we’ll use it later.\n0.6.3. Create the release pipeline in DevOps Go to Azure DevOps, and to Pipelines -\u0026gt; Releases to create the new release. Select “New release pipeline” and choose “Empty job” from the list.\nOn the artifact box select the build which will be used for this release definition:\nPick the build definition you want to use for the release in “Source”, “Latest” in “Default version” and push “Add”.\n0.6.3.1. Upload to LCS The next step we’ll take is adding a Task with the release pipeline for Dynamics. Go to the Tasks tab and press the plus button. A list with extension will appear, look for “Dynamics 365 Unified Operations Tools”:\nIf the extension hasn’t been added previously it can be done in this screen. In order to add it, the user used to create the release must have admin rights on the Azure DevOps account, not only in the project in which we’re creating the pipeline.\nWhen the task is created we need to fill some parameters:![Release Dynamics Operations](./MSDyn365 \u0026amp; Azure DevOps ALM - ariste.info_files/Captura-de-pantalla-2019-02-03-a-les-0.43.11-1024x508.png.webp \u0026ldquo;MSDyn365 \u0026amp; Azure DevOps ALM 84\u0026rdquo;)\n0.6.3.2. Apply deployable package This step is finally available for self-service environments! If you already set this for a regular environment you can still change the task to the new version.\nAzure DevOps asset deployment\nThe new task version 1 works for both type of environments: Microsoft managed (regular environments) and self-service environments. The task version 0 is the old one and will only work with regular environments. You can safely switch your deploy tasks to version 1.\nWhat’s different in task version 1? I guess that some work behind it that we don’t see to make it support self-service, but in the UI we only see a new field called “Name for the update“.\nName for the update field\nThis field is needed only for the self-service environments deployments, it will be ignored for regular ones, and corresponds to the field with the same name that appears on LCS when we apply an update to a sandbox environment:\nName for this update in LCS\nThe default field’s value is the variable $(Release.ReleaseName) that is the name of the release, but you can change it, for example I’ll be using a pattern like PREFIX BRANCH $(Build.BuildNumber) to have the same name we have for the builds and identifying what we’re deploying to prod quickier.\n0.6.4. Creating the LCS connection The first step in the task is setting up the link to LCS using the AAD app we created before. Press New and let’s fill the fields in the following screen:\nIt’s only necessary to fill in the connection name, username, password (from the user and Application (Client) ID fields. Use the App ID we got in the first step for the App ID field. The endpoint fields should be automatically filled in. Finally, press OK and the LCS connection is ready.\nIn the LCS Project Id field, use the ID from the LCS project URL, for example in https://lcs.dynamics.com/V2/ProjectOverview/1234567 the project is is 1234567.\nPress the button next to “File to upload” and select the deployable package file generated by the build:\nIf the build definition hasn’t been modified, the output DP will have a name like AXDeployableRuntime_VERSION_BUILDNUMBER.zip. Change the fixed Build Number for the DevOps variable $(Build.BuildNumber) like in the image below:\nThe package name and description in LCS are defined in “LCS Asset Name” and “LCS Asset Description”. For these fields, Azure DevOps’ build variables and release variables can be used. Use whatever fits your project, for example a prefix to distinguish between prod and pre-prod packages followed by $(Build.BuildNumber), will upload the DP to LCS with a name like Prod 2019.1.29.1, using the date as a DP name.\nSave the task and release definition and let’s test it. In the Releases select the one we have just created and press the “Create a release” button, in the dialog just press OK. The release will start and, if everything is OK we’ll see the DP in LCS when it finishes:\nThe release part can be automated, just press the lightning button on the artifact and enable the trigger:\nAnd that’s all! Now the build and the releases are both configured. Once the deployment package is published the CI scenario will be complete.\nMore automation! I’ve already explained in the past how to automate the builds, create the CI builds and create the release pipelines on Azure DevOps, what I want to talk about in this post is about adding a little bit more automation.\nBuilds In the build definition go to the “Triggers” tab and enable a scheduled build:\nThis will automatically trigger the build at the time and days you select. In the example image, every weekday at 16.30h a new build will be launched. But everyday? Nope! What the “Only schedule builds if the source or pipeline has changed” checkbox below the time selector makes is only triggering the build if there’s been any change to the codebase, meaning that if there’s no changeset checked-in during that day no build will be triggered.\nReleases First step done, let’s see what can we do with the releases:\nThe release pipeline in the image above is the one that launches after the build I’ve created in the first step. For this pipeline I’ve added the following:\nThe continuous deployment trigger has been enabled, meaning that after the build finishes this release will be automatically run. No need to define a schedule but you could also do that.\nAs you can see, the schedule screen is exactly the same as in the builds, even the changed pipeline checkbox is there. You can use any of these two approaches, CD or scheduled release, it’s up to your project or team needs.\nWith these two small steps you can have your full CI and CD strategy automatized and update a UAT environment each night to have all the changes done during that day ready for testing, with no human interaction!\nBut I like to add some human touch to it If you don’t like not knowing if an environment is being updated… well that’s IMPOSSIBLE because LCS will SPAM you to make sure you know what’s going on. But if you don’t want to be completely replaced by robots you can add approvals to your release flow:\nClicking the left lightning + person button on your release you can set the approvers, a person or a group (which is quite practical), and the kind of approval (all or single approver) and the timeout. You will also receive an email with a link to the approval form:\nAnd you can also postpone the deployment! Everything is awesome!\nExtra bonus! A little tip. Imagine you have the following release:\nThis will update 3 environments, but will also upload the same Deployable Package three times to LCS. Wouldn’t it be nice to have a single upload and that all the deployments used that file? Yes, but we can’t pass the output variable from the upload to other stages 🙁 Yes that’s unfortunately right. But we can do something with a little help from our friend Powershell!\nUpdate a variable in a release What we need to do is create a variable in the release definition and set its scope to “Release”:\nThen, for each stage, we need to enable this checkbox in the agent job:\nI explain later why we’re enabling this. We now only need to update this variable after uploading the DP to LCS. Add an inline Powershell step after the upload one and do this:\n# Populate store value to update pipeline $assetId\\= \u0026#34;$(GoldenUpload.FileAssetId)\u0026#34; Write\\-Output (\u0026#39;##vso\\[task.setvariable variable=localAsset\\]{0}\u0026#39; \\-f $assetId) #region variables  $ReleaseVariableName \\= \u0026#39;axzfileid\u0026#39; $releaseurl \\= (\u0026#39;{0}{1}/\\_apis/release/releases/{2}?api-version=5.0\u0026#39; \\-f $($env:SYSTEM\\_TEAMFOUNDATIONSERVERURI), $($env:SYSTEM\\_TEAMPROJECTID), $($env:RELEASE\\_RELEASEID) ) #endregion  #region Get Release Definition  Write\\-Host \u0026#34;URL: $releaseurl\u0026#34; $Release \\= Invoke\\-RestMethod \\-Uri $releaseurl \\-Headers @{ Authorization \\= \u0026#34;Bearer $env:SYSTEM\\_ACCESSTOKEN\u0026#34; } #endregion  #region Output current Release Pipeline  #Write-Output (\u0026#39;Release Pipeline variables output: {0}\u0026#39; -f $($Release.variables | #ConvertTo-Json -Depth 10)) #endregion  #Update axzfileid with new value $release.variables.($ReleaseVariableName).value \\= $assetId #region update release pipeline  Write\\-Output (\u0026#39;Updating Release Definition\u0026#39;) $json \\= @($release) | ConvertTo\\-Json \\-Depth 99 $enc \\= \\[System.Text.Encoding\\]::UTF8 $json\\= $enc.GetBytes($json) Invoke\\-RestMethod \\-Uri $releaseurl \\-Method Put \\-Body $json \\-ContentType \u0026#34;application/json\u0026#34; \\-Headers @{Authorization \\= \u0026#34;Bearer $env:SYSTEM\\_ACCESSTOKEN\u0026#34; } #endregion You need to change the following:\n Line 2: $assetId= “$(GoldenUpload.FileAssetId)”. Change $(GoldenUpload.FileAssetId) for your output variable name. Line 6: $ReleaseVariableName = ‘axzfileid’. Change axzfileid for your Release variable name.  And you’re done. This script uses Azure DevOps’ REST API to update the variable value with the file id, and we enabled the OAuth token checkbox to allow the usage of this API without having to pass any user credentials. This is not my idea obviously, I’ve done this thanks to this post from Stefan Stranger’s blog.\nNow, in the deploy stages you need to retrieve your variable’s value in the following way:\nDon’t forget the ( ) or it won’t work!\nAnd with these small changes you can have a release like this:\nWith a single DP upload to LCS and multiple deployments using the file uploaded in the first stage. With approvals, and delays, and emails, and everything!\nLCS DB API Call the LCS Database Movement API from your Azure DevOps Pipelines 2.1.1. What for? Basically, automation. Right now the API only allows the refresh from one Microsoft Dynamics 365 for Finance and Operations environment to another, so the idea is having fresh data from production in our UAT environments daily. I don’t know which new operations the API will support in the future but another idea could be adding the DB export operation (creating a bacpac) to the pipeline and having a copy of prod ready to be restored in a Dev environment.\nDon’t forget that the API has a limit of 3 refresh operations per environment per 24 hours. Don’t do this on a CI build! (it makes no sense either). Probably the best idea is to run this nightly with all your tests, once a day.\n2.1.2. Calling the API I’ll use PowerShell to call the API from a pipeline. PowerShell has a command called Invoke-RestMethod that makes HTTP/HTTPS requests. It’s really easy and we just need to do the same we did to call the API in my post.\n2.1.2.1. Getting the token $projectId \\= \u0026#34;1234567\u0026#34; $tokenUrl \\= \u0026#34;https://login.microsoftonline.com/common/oauth2/token\u0026#34; $clientId \\= \u0026#34;12345678-abcd-432a-0666-22de4c4321aa\u0026#34; $clientSecret \\= \u0026#34;superSeCrEt12345678\u0026#34; $username \\= \u0026#34;youruser@tenant.com\u0026#34; $password \\= \u0026#34;strongerThan123456\u0026#34; $tokenBody \\= @{ grant\\_type \\= \u0026#34;password\u0026#34; client\\_id \\= $clientId client\\_secret \\= $clientSecret resource \\= \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; username \\= $username password \\= $password } $tokenResponse \\= Invoke\\-RestMethod \\-Method \u0026#39;POST\u0026#39; \\-Uri $tokenUrl \\-Body $tokenBody $token \\= $tokenResponse.access\\_token To get the token we’ll use this script. Just change the variables for the ones of your project, AAD App registration, user (remember it needs access to the preview) and password and run it. If everything is OK you’ll get the JSON response in the $tokenResponse variable and from there you can get the token’s value using dot notation.\n2.1.2.2. Requesting the DB refresh $projectId \\= \u0026#34;1234567\u0026#34; $sourceEnvironmentId \\= \u0026#34;fad26410-03cd-4c3e-89b8-85d2bddc4933\u0026#34; $targetEnvironmentId \\= \u0026#34;cab68410-cd13-9e48-12a3-32d585aaa548\u0026#34; $refreshUrl \\= \u0026#34;https://lcsapi.lcs.dynamics.com/databasemovement/v1/databases/project/$projectId/source/$sourceEnvironmentId/target/$targetEnvironmentId\u0026#34; $refreshHeader \\= @{ Authorization \\= \u0026#34;Bearer $token\u0026#34; \u0026#34;x-ms-version\u0026#34; \\= \u0026#39;2017-09-15\u0026#39; \u0026#34;Content-Type\u0026#34; \\= \u0026#34;application/json\u0026#34; } $refreshResponse \\= Invoke\\-RestMethod $refreshUrl\u0026amp;nbsp;\\-Method \u0026#39;POST\u0026#39; \\-Headers $refreshHeader This will be the call to trigger the refresh. We’ll need the token we’ve just obtained in the first step to use it in the header and the source and target environment Ids.\nIf it’s successful the response will be a 200 OK.\n2.1.3. Add it to your pipeline Adding this to an Azure DevOps pipeline is no mistery. Select and edit your pipeline, I’m doing it on a nigthly build (it’s called continuous but it’s not…) that runs after the environment has been updated with code, and add a new PowerShell task:\nSelect the task and change it to “Inline”:\nThen just paste the script we’ve created in the Script field and done! You’ll get a refresh after the tests!\nYou can also run this on your release pipeline BUT if you do it after the deploy step remember to mark the “Wait for Completion” option or the operation will fail because the environment will already be servicing! And even then it could fail if the servicing goes over the timeout time. So… don’t run this on your release pipeline!\nAnd that’s all. Let’s which new operations will be added to the API and what we can do with them.\n2.1.4. Use d365fo.tools in your Azure Pipeline Thanks to Mötz’s comment pointing me to how to add d365fo.tools to a hosted pipeline I’ve created a pipeline which will install the tools and run the commands. It’s even easier to do than with the Invoke-RestMethod.\n2.1.4.1. But first… Make sure that in your Azure Active Directory app registration you’ve selected “Treat application as a public client” under Authentication:\n2.1.4.2. The task First we need to install d365fo.tools and then we can use its commands to call the LCS API:\nInstall\\-PackageProvider nuget \\-Scope CurrentUser \\-Force \\-Confirm:$false Install\\-Module \\-Name AZ \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$False \\-SkipPublisherCheck Install\\-Module \\-Name d365fo.tools \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$false Get\\-D365LcsApiToken \\-ClientId \u0026#34;{YOUR\\_APP\\_ID}\u0026#34; \\-Username \u0026#34;{USERNAME}\u0026#34; \\-Password \u0026#34;{PASSWORD}\u0026#34; \\-LcsApiUri \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; \\-Verbose | Set\\-D365LcsApiConfig \\-ProjectId 1234567 Invoke\\-D365LcsDatabaseRefresh \\-SourceEnvironmentId \u0026#34;958ae597-f089-4811-abbd-c1190917eaae\u0026#34; \\-TargetEnvironmentId \u0026#34;13cc7700-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; \\-SkipInitialStatusFetch As you can see it a bit easier to do the refresh using d365fo.tools. We get the token and pipeline the output to the Set-D365LcsApiConfig command which will store the token (and others). This also helps to not having to duplicate AppIds, users, etc. and as you can see to call the refresh operation we just need the source and target environment Ids!\nAutomating Prod to Dev DB copies The new LCS DB API endpoint to create a database export has been published! With it we now have a way of automating and scheduling a database refresh from your Dynamics 365 FnO production environment to a developer or Tier 1 VM.\nUsing the LCS DB API\n2.2.1. The bacpac issue One of the main setbacks we currently have with prod DB refreshes is that it’s not a quick thing to do because you need to:\n Refresh a Tier 2+ environment with prod’s DB Export a bacpac from the Tier 2+ environment Restore the bacpac on a Tier 1 VM.  This happens because Tier 2+ environments use Azure SQL as the DB engine and Tier 1 VMs use SQL Server.\nThe time it takes to complete the process depends on the size of the database and the performance of the VM you’ll restore it to. But it’s not a fast process at all. For a 60GB database you’ll get a bacpac around 7GB that will take:\n 1 to 2 hours to refresh to UAT 2 to 4 hours for the bacpac to be exported At least 4 hours to restore it to a Tier 1 VM.  That’s between 7 and 11 hours until you have the DB on a developer machine. Once it’s there you can quickly get a BAK and share it. But you might need the time of a full working day to have that data available.\n2.2.2. Save us LCS DB API! Thanks to the new LCS DB API’s endpoint we can perform all these steps automatically, and with the help of d365fo.tools it’ll be even easier. But first…\nDue to the extensive time it takes to complete all the process, we first have to decide a schedule (daily, weekly, etc.) and then this schedule must be compatible with the release cadence to UAT/Prod, because only one operation at a time can be done.\nThere’s still another problem but I’ll talk about it after seeing the scripts.\n2.2.3. My proposal To do the last part of the LCS DB API flow from prod to dev, we need a Tier 1 VM where the bacpac will be restored. My idea is using the build VM on Microsoft’s subscription and an Azure DevOps pipeline to run all the scripts that will restore the DB in that VM. It’s an underused machine and it fits perfectly to this purpose.\nI want to clarify why I’ve thought about doing this using the build VM. In most cases this VM will be doing nothing during the night, maybe only running some tests, and it’s during that period of time when I suggest doing all this. But be aware that depending on your DB size this won’t be possible or you’ll run out of space after 2 o 3 restores.\nSo think about deploying an extra VM and install an agent there to do this, whatever you do don’t mess with the build VM if you don’t know what you’re doing! Try this on a dev VM or anywhere else if you’re afraid of breaking something. Remember you’ll lose the capacity to generate DPs and run pipelines if this environments breaks!\nThis post is just an example of a possible solution, you need to decide what suits you best! End of the update.\nAs I said before I’ll be using Mötz Jensen‘s d365fo.tools, we could do everything without them but that would be a bit stupid because using the tools is easier, faster and makes everything clearer.\nI’ve separated all the steps in 3 Powershell scripts: execute the refresh, export the bacpac and restore the bacpac.\n2.2.3.1. Refresh database This will refresh the prod environmnet to a Tier 2+:\n$clientId \\= \u0026#34;ab12345-6220-4566-896a-19a4ad41783f\u0026#34; $userName \\= \u0026#34;admin@tenant\u0026#34; $passWord \\= \u0026#34;admin123456\u0026#34; $projectId \\= \u0026#34;1234567\u0026#34; $sourceEnvId \\= \u0026#34;958bc863-f089-4811-abbd-c1190917eaae\u0026#34; $targetEnvId \\= \u0026#34;13aa6872-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; Get-D365LcsApiToken \\-ClientId $clientId \\-Username $userName \\-Password $passWord \\-LcsApiUri \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; \\-Verbose | Set\\-D365LcsApiConfig \\-ProjectId $projectId Invoke-D365LcsDatabaseRefresh \\-SourceEnvironmentId $sourceEnvId \\-TargetEnvironmentId $targetEnvId \\-SkipInitialStatusFetch 2.2.3.2. Export database This part will trigger the bacpac export from the Tier 2+ environment which we’ve just refreshed:\n$sourceEnvId \\= \u0026#34;958bc863-f089-4811-abbd-c1190917eaae\u0026#34; $targetEnvId \\= \u0026#34;13aa6872-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; Get-D365LcsApiConfig | Invoke-D365LcsApiRefreshToken | Set\\-D365LcsApiConfig Invoke-D365LcsDatabaseExport \\-SourceEnvironmentId $targetEnvId \\-BackupName $bacpacName 2.2.3.3. Restore bacpac And the final step will download the bacpac and restore it to a new database:\n$currentDate \\= Get-Date \\-Format yyyymmdd $bacpacName \\= \u0026#34;UAT{0}\u0026#34; \\-f $currentDate $downloadPath \\= \u0026#34;D:\\\\UAT{0}.bacpac\u0026#34; \\-f $currentDate $newDBName \\= \u0026#34;AxDB\\_{0}\u0026#34; \\-f $currentDate Get-D365LcsApiConfig | Invoke-D365LcsApiRefreshToken | Set\\-D365LcsApiConfig $backups \\= Get-D365LcsDatabaseBackups $fileLocation \\= $backups\\[0\\].FileLocation Invoke-D365AzCopyTransfer \\-SourceUri $fileLocation \\-DestinationUri $downloadPath Import-D365Bacpac \\-ImportModeTier1 \\-BacpacFile $downloadPath \\-NewDatabaseName $newDBName 2.2.3.4. Using it in an Azure DevOps pipeline Azure DevOps pipeline\nThis is it. Create a Powershell script, place it in the Build VM and call it in your pipeline. This is only valid for the agent hosted in the build VM. Everything can probably be run in an Azure hosted agent, but I’ll not cover it here because I think that using the build VM, where we can restore the DB, is more useful to us.\n2.2.4. Timing These 3 scripts will call the LCS DB API to refresh, export and restore the DB. But there’s the timing issue.\nRefreshing the database takes some time and exporting it too. You need to find a way to control the status of the operations. The LCS DB API offers an operation you can use to get the status of the ongoing operation. Using d365fo.tools:\nGet-D365LcsDatabaseRefreshStatus \\-OperationActivityId 123456789 \\-EnvironmentId \u0026#34;99ac6587-c13b-4ea3-81cd-2d26fa72ec5e\u0026#34; You can choose to control that inside your Powershell scripts, but if we use the agent on the build VM that means we cannot use it for anything else until everything is done.\nThat’s why I separated the process in 3 steps. You can manually schedule 3 pipelines, one for each step at the times you know each stage ends. Then you can choose the order: export, restore, refresh or refresh, export, restore.\nYou could also use Windows Task Scheduler and forget about AZDO Pipelines, but we’re not doing that because we love pipelines.\nAnd that’s all, we finally have a way of moving data without having to do it manually, we can schedule it, but we need to take some decisions on how we’ll do things. And I’ll leave that up to you 🙂\nSecure your Azure Pipelines with Azure Key Vault But creating a pipeline with a password in plain sight was not very secure. How could we add extra security to a pipeline? Once again we can turn to an Azure tool to help us, the Azure Key Vault.\nAzure Key Vault A Key Vault is a service that allows us to safely store certificates or secrets and later use them in our applications and services. And like many other Azure services it has a cost but it’s really low and, for a normal use, you will be billed like a cent or none a month. Don’t be stingy with security!\nYou might already know about Azure Key Vault because we can use it in Microsoft Dynamics 365 for Finance and Operations under System Administration. For example it’s how the company certificates for the Spanish SII or Brazilian NF-e are stored and later retrieved to call the web services.\nSecuring your Azure DevOps Pipelines Thanks to the Azure Key Vault task (which is open source like many other tasks) getting a secret from a Key Vault has no secret (badum tssss).\n4.1.1. Create a Key Vault Go to your Azure subscription and look for Key Vaults in the top search bar. If you don’t have an Azure subscription you can get one free with a credit of 170€/200$ for 30 days and try this or other things.\nIn the Key Vault page click on “Create key vault” and fill the fields\nYou can go through other tabs but I will just click “Review \u0026amp; Create” to create the vault.\n4.1.2. Add the task to DevOps Now go to Azure DevOps and create a new pipeline or edit an existing one. Add a task to the agent job and look for azure key vault:\nIt’s possible that you might need to get the task from the marketplace first, if so remember you need to have enough right on the organization and not only the AZDO project you’re in. Now go to the task and select your subscription:\nOnce selected click the “Authorize” button. This will create a service principal in your subscription, we’ll use it later. After authorizing you just need to select the key vault you’ve created in the first step. And back to Azure.\n4.1.3. Setup and secret creation Go to your key vault, “Access policies” and click “Add Access Policy”:\nWhen we authorized the task to access our Azure subscription it created a service principal now we need to select it to list and get the secrets to be able to use them in our pipeline. Click on “Select principal”:\nIn the search bar type your subscription’s name, the principal should start with it and end with the same ID of your subscription. Select it and click the “Select” button at the bottom:\nNow click on the “Secret permissions” lookup and under “Secret Management Operations” select Get and List:\nIf you want to also use certificates or keys you should do the same. Finally click the “Add” button and don’t forget to click “Save”!! Otherwise nothing will be saved:\nNow we can create a secret in the key vault. Go to secrets and click on “Generate/Import”, complete the fields and finally click on the “Create” button:\n4.1.4. Using the secrets in your pipelines We’re ready to use the secret in our pipeline. I will add a PowerShell task to call the LCS DB API using d365fo.tools but I’ll change all the variables to the secrets:\n# Write your PowerShell commands here. Install\\-PackageProvider nuget \\-Scope CurrentUser \\-Force \\-Confirm:$false Install\\-Module \\-Name AZ \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$False \\-SkipPublisherCheck Install\\-Module \\-Name d365fo.tools \\-AllowClobber \\-Scope CurrentUser \\-Force \\-Confirm:$false Get\\-D365LcsApiToken \\-ClientId \u0026#34;$(myAppId)\u0026#34; \\-Username \u0026#34;$(myUserName)\u0026#34; \\-Password \u0026#34;$(mySecretPassword)\u0026#34; \\-LcsApiUri \u0026#34;https://lcsapi.lcs.dynamics.com\u0026#34; \\-Verbose | Set\\-D365LcsApiConfig \\-ProjectId $(myProjectId) Get\\-D365LcsDatabaseBackups As you can see now even the AAD App Id is masked.\nWhat the Azure Key Vault task does is getting the secrets from Azure and storing them in variables when the pipeline runs:\nThen we can access it’s value with the $(variableName) notation in the PowerShell script. If you try to print the secrets’ values using the Write-Host command all you’ll get will be three asterisks, so you can see that using the Key Vault is more than safe. If we check the result of running the Get-D365LcsDatabaseBackups command we’ll see how good is this:\nThe ProjectId value is not printed because it was one of our secret values!\nAnd this is how you can add extra security to your Dev ALM!\n","permalink":"https://nuxulu.com/posts/2021-05-01-dynamics-365-for-finance-operations-and-azure-devops/","summary":"Dynamics 365 for Finance \u0026amp; Operations and Azure DevOps 0.1.1. Azure DevOps Azure DevOps will be the service we will use for source control. Microsoft Dynamics 365 for Finance and Operations supports TFVC out of the box as its version-control system.\nBut Azure DevOps does not only offer a source control tool. Of course, developers will be the most benefited of using it, but from project management to the functional team and customers, everybody can be involved in using Azure DevOps.","title":"Dynamics 365 for Finance \u0026 Operations and Azure DevOps"},{"content":"Hello Dynamics \u0026amp; Power Community,\nI’m very happy to announce a good news for you the Dynamics365 F\u0026amp;O Community. I’ve got a little gift for you, as part of the new LCS API.\nIf you were, like me as an AX 2012 or F\u0026amp;O Technical/Architect Consultant, you probably already suffered from not being able to offer customers the ability to automatically refresh your database from one environment to other. Of course, you could do it via scripts, powershell (but really complicated for LCS and F\u0026amp;O : see even impossible), but finally doing it by code actually…. and here I will show you the benefit of the PowerAutomate / Flow to refresh automatically your F\u0026amp;O production instance to an UAT instance, without any line of code !\nI will provide you, the How-To setup here in this article (step-by-step), also 3 samples of my GitHub account for PowerAutomate flow.\nHope you will like it, and feel free to share it :)\n![LCS_API_DatabaseMovement.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/LCS_API_DatabaseMovement.png)\nSo…. how to start :)\nFirst of all, here is a quick summary of the new API for LCS - in Dynamics 365 F\u0026amp;O.\nLike you know, we were not able to perform automatic Database movement in the past. We get only to an Azure Marketplace a flow between Azure DevOps release pipeline in order to upload your BUILD package and also setup an automatic deployment. It was the very beginning to perform great automatic release pipelines : like me, deploying every night for customers a recent package in our TEST environments every night after BUILD completed.\nDatabase movement operations are a suite of self-service actions that can be used as part of Data Application Lifecycle Management (also referred to as DataALM). These actions provide structured processes for common implementation scenarios such as golden configuration promotion, debugging/diagnostics, destructive testing, and general refresh for training purposes.\nIn this topic, you will learn how to use database movement operations to perform refresh, export, import, and various flavors of point-in-time restore.\nAt the time, I write this article, the LCS API have only 4 methods but I hope Microsoft will add soon more methods and of course I will share that with you depending on which and when it will be possible.\nWe can perform the following operations through the RESTful API:\n  List DB backups for an LCS project.\n  Create a DB refresh between two environments (only Prod and Sandbox environments, like we can do in LCS).\n  Get an ongoing operation status.\n  Quick feedbacks for Microsoft : we really need to add also monitoring KPI stuff methods in order to get the DTU SQL Percentage Live for production or other stuff like that, in order to not connect every time in the LCS Portal and check if everything looks good : to be proactive in fact / and maybe also restart service etc… : but yep I maybe ask a lot, it’s a good start now that this API is open, so I know that Microsoft will bring this API even better this year !\nMicrosoft Documentation - LCS API\n First, you will need to create an Azure AAD in order to connect to the LCS API.\nYou can go here for the How-To setup :\nhttps://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/database/api/dbmovement-api-authentication\nDon’t forget to copy somewhere your ClientID : ApplicationID and also to generate a ClientSecret. Also don’t forget to put the Dynamics Lifecycle Services like below as a Delegated access, with Grant Admin permission.\n![Screenshot 2020-04-14_21-57-02-177.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_21-57-02-177.png)\nSo, now you have the access to call the LCS API, and you can authenticate to it. But maybe, like me, if you had in the past configure the AzureDevOps release pipeline process to connect to LCS, you had maybe already an App Registration in your Azure AD, so you can keep it and reuse for this purpose.\n Just for your information, It was not possible to create directly for you Custom Connectors in PowerAutomate in order to deploy it more easily. Since the LCS API use at the moment a strange and old school authentification. Hope I can share that when OAuth 2.0 will be used for this API !). But these samples will be enough and easy to configure, trust me !\nNow, you can get my PowerAutomate / Flow samples directly in my Github Account :\nDownload all the 3 ZipFiles\nALM LCS PowerAutomate Samples - Github\nAfter, you can go to your PowerAutomate account. I know that partners/customers in FinOps are not (yet ?) going very much in there, but still, I hope I will help you to discover this fantastic feature in the PowerPlatform ! And yes, normally (like me), if you had bought your F\u0026amp;O licences, you have already a free account for every users, and it will be more than enough to setup some flows like this one.\nhttps://flow.microsoft.com\nGo, to “My Flows” in order to import it.\n![Screenshot 2020-04-14_22-13-04-870.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-13-04-870.png)\nSelect my ZIP file from my Github,\n![Screenshot 2020-04-14_22-17-15-902.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-17-15-902.png)\nHere, don’t be afraid for the 2 resources / connectors (Mail \u0026amp; Push mobile notification) you can setup it before importing or just map to your connectors if you had already setup for other flows before. I just used them in order to give you some example of actions, you can delete them after import process.\nWhen you have done all done 3 imports of the sample, let’s go in 1 by 1 if you don’t mind !\n  1. PowerAutomate Sample - LCS API - Automatic refresh database   ![Screenshot 2020-04-14_22-26-41-946.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-26-41-946.png)\nIf you go in, I will explain step-by-step in order to configure it and also give inputs why I’ve done that :)\nClick on “EDIT” button.\nHere is below the whole flow, without any line of code :)\n![Screenshot 2020-04-14_22-27-54-950.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-27-54-950.png)\nSo, first step, since it’s a scheduled flow in my example, I can setup when my flow will run. Here I have put every weedays at 7 p.m but sure you can edit it of course. Keep in mind, that you can of course do it by a trigger from other connector in PowerAutomate. Like maybe do it after an Azure DevOps Pipeline is finished or after a BUILD process, after also an approval by someone else in Microsoft Teams in adaptive card, by a click in a button in a PowerApps : well, I can give 150 examples so you can setup like you want !\nJust be careful also on the interval, you can setup only 3 refresh in a day for 1 target environment ! (limitation by LCS)\n![Screenshot 2020-04-14_22-33-15-144.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-33-15-144.png)\nAfter I just create a variable to put your LCS project ID. Of course, change the value in it to reflect your current LCS project.\nYou can grab it directly in your LCS portal if you don’t know it\n![Screenshot 2020-04-14_22-36-00-492.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-36-00-492.png)\n![Screenshot 2020-04-14_22-36-59-170.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-36-59-170.png)\nAfter, I use the HTTP connector to perform the authentification with of course your Azure Application ID and Secret token that you have created earlier.\n![Screenshot 2020-04-14_22-41-22-394.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-41-22-394.png)\nYou will need here only to change the values of : username, password, clientid and clientsecret.\nKeep the resource and grant_type like this, as it is. Don’t change the Content-Type of the header.\nOne more thing : don’t use an LCS account that of course doesn’t have access and have appropriate security role. And like me, don’t use an account that has a MFA account (Multi Factor Authentification), use like an administrator/service account.\nAfter I just use the Parse JSON feature, in order to get the object of the response. The most important thing is the property : Access_Token\nIt will be used after for every call to the LCS API, in the header, you will see :)\nKeep in mind that the Token is for 1 hour of expiration !! So don’t forget to refresh your call to authenticate after 1 hour if you need to !\n![Screenshot 2020-04-14_22-44-33-225.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-44-33-225.png)\nAfter I have created 2 more variables : the source \u0026amp; target environment of your LCS environments that you want to refresh database. Of course, change by your need !\n![Screenshot 2020-04-14_22-51-43-762.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-51-43-762.png)\nAs a reminder, you can get the Environment ID by going in more details of LCS.\n![Screenshot 2020-04-14_22-49-46-630.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-49-46-630.png)\nFinally I call the LCS API to refresh the database, like this :\nHere you don’t have to change anything, good right ? :)\nBut, I want to explain the Authorization header. It’s here that we put the Access_Token of the response after authentification.\nIt’s like : Bearer xxxxxxxxxxxxxxxxx_veryhugemassive_token\nThe space key is mandatory, so don’t forget it !\nOf course at the end, I add it also a Parse JSON to build the object response of the API.\n![Screenshot 2020-04-14_22-52-53-666.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-52-53-666.png)\nAfter, I added a conditional split. Of course, the purpose is to achieve a different path in the Flow, if the response is OK or not…\n![Screenshot 2020-04-14_22-56-40-282.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-56-40-282.png)\nIn my “not success path”, I just put a Push Mobile Notification Action, but it’s just an example of course. You can change it by whatever you like. Don’t forget you have now more than 300 connectors in PowerAutomate (Microsoft and other third-party apps !)\n![Screenshot 2020-04-14_22-58-57-340.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_22-58-57-340.png)\nOn my “success path”, here is for me the most important feature !\nFirst step here, I wait for 1 hour, magic no ? but why ? Well because, we don’t have to call LCS API to fetch the status of the current activity of the database refresh until more than 1 hour. Like me, it takes 1hour30min and sometimes maybe 2 hours to finish all the refresh. But you change it of course by adding just 30 minutes of wait if you had experience faster refresh on your side ! It’s also for me a good example to show you that we can put Expression without to be a massive developper !\n![Screenshot 2020-04-14_23-00-18-825.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-00-18-825.png)\nAfter I’ve done a Do Until feature, in order to call every 5 minutes the LCS API to fetch the status. In order to after doing some actions in the same flow.\nThe end of my “while'“ loop is until we’ve got a Status Operation than is not equal to : In Progress. Just be careful of the Change Limits here, you have to increase it to 300 like me, because by default it’s 60 : that’s say that we will only do this part for 60 minutes. But I already do that for you :)\nUpdate (17/04/20) : Just found that you can also increase the timeout of the Do Until statement. By default it’s PT1H , you can change by PT2H for 2 hours or more if you need.\n![Screenshot 2020-04-14_23-04-57-449.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-04-57-449.png)\nAfter, I just fetch the status of the Operation/Activity of the database refresh to know if it’s finished :) Parsing JSON again of course, every time in fact so I hope you are now familiar with that.\n![Screenshot 2020-04-14_23-05-29-692.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-05-29-692.png)\nAnd now, when it’s ended, why I’ve done that ? Well because on my side (and I think you also), you have some “Post-Refresh” statement - script or SQL script to achieve. As you know, after a refresh database, you have all BatchJobs On Hold, only 1 user access activated, and you want maybe to change some parameter to reflect the change and of course don’t activate the Email distributor batch to sent email to a customer from an UAT Instance… believe me I’ve seen that many times !\nSo that’s why I’ve put this last step here and will explain more deeper with just a small part of X++ code for AX Technical guys.\n![Screenshot 2020-04-14_23-12-48-036.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-12-48-036.png)\nIn fact, it was not possible for me to give you a sample of the Dyn365 F\u0026amp;O connector to share it (without giving my credentials) directly in this sample, so you will need to do it yourself for this part.\nSo, of course you will erase this part of sending email after completion, it’s irrelevant. You already receive an email from LCS when the refresh is done.\nFirst, you can of course add other actions if you want to, like I’ve said before you have 300 connectors so you can put something else.\nBut for me, you will change by that !! The Dynamics 365 FinOps connector in order to call Action / in fact it’s like a job in AX. It refers to the methods applied to a data entity.\nCreate a new step like that by choosing the right connector. You will notice that you have multiple actions. Here you can see that for other Flow/PowerAutomate you can achieve more : by creating, updating, retrieving records etc…. so for other use cases, you can have a look !\n![Screenshot 2020-04-14_23-18-07-784.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-18-07-784.png)\nHere, I will pick “Execute Action” :\n![Screenshot 2020-04-14_23-20-21-574.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-20-21-574.png)\nBut, like me, my question was : “What the hell are coming from these actions ???”\nSee the documentation of the connector F\u0026amp;O :\nLearn more\nLike I said, it’s coming from Methods in the Data Entities in F\u0026amp;O. So like I’ve done below, you can create a custom one, and create method that in fact will do of course your post-refresh database script. Like me, it was done for reactivate all users, batchjobs. But sure, you can do it depending on your needs !\n Just as a reminder, don’t forget to use the connector F\u0026amp;O with the Environment Manager that have access after a database refresh !  So just create a new Data entity, that refer to whatever table coming from AX like that :\n![Screenshot 2020-04-14_23-24-55-272.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-24-55-272.png)\nCreate your methods, that refer to SysODataActionAttribute.\nHere I don’t give a response (void) but you can of course also put some object if you need to get back to PowerAutomate flow.\n![Screenshot 2020-04-14_23-25-11-005.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-25-11-005.png)\nHere we are, finished for this first sample.\n 2. LCS API - Export database   You already know that we can export database also on the LCS Portal by manual action on a Sandbox instance. Of course, it can be good to do it like just after our previous sample because we have done a flow (DataALM) to refresh the production to an UAT instance. If you want to get the .bacpac (backup of SQL Azure) in order to deploy it (implied your production database) to a DEV environment. Maybe also because you want to archive it to a safe repository. So yes, the LCS API has already a method for that, that’s why I made this sample for you.\nI assume that you have already download it from my GitHub Account before and already import it like the previous one.\nGo in this flow.\nI will not explain again this part, because it’s exactly the same as the Database Refresh.\n![Screenshot 2020-04-14_23-35-58-495.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-35-58-495.png)\nAfter, I made 2 variables : the backupName wanted and also the environmentId that you want to export the database.\n![Screenshot 2020-04-14_23-36-59-889.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-36-59-889.png)\nAfter, I call the LCS API and grab the response to a JSON object.\n![Screenshot 2020-04-14_23-38-25-516.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-38-25-516.png)\nFinally the conditional split, you can change like before by whatever you want.\n![Screenshot 2020-04-14_23-38-38-379.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-38-38-379.png)\n 3. Retrieve list of database backup   Here, we will have the possibility to retrieve the list of the whole backup list in your LCS Asset Library. As you know, it’s the default storage, where all database backup are coming.\n![Screenshot 2020-04-14_23-42-44-006.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-42-44-006.png)\nAt the end I’ve done a For Each feature, good way to show you this point !\n![Screenshot 2020-04-14_23-44-54-577.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/Screenshot+2020-04-14_23-44-54-577.png)\nWell… sad news : true. Because I try to add also an another HTTP Connector for the FileLocation (it’s the HTTP location where you can download your backup) but the limitation is for 100MB so you will get like me an error. My aim was after to grab the fileContent to a OneDrive, Google Drive, Sharepoint or Azure Blob Storage, but yes… at this time we can’t.\nThe only way that I have for you, is to do it by a more complicated way. You can create a powershell script in your own network / share folder (onPremise). You can after download the Data Gateway for PowerAutomate in order to call your script, passing through the FileLocation and your Powershell will download it on your onPremise folder. Yes I didn’t talk about that, but yes we can achieve a connection from PowerAutomate (SaaS - Cloud Only) to connect on an onPremise API - Service.\n![on-premises-data-gateway.png](./LCS API Database Movement with PowerAutomate — PowerAzure365_files/on-premises-data-gateway.png)\nPowerAutomate - Data Gateway onPremise\nPowerAutomate - Powershell onPremise\n To conclude : I hope you like it :) Besides that, feel free to contact me, comment, do feedbacks etc…\nYou can see easily that PowerAutomate in part of the PowerPlatform is really a huge feature that will help us in a lot of flows for F\u0026amp;O project.\nBecause despite this technical part of DataALM, like I’ve done for other articles in this blog, we can achieve a lot more of use cases now ! Business cases , Business Events etc…\nSee you ! Stay tuned. Add me on social networks :)\n","permalink":"https://nuxulu.com/posts/2021-05-05-lcs-api-database-movement-with-powerautomate/","summary":"Hello Dynamics \u0026amp; Power Community,\nI’m very happy to announce a good news for you the Dynamics365 F\u0026amp;O Community. I’ve got a little gift for you, as part of the new LCS API.\nIf you were, like me as an AX 2012 or F\u0026amp;O Technical/Architect Consultant, you probably already suffered from not being able to offer customers the ability to automatically refresh your database from one environment to other. Of course, you could do it via scripts, powershell (but really complicated for LCS and F\u0026amp;O : see even impossible), but finally doing it by code actually….","title":"LCS API Database Movement with Power Automate"},{"content":"Abbott: Strange as it may seem, they give ball players nowadays very peculiar names.\nCostello: Funny names?\nAbbott: Nicknames, nicknames. Now, on the St. Louis team we have Who\u0026rsquo;s on first, What\u0026rsquo;s on second, I Don\u0026rsquo;t Know is on third\u0026ndash;\nCostello: That\u0026rsquo;s what I want to find out. I want you to tell me the names of the fellows on the St. Louis team.\nAbbott: I\u0026rsquo;m telling you. Who\u0026rsquo;s on first, What\u0026rsquo;s on second, I Don\u0026rsquo;t Know is on third\u0026ndash;\nCostello: You know the fellows' names?\nAbbott: Yes.\nCostello: Well, then who\u0026rsquo;s playing first?\nAbbott: Yes.\nCostello: I mean the fellow\u0026rsquo;s name on first base.\nAbbott: Who.\nCostello: The fellow playin' first base.\nAbbott: Who.\nCostello: The guy on first base.\nAbbott: Who is on first.\nCostello: Well, what are you askin' me for?\nAbbott: I\u0026rsquo;m not asking you\u0026ndash;I\u0026rsquo;m telling you. Who is on first.\nCostello: I\u0026rsquo;m asking you\u0026ndash;who\u0026rsquo;s on first?\nAbbott: That\u0026rsquo;s the man\u0026rsquo;s name.\nCostello: That\u0026rsquo;s who\u0026rsquo;s name?\nAbbott: Yes.\nCostello: When you pay off the first baseman every month, who gets the money?\nAbbott: Every dollar of it. And why not, the man\u0026rsquo;s entitled to it.\nCostello: Who is?\nAbbott: Yes.\nCostello: So who gets it?\nAbbott: Why shouldn\u0026rsquo;t he? Sometimes his wife comes down and collects it.\nCostello: Who\u0026rsquo;s wife?\nAbbott: Yes. After all, the man earns it.\nCostello: Who does?\nAbbott: Absolutely.\nCostello: Well, all I\u0026rsquo;m trying to find out is what\u0026rsquo;s the guy\u0026rsquo;s name on first base?\nAbbott: Oh, no, no. What is on second base.\nCostello: I\u0026rsquo;m not asking you who\u0026rsquo;s on second.\nAbbott: Who\u0026rsquo;s on first!\nCostello: St. Louis has a good outfield?\nAbbott: Oh, absolutely.\nCostello: The left fielder\u0026rsquo;s name?\nAbbott: Why.\nCostello: I don\u0026rsquo;t know, I just thought I\u0026rsquo;d ask.\nAbbott: Well, I just thought I\u0026rsquo;d tell you.\nCostello: Then tell me who\u0026rsquo;s playing left field?\nAbbott: Who\u0026rsquo;s playing first.\nCostello: Stay out of the infield! The left fielder\u0026rsquo;s name?\nAbbott: Why.\nCostello: Because.\nAbbott: Oh, he\u0026rsquo;s center field.\nCostello: Wait a minute. You got a pitcher on this team?\nAbbott: Wouldn\u0026rsquo;t this be a fine team without a pitcher?\nCostello: Tell me the pitcher\u0026rsquo;s name.\nAbbott: Tomorrow.\nCostello: Now, when the guy at bat bunts the ball\u0026ndash;me being a good catcher\u0026ndash;I want to throw the guy out at first base, so I pick up the ball and throw it to who?\nAbbott: Now, that\u0026rsquo;s he first thing you\u0026rsquo;ve said right.\nCostello: I DON\u0026rsquo;T EVEN KNOW WHAT I\u0026rsquo;M TALKING ABOUT!\nAbbott: Don\u0026rsquo;t get excited. Take it easy.\nCostello: I throw the ball to first base, whoever it is grabs the ball, so the guy runs to second. Who picks up the ball and throws it to what. What throws it to I don\u0026rsquo;t know. I don\u0026rsquo;t know throws it back to tomorrow\u0026ndash;a triple play.\nAbbott: Yeah, it could be.\nCostello: Another guy gets up and it\u0026rsquo;s a long ball to center.\nAbbott: Because.\nCostello: Why? I don\u0026rsquo;t know. And I don\u0026rsquo;t care.\nAbbott: What was that?\nCostello: I said, I DON\u0026rsquo;T CARE!\nAbbott: Oh, that\u0026rsquo;s our shortstop!\n","permalink":"https://nuxulu.com/posts/2010-01-08-post-chat/","summary":"Abbott: Strange as it may seem, they give ball players nowadays very peculiar names.\nCostello: Funny names?\nAbbott: Nicknames, nicknames. Now, on the St. Louis team we have Who\u0026rsquo;s on first, What\u0026rsquo;s on second, I Don\u0026rsquo;t Know is on third\u0026ndash;\nCostello: That\u0026rsquo;s what I want to find out. I want you to tell me the names of the fellows on the St. Louis team.\nAbbott: I\u0026rsquo;m telling you. Who\u0026rsquo;s on first, What\u0026rsquo;s on second, I Don\u0026rsquo;t Know is on third\u0026ndash;","title":"Post: Chat"},{"content":"This theme supports link posts, made famous by John Gruber. To use, just add link: http://url-you-want-linked to the post\u0026rsquo;s YAML front matter and you\u0026rsquo;re done.\n And this is how a quote looks.\n Some link can also be shown.\n","permalink":"https://nuxulu.com/posts/2010-03-07-post-link/","summary":"This theme supports link posts, made famous by John Gruber. To use, just add link: http://url-you-want-linked to the post\u0026rsquo;s YAML front matter and you\u0026rsquo;re done.\n And this is how a quote looks.\n Some link can also be shown.","title":"Post: Link"},{"content":"This post has been updated and should show a modified date if used in a layout.\nAll children, except one, grow up. They soon know that they will grow up, and the way Wendy knew was this. One day when she was two years old she was playing in a garden, and she plucked another flower and ran with it to her mother. I suppose she must have looked rather delightful, for Mrs. Darling put her hand to her heart and cried, \u0026ldquo;Oh, why can\u0026rsquo;t you remain like this for ever!\u0026rdquo; This was all that passed between them on the subject, but henceforth Wendy knew that she must grow up. You always know after you are two. Two is the beginning of the end.\n","permalink":"https://nuxulu.com/posts/2010-01-07-post-modified/","summary":"This post has been updated and should show a modified date if used in a layout.\nAll children, except one, grow up. They soon know that they will grow up, and the way Wendy knew was this. One day when she was two years old she was playing in a garden, and she plucked another flower and ran with it to her mother. I suppose she must have looked rather delightful, for Mrs.","title":"Post: Modified Date"},{"content":"A notice displays information that explains nearby content. Often used to call attention to a particular detail.\nWhen using Kramdown {: .notice} can be added after a sentence to assign the .notice to the \u0026lt;p\u0026gt;\u0026lt;/p\u0026gt; element.\nChanges in Service: We just updated our privacy policy here to better service our customers. We recommend reviewing the changes. {: .notice}\nPrimary Notice: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. {: .notice\u0026ndash;primary}\nInfo Notice: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. {: .notice\u0026ndash;info}\nWarning Notice: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. {: .notice\u0026ndash;warning}\nDanger Notice: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. {: .notice\u0026ndash;danger}\nSuccess Notice: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. {: .notice\u0026ndash;success}\nWant to wrap several paragraphs or other elements in a notice? Using Liquid to capture the content and then filter it with markdownify is a good way to go.\n{% raw %}{% capture notice-2 %} #### New Site Features * You can now have cover images on blog pages * Drafts will now auto-save while writing {% endcapture %}{% endraw %} \u0026lt;div class=\u0026#34;notice\u0026#34;\u0026gt;{% raw %}{{ notice-2 | markdownify }}{% endraw %}\u0026lt;/div\u0026gt; {% capture notice-2 %}\nNew Site Features  You can now have cover images on blog pages Drafts will now auto-save while writing {% endcapture %}  Or you could skip the capture and stick with straight HTML.\n\u0026lt;div class=\u0026#34;notice\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;Message\u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt;A basic message.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ","permalink":"https://nuxulu.com/posts/2010-02-05-post-notice/","summary":"A notice displays information that explains nearby content. Often used to call attention to a particular detail.\nWhen using Kramdown {: .notice} can be added after a sentence to assign the .notice to the \u0026lt;p\u0026gt;\u0026lt;/p\u0026gt; element.\nChanges in Service: We just updated our privacy policy here to better service our customers. We recommend reviewing the changes. {: .notice}\nPrimary Notice: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio.","title":"Post: Notice"},{"content":" Only one thing is impossible for God: To find any sense in any copyright law on the planet.\n  Mark Twain ","permalink":"https://nuxulu.com/posts/2010-02-05-post-quote/","summary":" Only one thing is impossible for God: To find any sense in any copyright law on the planet.\n  Mark Twain ","title":"Post: Quote"},{"content":"All children, except one, grow up. They soon know that they will grow up, and the way Wendy knew was this. One day when she was two years old she was playing in a garden, and she plucked another flower and ran with it to her mother. I suppose she must have looked rather delightful, for Mrs. Darling put her hand to her heart and cried, \u0026ldquo;Oh, why can\u0026rsquo;t you remain like this for ever!\u0026rdquo; This was all that passed between them on the subject, but henceforth Wendy knew that she must grow up. You always know after you are two. Two is the beginning of the end.\nMrs. Darling first heard of Peter when she was tidying up her children\u0026rsquo;s minds. It is the nightly custom of every good mother after her children are asleep to rummage in their minds and put things straight for next morning, repacking into their proper places the many articles that have wandered during the day.\nThis post has a manual excerpt \u0026lt;!--more--\u0026gt; set after the second paragraph. The following YAML Front Matter has also be applied:\nexcerpt_separator:\u0026#34;\u0026lt;!--more--\u0026gt;\u0026#34;If you could keep awake (but of course you can\u0026rsquo;t) you would see your own mother doing this, and you would find it very interesting to watch her. It is quite like tidying up drawers. You would see her on her knees, I expect, lingering humorously over some of your contents, wondering where on earth you had picked this thing up, making discoveries sweet and not so sweet, pressing this to her cheek as if it were as nice as a kitten, and hurriedly stowing that out of sight. When you wake in the morning, the naughtiness and evil passions with which you went to bed have been folded up small and placed at the bottom of your mind and on the top, beautifully aired, are spread out your prettier thoughts, ready for you to put on.\nI don\u0026rsquo;t know whether you have ever seen a map of a person\u0026rsquo;s mind. Doctors sometimes draw maps of other parts of you, and your own map can become intensely interesting, but catch them trying to draw a map of a child\u0026rsquo;s mind, which is not only confused, but keeps going round all the time. There are zigzag lines on it, just like your temperature on a card, and these are probably roads in the island, for the Neverland is always more or less an island, with astonishing splashes of colour here and there, and coral reefs and rakish-looking craft in the offing, and savages and lonely lairs, and gnomes who are mostly tailors, and caves through which a river runs, and princes with six elder brothers, and a hut fast going to decay, and one very small old lady with a hooked nose. It would be an easy map if that were all, but there is also first day at school, religion, fathers, the round pond, needle-work, murders, hangings, verbs that take the dative, chocolate pudding day, getting into braces, say ninety-nine, three-pence for pulling out your tooth yourself, and so on, and either these are part of the island or they are another map showing through, and it is all rather confusing, especially as nothing will stand still.\nOf course the Neverlands vary a good deal. John\u0026rsquo;s, for instance, had a lagoon with flamingoes flying over it at which John was shooting, while Michael, who was very small, had a flamingo with lagoons flying over it. John lived in a boat turned upside down on the sands, Michael in a wigwam, Wendy in a house of leaves deftly sewn together. John had no friends, Michael had friends at night, Wendy had a pet wolf forsaken by its parents, but on the whole the Neverlands have a family resemblance, and if they stood still in a row you could say of them that they have each other\u0026rsquo;s nose, and so forth. On these magic shores children at play are for ever beaching their coracles [simple boat]. We too have been there; we can still hear the sound of the surf, though we shall land no more.\nOf all delectable islands the Neverland is the snuggest and most compact, not large and sprawly, you know, with tedious distances between one adventure and another, but nicely crammed. When you play at it by day with the chairs and table-cloth, it is not in the least alarming, but in the two minutes before you go to sleep it becomes very real. That is why there are night-lights.\nOccasionally in her travels through her children\u0026rsquo;s minds Mrs. Darling found things she could not understand, and of these quite the most perplexing was the word Peter. She knew of no Peter, and yet he was here and there in John and Michael\u0026rsquo;s minds, while Wendy\u0026rsquo;s began to be scrawled all over with him. The name stood out in bolder letters than any of the other words, and as Mrs. Darling gazed she felt that it had an oddly cocky appearance.\n","permalink":"https://nuxulu.com/posts/2010-01-07-post-standard/","summary":"\u003cp\u003eAll children, except one, grow up. They soon know that they will grow up, and the way Wendy knew was this. One day when she was two years old she was playing in a garden, and she plucked another flower and ran with it to her mother. I suppose she must have looked rather delightful, for Mrs. Darling put her hand to her heart and cried, \u0026ldquo;Oh, why can\u0026rsquo;t you remain like this for ever!\u0026rdquo; This was all that passed between them on the subject, but henceforth Wendy knew that she must grow up. You always know after you are two. Two is the beginning of the end.\u003c/p\u003e\n\u003cp\u003eMrs. Darling first heard of Peter when she was tidying up her children\u0026rsquo;s minds. It is the nightly custom of every good mother after her children are asleep to rummage in their minds and put things straight for next morning, repacking into their proper places the many articles that have wandered during the day.\u003c/p\u003e","title":"Post: Standard"}]